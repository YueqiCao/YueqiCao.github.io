<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>List of Convergence Rates in TDA</title>
      <link href="/2021/10/16/List-of-Convergence-Rates-in-TDA/"/>
      <url>/2021/10/16/List-of-Convergence-Rates-in-TDA/</url>
      
        <content type="html"><![CDATA[<h2 id="frechet-means-for-distributions-of-persistence-diagrams">Frechet means for distributions of persistence diagrams</h2><p>Consider the measure <span class="math inline">\(\rho = 1/m \sum_{i=1}^m \delta_{Z_i}\)</span> on the space of persistence diagrams. The population Frechet mean is defined by</p><p><span class="math display">\[Y = \arg\min_Z \int_{\mathcal{D}} d(X,Z)^2 {\rm d}\rho(X)\]</span></p><p>Let <span class="math inline">\(X_1,\cdots, X_n\)</span> be iid samples from <span class="math inline">\(\rho\)</span>. Let <span class="math inline">\(\rho_n = 1/n\sum_{i=1}^n \delta_{X_i}\)</span>. The empirical Frechet mean is defined by</p><p><span class="math display">\[Y_n = \arg\min_Z \int_{\mathcal{D}} d(X,Z)^2 {\rm d}\rho_n(X)\]</span></p><blockquote><p>There exists a <span class="math inline">\(Y_n\)</span> such that with probability greater than <span class="math inline">\(1-\delta\)</span> <span class="math display">\[d(Y,Y_n)^2\le \frac{m^2F(Y)}{n}\log(\frac{m}{\delta})\]</span> for <span class="math inline">\(n\ge 8m\log(m/\delta)\)</span> and the right hand side is less than <span class="math inline">\(r^2\)</span> where <span class="math inline">\(r\)</span> characterizes the separation between the local minima of <span class="math inline">\(F\)</span>.</p></blockquote><h2 id="convergence-rates-for-persistence-diagrams-estimation-in-topological-data-analysis">Convergence rates for persistence diagrams estimation in topological data analysis</h2><p>Let <span class="math inline">\((M,\rho)\)</span> be a metric space and <span class="math inline">\(\mu\)</span> is a measure on it with support <span class="math inline">\(M_\mu\)</span>. Further, assume that <span class="math inline">\(\mu\)</span> satisfies the <span class="math inline">\((a,b)\)</span>-standard assumption: for any <span class="math inline">\(x\)</span> in <span class="math inline">\(M_\mu\)</span> and <span class="math inline">\(r&gt;0\)</span>, <span class="math inline">\(\mu(B(x,r))\ge \min(ar^b,1)\)</span> (usually <span class="math inline">\(b=d\)</span> if in <span class="math inline">\(\mathbb{R}^d\)</span>). Let <span class="math inline">\(\mathbb{X}=\{X_1,\cdots,X_n\}\)</span> be iid samples from <span class="math inline">\(\mu\)</span>. Then we have</p><blockquote><p><span class="math inline">\(\mathbb{E}[d_\infty(\text{Dgm}(M_\mu),\text{Dgm}(\mathbb{X}))]\le C(\frac{\log n}{n})^{1/b}\)</span> where <span class="math inline">\(C\)</span> is a constant depending on <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p></blockquote><p>Essentially the result comes from the stability theorem of persistence diagrams and convergence for random sets with respect to Hausdorff distance. In fact, we have</p><blockquote><p>for any <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math inline">\(\mathbb{P}[d_H(M_\mu,\mathbb{X})]\le \frac{2^b}{a\epsilon^b}\exp(-na\epsilon^b)\wedge 1\)</span></p></blockquote><h2 id="subsampling-methods-for-persistent-homology">Subsampling methods for persistent homology</h2><p>Let <span class="math inline">\((M,\rho,\mu)\)</span> be the metric measure space satisfying the <span class="math inline">\((a,b)\)</span>-standard assumption. Let <span class="math inline">\(S_1,\cdots,S_n\)</span> be <span class="math inline">\(n\)</span> iid samples of size <span class="math inline">\(m\)</span> from <span class="math inline">\(\mu\)</span>. Define the empirical average landscape by</p><p><span class="math display">\[(\hat\lambda) = \frac{1}{n}\sum_{i=1}^n\lambda_{S_i}\]</span></p><p>The empirical average landscape is supposed to converge to the population average landscape which is</p><p><span class="math display">\[\mathbb{E}_{(\mu^{\otimes m})_*}[\lambda]\]</span></p><p>The bias part is controlled by the following</p><blockquote><p>Let <span class="math inline">\(r_m = 2(\frac{\log m}{am})^{1/b}\)</span>. If <span class="math inline">\(\mu\)</span> satisfies the <span class="math inline">\((a,b,r_0)\)</span> assumption, then <span class="math inline">\(\|\lambda_{M_\mu}-\mathbb{E}[\lambda]\|_\infty\le r_0+r_m 1_{(r_0,\infty)}(r_m)+C(a,b)\frac{r_m}{(\log m)^2}\)</span></p></blockquote><p>The variance part is controlled by</p><blockquote><p><span class="math inline">\(\mathbb{E}\|\hat{\lambda}-\mathbb{E}[\lambda]\|_\infty\le O(1/\sqrt{n})\)</span></p></blockquote><h2 id="estimation-and-quantization-of-expected-persistence-diagrams">Estimation and quantization of expected persistence diagrams</h2><p>Let <span class="math inline">\(\Omega=\{(x,y)|y&gt;x\}\)</span> be the open half-plane, and <span class="math inline">\(\partial\Omega=\{(x,x)|x\in\mathbb{R}\}\)</span> be the boundary of <span class="math inline">\(\Omega\)</span>. Let <span class="math inline">\(\mathcal{M}^p\)</span> be the space of Radon measures supported on <span class="math inline">\(\Omega\)</span> that have finite total <span class="math inline">\(p\)</span>-persistence, i.e. <span class="math inline">\(\int \|x-\partial\Omega\|^p {\rm d}\mu(x)&lt;\infty\)</span>. Define the distance between two measures <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span> by</p><p><span class="math display">\[\textrm{OT}_p = \inf_{\pi}\left(\int_{\bar{\Omega}\times\bar{\Omega}}\|x-y\|^p {\rm d}\pi(x,y)\right) \]</span></p><p>where <span class="math inline">\(\pi\)</span> ranges all measures supported on <span class="math inline">\(\bar{\Omega}\times\bar{\Omega}\)</span> whose first marginal coincides with <span class="math inline">\(\mu\)</span> and second marginal coincides with <span class="math inline">\(\nu\)</span>. Let <span class="math inline">\(P\)</span> be a probability distribution supported on <span class="math inline">\((\mathcal{M}^p,\textrm{OT}_p)\)</span>. Let <span class="math inline">\(\mathbf{E}(P)\)</span> be the measure defined by, for <span class="math inline">\(A\subset \Omega\)</span> compact,</p><p><span class="math display">\[\mathbb{E}(P)[A] = \mathbb{E}_P[\mu(A)]\]</span></p><p><span class="math inline">\(\mathbf{E}(P)\)</span> is called the expected persistence diagram. If <span class="math inline">\(P\)</span> is a distribution supported on the space of persistence diagrams, it is proved under mild assumptions, <span class="math inline">\(\mathbb{E}(P)\)</span> admits a density with respect to the Lebesgue measure on <span class="math inline">\(\Omega\)</span>.</p><p>Given iid samples <span class="math inline">\(\mu_1,\cdots,\mu_n\sim P\)</span>, the empirical expected persistence diagram is defined by <span class="math inline">\(\bar{\mu}_n=\frac{1}{n}\sum \mu_i\)</span>. We have <span class="math inline">\(\bar{\mu}_n\rightarrow\mathbf{E}(P)\)</span> under <span class="math inline">\(\textrm{OT}_p\)</span> almost surely. Specifically,</p><p><span class="math display">\[\mathbb{E}[\textrm{OT}_p^p(\bar{\mu}_n, \mathbf{E}(P))] = O(\frac{1}{n^{1/2}}+\frac{a_p(n)}{n^{p-q}})\]</span></p><p>where <span class="math inline">\(1\le p&lt;\infty\)</span>, <span class="math inline">\(0\le q&lt; p\)</span>, and <span class="math inline">\(P\)</span> is a distribution with 'some' restrictions.</p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Error Estimate under Hausdorff Distance</title>
      <link href="/2021/07/22/Error-Estimate-under-Hausdorff-Distance/"/>
      <url>/2021/07/22/Error-Estimate-under-Hausdorff-Distance/</url>
      
        <content type="html"><![CDATA[<p>This is a useful technique I learned from <a href="https://jmlr.csail.mit.edu/papers/volume16/chazal15a/chazal15a.pdf" target="_blank" rel="noopener">Convergence rates for persistence diagram estimation in topological data analysis</a> when one wants to prove convergence results for some estimator under Hausdorff distance. Typical settings are</p><ul><li>A metric space <span class="math inline">\((X,\rho)\)</span>;</li><li>A probability measure <span class="math inline">\(\mu\)</span> with support <span class="math inline">\(X_\mu\)</span>;</li><li>i.i.d samples <span class="math inline">\(\hat{X}=\{X_1,\cdots,X_n\}\)</span> from <span class="math inline">\(\mu\)</span>;</li></ul><p>the final essential condition is the <span class="math inline">\((a,b,r_0)\)</span>-standard assumption:</p><ul><li>when <span class="math inline">\(r&gt;r_0\)</span>, the ball with radius <span class="math inline">\(r\)</span> has volume <span class="math inline">\(\mu(B(r))&gt; ar^b\wedge 1\)</span>.</li></ul><h2 id="bound-on-covering-number">Bound on Covering Number</h2><p>The covering number <span class="math inline">\(Cv(A,r)\)</span> for a metric space <span class="math inline">\(A\)</span> is the minimum number of balls of radius <span class="math inline">\(r\)</span> that cover <span class="math inline">\(A\)</span>.</p><p>The packing number <span class="math inline">\(Pk(A,r)\)</span> for a metric space <span class="math inline">\(A\)</span> is the maximum number of balls of radius <span class="math inline">\(r\)</span> such that they are mutually disjoint.</p><p>We have the following relation</p><blockquote><p><span class="math inline">\(Pk(A,r)\le Cv(A,r)\le Pk(A,r/2)\)</span></p></blockquote><p><strong>proof.</strong> Set <span class="math inline">\(m=Pk(A,r)\)</span>. There exists <span class="math inline">\(m\)</span> points from <span class="math inline">\(A\)</span> such that the mutual distance is greater than <span class="math inline">\(2r\)</span>. If <span class="math inline">\(Cv(A,r)&lt;m\)</span>, then by pigeonhole principle there are at least two points lying in the same ball of radius <span class="math inline">\(2r\)</span> which is a contradiction.</p><p>Similarly, let <span class="math inline">\(m&#39;=Pk(A,r/2)\)</span>, and let <span class="math inline">\(x_1,\cdots, x_{m&#39;}\)</span> be points realizing the packing number. By definition, for any other point <span class="math inline">\(x\)</span> there exists at least one <span class="math inline">\(x_i\)</span> such that <span class="math inline">\(\rho(x,x_i)\le r\)</span>. Therefore, <span class="math inline">\(\cup B(x_i,r)=A\)</span>.<span class="math inline">\(\blacksquare\)</span></p><p>The <span class="math inline">\((a,b,r_0)\)</span>-standard assumption helps us to bound the packing number and covering number.</p><blockquote><p>when <span class="math inline">\(r&gt;r_0\)</span>, the packing number and covering number are such that <span class="math inline">\(Pk(A,r)\le \frac{1}{ar^b}\vee 1\)</span> and <span class="math inline">\(Cv(A,r)\le \frac{2^b}{ar^b}\vee 1\)</span>.</p></blockquote><p><strong>proof.</strong> It suffices to prove for <span class="math inline">\(r&lt;a^{-1/b}\)</span>. Let <span class="math inline">\(m=Pk(A,r)\)</span>. Choose a packing <span class="math inline">\(B_1,\cdots,B_m\)</span>. Since they are disjoint, by assumption we have</p><p><span class="math display">\[\sum_{i=1}^m\mu(B_i)\le 1\implies m\le \frac{1}{ar^b}\vee 1\]</span></p><p>and for covering numbers we have</p><p><span class="math display">\[Cv(A,r)\le Pk(A,r/2)\le \frac{2^b}{ar^b}\vee 1\quad\blacksquare\]</span></p><h2 id="bound-on-hausdorff-distance">Bound on Hausdorff distance</h2><p>When the sample size <span class="math inline">\(n\)</span> tends to infinity, our intuition tells us that the Hausdorff distance between samples and <span class="math inline">\(X_\mu\)</span> should converge to zeros. The idea of our proof is to reduce the distance between <span class="math inline">\(X_\mu\)</span> and <span class="math inline">\(\hat{X}\)</span> to the distance between covering set and <span class="math inline">\(\hat{X}\)</span>, where in the latter case we are dealing with a fixed finite set. More precisely, let <span class="math inline">\(\mathcal{C}=\{c_1,\cdots,c_p\}\)</span> be the set of points realizing the covering number <span class="math inline">\(Cv(X_\mu,r/2)\)</span>. Then</p><p><span class="math display">\[\begin{aligned}\mathbb{P}(d_H(X,X_\mu)&gt;r)\le &amp; \mathbb{P}(d_H(X,\mathcal{C})+d_H(\mathcal{C},\hat{X})&gt;r)\\\le &amp; \mathbb{P}(d_H(\mathcal{C},\hat{X})&gt;r/2)\\\le &amp; \mathbb{P}(\exists i\in\{1,\cdots,p\} \text{ such that } \hat{X}\cap B(c_i)=\emptyset)\\\le &amp; \sum_{i=1}^p \mathbb{P}(\hat{X}\cap B(c_i)=\emptyset)\\\le &amp; \frac{4^b}{ar^b}[1-\frac{ar^b}{2^b}]^n\\\le &amp; \frac{4^b}{ar^b}\exp(-n\frac{a}{2^b}r^b)\end{aligned}\]</span></p><p>The first inequality comes from the triangle inequality. The second line comes from the fact that any point in <span class="math inline">\(X_\mu\)</span> is in the ball of radius <span class="math inline">\(r/2\)</span> centered at covering set. The third line comes from the definition of Hausdorff distance. Note that for any sample point the distance to <span class="math inline">\(\mathcal{C}\)</span> is less than <span class="math inline">\(r/2\)</span>. However, for a covering point the distance to <span class="math inline">\(\hat{X}\)</span> can be larger than <span class="math inline">\(r/2\)</span>. The fourth line is a relaxation of the third line. The fifth line substitutes the bound on covering number and standard assumption.</p><p>Then we have</p><p><span class="math display">\[\begin{aligned}\mathbb{E}[d_H(\hat{X},X_\mu)]=&amp;{}\int_{r&gt;0}\mathbb{P}(d_H(\hat{X},X_\mu))\ {\rm d}r\\\le&amp;{} r_0 + \int_{r&gt;r_0}\mathbb{P}(d_H(\hat{X},X_\mu))\ {\rm d}r\end{aligned}\]</span></p><p>Let <span class="math inline">\(r_n=2(\frac{\log n}{an})^{1/b}\)</span> (the choice of <span class="math inline">\(r_n\)</span> depends on the term inside the exponential). Then</p><p><span class="math display">\[\begin{aligned}\mathbb{E}[d_H(\hat{X},X_\mu)]= r_0+r_n1_{(r_0,\infty)}+\int_{r&gt;r_n}\mathbb{P}(d_H(\hat{X},X_\mu))\ {\rm d}r\end{aligned}\]</span></p><p>It suffices to bound the integral. Use the transformation of variable <span class="math inline">\(t=nar^b/2^b\)</span>. The integral becomes</p><p><span class="math display">\[\begin{aligned}   \int_{r&gt;r_n}\frac{4^b}{ar^b}\exp(-n\frac{a}{2^b}r^b)\,{\rm d}r &amp;\le \int_{r&gt;\log n} \frac{2^bn}{t}\exp(-t)\, {\rm d}(\frac{2t^{1/b}}{(na)^{1/b}})\\   &amp; = \frac{2^{b+1}n^{1-1/b}}{a^{1/b}b}\int_{r&gt;\log n}t^{1/b-2}\exp(-t)\,{\rm d}t\\    &amp; =  \frac{2^{b+1}n^{1-1/b}}{a^{1/b}b}(\log n)^{1/b-2}\exp(-t)|^{\log n}_\infty\\    &amp; = \frac{2^{b+1}}{a^{1/b}b}(\frac{\log n}{an})^{1/b}\frac{1}{(\log n)^2}\end{aligned}\]</span></p><p>If <span class="math inline">\(r_0=0\)</span>, then we see that</p><p><span class="math display">\[\begin{aligned}\mathbb{E}[d_H(\hat{X},X_\mu)]=O((\log n/n)^{1/b}) \end{aligned}\]</span></p><p>Moreover, we see that</p><p><span class="math display">\[\begin{aligned}   \mathbb{P}(d_H(\hat{X},X_\mu)\le r_n) = 1-\mathbb{P}(d_H(\hat{X},X_\mu)&gt;r_n)\ge 1-O((\log n)^{1/b-2}/n^{1/b}) \end{aligned}\]</span></p><p>thus with high probability,</p><p><span class="math display">\[d_H(\hat{X},X_\mu)\le r_n\]</span></p><p>or</p><p><span class="math display">\[\lim_{n\to \infty}\mathbb{P}(d_H(\hat{X},X_\mu)\le r_n)=1\]</span></p><h2 id="bound-on-bottleneck-distance">Bound on Bottleneck Distance</h2><p>As an application we can prove the convergence rate for persistence diagrams. Suppose we are sampling points from a measure <span class="math inline">\(\mu\)</span> with support <span class="math inline">\(X_\mu\)</span>. The support <span class="math inline">\(X_\mu\)</span> has a true persistence diagram <span class="math inline">\(D(X_\mu)\)</span>. From the sampling points we have an empirical persistence diagram <span class="math inline">\(D(\hat{X})\)</span>. We ask if <span class="math inline">\(D(\hat{X})\)</span> is a good estimator of <span class="math inline">\(D(X_\mu)\)</span>. By the <a href="http://yueqicao.top/2019/10/02/Stability-Theorems-in-Topological-Data-Analysis-3/">stability theorem</a>,</p><p><span class="math display">\[d_B(D(X_\mu),D(\hat{X}))\le d_H(X_\mu,\hat{X})\]</span></p><p>therefore we find that with high probability,</p><p><span class="math display">\[d_B(D(X_\mu),D(\hat{X}))\le r_n\]</span></p><p>Thus we can approximate the true persistence diagram by sampling. As the sample size tends to infinity, the empirical persistence diagram will approximate the true diagram with rate at least <span class="math inline">\(O(r_n)\)</span>. However, since the computational cost for persistence homology is not economic, in practice we cannot have persistence diagram from large samples. In this case, we need bootstrap method to first approximate the persistence diagram for the large sample. See <a href="http://proceedings.mlr.press/v37/chazal15.pdf" target="_blank" rel="noopener">Subsampling methods fro persistent homology</a>.</p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tex2svg</title>
      <link href="/2021/05/31/tex2svg/"/>
      <url>/2021/05/31/tex2svg/</url>
      
        <content type="html"><![CDATA[<p>When I want to save a bunch of tex equations/symbols as svg files I find there is no efficient software to fulfill this goal. But I find the interesting <a href="https://xiaoxiang.io/5aab4864f26b75000111c679/" target="_blank" rel="noopener">pipeline</a> which is implemented on a Linux system. The following is what I did on my CentOS7 server.</p><h3 id="install-latex">Install Latex</h3><p>Login in as <strong>root</strong>. Then install texlive using the following two commands</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install texlive-latex</span><br><span class="line">yum install texmaker</span><br></pre></td></tr></table></figure><p>Check your pdflatex version using</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pdflatex --version</span><br></pre></td></tr></table></figure><h3 id="install-pdfcrop">Install pdfcrop</h3><p><a href="https://github.com/ho-tex/pdfcrop" target="_blank" rel="noopener">pdfcrop</a> is written by perl. First install perl on CentOS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install perl</span><br></pre></td></tr></table></figure><p>Check the perl version using</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perl -v</span><br></pre></td></tr></table></figure><p>Then download or clone the pdfcrop repository</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ho-tex/pdfcrop.git</span><br></pre></td></tr></table></figure><p>cd to the directory where you can see the perl file. Run with a test pdf using</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perl pdfcrop.pl test.pdf</span><br></pre></td></tr></table></figure><p>The output is named as</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test-crop.pdf</span><br></pre></td></tr></table></figure><h3 id="install-poppler">Install poppler</h3><p>The original pipeline uses <a href="https://github.com/dawbarton/pdf2svg" target="_blank" rel="noopener">pdf2svg</a> which requires <a href="https://poppler.freedesktop.org/" target="_blank" rel="noopener">poppler</a> and <a href="https://www.cairographics.org/" target="_blank" rel="noopener">cairo</a> of high version. Installing dependencies on CentOS7 is a terrible struggle. The simplest way is to use pdftocairo which is contained in poppler-utils.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install poppler-utils</span><br></pre></td></tr></table></figure><p>Then run with a test pdf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pdftocairo test.pdf output.svg -svg</span><br></pre></td></tr></table></figure><h3 id="pack-the-pipeline">Pack the pipeline</h3><p>Write a bash script to pack all steps together.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">vim test.tex</span><br><span class="line">pdflatex test.tex</span><br><span class="line">perl pdfcrop.pl test.pdf</span><br><span class="line">pdftocairo test-crop.pdf output.svg -svg</span><br><span class="line">rm test.* test-crop.pdf</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> CS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Random Perturbation to Low Rank Matrices</title>
      <link href="/2021/04/09/Random-Perturbation-to-Low-Rank-Matrices/"/>
      <url>/2021/04/09/Random-Perturbation-to-Low-Rank-Matrices/</url>
      
        <content type="html"><![CDATA[<h2 id="motivation">Motivation</h2><p>Let <span class="math inline">\(A\)</span> stand for the true symmetric matrix and <span class="math inline">\(E\)</span> represent the perturbation. Let <span class="math inline">\(\lambda_i\)</span>'s be the eigenvalues of <span class="math inline">\(A\)</span>, sorted in descending order and denote <span class="math inline">\(\delta=\lambda_1-\lambda_2\)</span> to be the eigengap. Let <span class="math inline">\(u_i\)</span> be the eigenvector of <span class="math inline">\(A\)</span> and <span class="math inline">\(v_i\)</span> be the eigenvector of <span class="math inline">\(A+E\)</span>. The classical <a href="http://yueqicao.top/2021/01/12/Davis-Kahan-s-Theorem/">Davis-Kahan theorem</a> states that</p><p><span class="math display">\[\sin(u_1,v_1)\le \frac{2\|E\|_{op}}{\delta}\]</span></p><p>up to constant the DK bound is sharp, as we see in the following example</p><p><span class="math display">\[A=\left[\begin{array}{cc}1+\epsilon &amp; 0\\ 0 &amp;1-\epsilon    \end{array}\right],\, E=\left[\begin{array}{cc}    -\epsilon &amp; \epsilon \\ \epsilon &amp; \epsilon\end{array}\right]\]</span></p><p>However, researchers find that in certain cases the bound can be improved. In <a href="https://arxiv.org/abs/1311.2657v5" target="_blank" rel="noopener">Vu's paper</a>, the author considered the following setting: <span class="math inline">\(A\)</span> is an <span class="math inline">\(n\times n\)</span> matrix but of low rank <span class="math inline">\(r\)</span>, <span class="math inline">\(E\)</span> is a random Bernoulli matrix whose entries are iid random variables taking values <span class="math inline">\(\pm 1\)</span> with probability <span class="math inline">\(1/2\)</span>. From random matrix theory we know that <span class="math inline">\(\|E\|_{op}=(2+o(1))\sqrt{n}\)</span>. Then the classical DK bound is about <span class="math inline">\(4\sqrt{n}/\delta\)</span>. But the experiment shows that this bound is far from optimal.</p><figure><img src="/2021/04/09/Random-Perturbation-to-Low-Rank-Matrices/DKbound.png" alt="DK"><figcaption aria-hidden="true">DK</figcaption></figure><p>In the experiment we construct a <span class="math inline">\(400\times 400\)</span> matrix <span class="math inline">\(A\)</span> such that <span class="math inline">\(A(1,1)=10+\delta, A(2,2)=10\)</span> and zero otherwise. We set <span class="math inline">\(\delta=100(1+rand)\)</span> where <span class="math inline">\(rand\)</span> is uniform on <span class="math inline">\([0,1]\)</span> and we run 20 times. The blue bar represents the true <span class="math inline">\(\sin\)</span> and red bar represents the classical DK bound. The result motivates us to find a sharper bound in random settings.</p><h2 id="random-noises">Random Noises</h2><p>We state a simplified assumption on the random matrix <span class="math inline">\(E\)</span> from <a href="https://arxiv.org/abs/1311.2657v5" target="_blank" rel="noopener">Vu</a> (just omit the constants). The <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(E\)</span> is <span class="math inline">\(\gamma\)</span>-concentrated if for all unit vectors <span class="math inline">\(u,v\)</span> and every <span class="math inline">\(t\)</span> we have</p><p><span class="math display">\[\log\mathbb{P}(|u^TEv|&gt;t)=O(-t^\gamma)\]</span></p><p>i.e. the rate tending to <span class="math inline">\(-\infty\)</span> is in order at least <span class="math inline">\(\gamma\)</span>.</p><p><strong>Examples:</strong></p><ul><li>Bernoulli matrix is <span class="math inline">\(2\)</span>-concentrated;</li><li>Gaussian matrix is <span class="math inline">\(2\)</span>-concentrated;</li><li>Sub-exponential matrix is <span class="math inline">\(1\)</span>-concentrated.</li></ul><p>In particular, for bounded random matrices we have</p><blockquote><p>Let <span class="math inline">\(E=(\xi_{ij})\)</span> be a matrix with independent random variables each with mean zero. Furthermore, <span class="math inline">\(|\xi_{ij}|\le K\)</span> a.s. for all <span class="math inline">\(i,j\)</span>. Then <span class="math inline">\(E\)</span> is <span class="math inline">\(2\)</span>-concentrated and <span class="math display">\[\mathbb{P}(|u^TEv|&gt;t)\le 2\exp(\frac{-t^2}{2K^2}) \]</span></p></blockquote><h2 id="improved-dk-bound">Improved DK bound</h2><p>Note that</p><p><span class="math display">\[\sin^2\angle (u_1,v_1) = 1-\cos^2\angle (u_1,v_1) = \sum_{i=2}^n |u_k\cdot v_1|^2\]</span></p><p>Thus it suffices to bound <span class="math inline">\(|u_k\cdot v_1|\)</span>. In fact, Let <span class="math inline">\(Q=[u_2,\cdots,u_r]\)</span> and <span class="math inline">\(P=[u_{r+1},\cdots,u_n]\)</span>. we want to bound</p><p><span class="math display">\[\|Q^Tv_1\|^2 \text{ and } \|P^Tv_1\|^2\]</span></p><p>For <span class="math inline">\(Q^Tv_1\)</span> we have</p><p><span class="math display">\[Q^T(A+E)v_1-Q^TAv_1=Q^TEv_1\]</span></p><p>which is equivalent to</p><p><span class="math display">\[(\mu_1I-\Lambda_r)Q^Tv_1 = Q^TEv_1\]</span></p><p>where <span class="math inline">\(\Lambda_k\)</span> is the diagonal matrix consisting of <span class="math inline">\(\lambda_2,\cdots,\lambda_r\)</span>. It follows</p><p><span class="math display">\[|\mu_1-\lambda_2|\|Q^Tv_1\|\le \|Q^TEv_1\|\]</span></p><p>Thus we need a lower bound for gap <span class="math inline">\(|\mu_1-\lambda_2|\)</span> and an upper bound for <span class="math inline">\(|Q^TEv_1|\)</span>. For <span class="math inline">\(\mu_1\)</span> we have</p><p><span class="math display">\[\mu_1 = \|A+E\|_{op}\ge u_1^T(A+E)u_1 = \lambda_1 + u_1^TEu_1 \]</span></p><p>Since <span class="math inline">\(E\)</span> is <span class="math inline">\(\gamma\)</span>-concentrated, we see that <span class="math inline">\(\mu_1\ge \lambda_1-t\)</span> with probability at least <span class="math inline">\(1-\exp(O(-t^\gamma))\)</span>. If the eigengap <span class="math inline">\(\delta=\lambda_1-\lambda_2\)</span> is sufficient large (for example, <span class="math inline">\(\delta= 2t\)</span>) then <span class="math inline">\(\mu_1-\lambda_2&gt;\delta/2\)</span> with probability at least <span class="math inline">\(1-\exp(O(-\delta^\gamma))\)</span>.</p><p>From the above discussion we know <span class="math inline">\(\mu_1\ge \lambda_1/2\)</span> with probability at least <span class="math inline">\(1-\exp(O(-\lambda_1^\gamma))\)</span>. Then</p><p><span class="math display">\[(PP^Tv_1)^T(A+E)v_1-(PP^Tv_1)^TAv_1=(PP^Tv_1)^TEv_1\]</span></p><p>Note that <span class="math inline">\(P^TA=0\)</span> by our construction and so <span class="math inline">\(\mu_1(PP^Tv_1)^Tv_1=(PP^Tv_1)^TEv_1\)</span>. We know that</p><p><span class="math display">\[\|P^Tv_1\|^2 = (PP^Tv_1)^Tv_1 \le \frac{1}{\mu_1}\|P^Tv_1\|\|E\|_{op}\]</span></p><p>Therefore, <span class="math inline">\(\|P^Tv_1\|\le 2\|E\|_{op}/\lambda_1\)</span> with probability at least <span class="math inline">\(1-\exp(O(-\lambda_1^\gamma))\)</span>.</p><p>Now if we write <span class="math inline">\(v_1= (I-PP^T)v_1+PP^Tv_1\)</span> then</p><p><span class="math display">\[\|Q^TEv_1\|\le \|(I-PP^T)^TE(I-PP^T)\|_{op}+\|E\|_{op}\|P^Tv_1\|\]</span></p><p>By a standard <span class="math inline">\(\epsilon\)</span>-net argument the first term is proved to be no greater than <span class="math inline">\(tr^{1/\gamma}\)</span> with probability at least <span class="math inline">\(1-\exp(O(-rt^\gamma)+O(r))\)</span>. Above all we have</p><p><span class="math display">\[\|Q^Tv_1\|\le \frac{2}{\delta}(tr^{1/\gamma}+2\frac{\|E\|_{op}^2}{\lambda_1})\]</span></p><p>with probability at least <span class="math inline">\(1-\exp(O(-rt^\gamma)+O(r))-\exp(O(-\lambda_1^\gamma))-\exp(O(-\delta^\gamma))\)</span>.</p><p>Finally we see that the new bound is related to three items: the largest eigenvalue <span class="math inline">\(\lambda_1\)</span>, the eigengap <span class="math inline">\(\delta\)</span> and the rank <span class="math inline">\(r\)</span>:</p><p><span class="math display">\[\sin\angle (u_1,v_1)\le 4(\frac{tr^{1/\gamma}}{\delta}+\frac{\|E\|_{op}}{\lambda_1}+\frac{\|E\|^2_{op}}{\lambda_1\delta})\]</span></p><p>with probability at least <span class="math inline">\(1-\exp(O(-rt^\gamma)+O(r))-\exp(O(-\lambda_1^\gamma))-\exp(O(-\delta^\gamma))\)</span>.</p><p><strong>Remark 1.</strong> This result is only nontrivial if the amplitude of <span class="math inline">\(\lambda_1\)</span> is much larger than <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\|E\|_{op}\)</span> (otherwise when <span class="math inline">\(\lambda_1\sim\delta\)</span> the bound is worse than classical DK bound). In this case the bound is <span class="math inline">\(O(r^{\gamma}/\delta)\)</span> which is much better than <span class="math inline">\(O(n^{1/\gamma}/\delta)\)</span>.</p><p><strong>Remark 2.</strong> From the above simulation we see that the bound is not sharp. Therefore there are chances to obtain better results for random perturbations.</p>]]></content>
      
      
      <categories>
          
          <category> Matrix-Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Perturbation Theory </tag>
            
            <tag> Matrix Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Davis-Kahan Theorem</title>
      <link href="/2021/01/12/Davis-Kahan-s-Theorem/"/>
      <url>/2021/01/12/Davis-Kahan-s-Theorem/</url>
      
        <content type="html"><![CDATA[<h2 id="problem-statement">Problem Statement</h2><p>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n\times n\)</span> Hermitian matrix, and suppose we have the following spectral decomposition for <span class="math inline">\(A\)</span></p><p><span class="math display">\[A=\sum_{i=1}^n\lambda_iu_iu_i^*\]</span></p><p>where <span class="math inline">\(\lambda_i\)</span>'s are eigenvalues of <span class="math inline">\(A\)</span> (<em>we do not need to sort eigenvalues here</em>), and <span class="math inline">\(u_i\)</span>'s are corresponding eigenvectors and <span class="math inline">\(u^*_i\)</span> denotes conjugate transpose. Now, suppose <span class="math inline">\(H\)</span> is also an <span class="math inline">\(n\times n\)</span> Hermitian matrix that represents some perturbation added to <span class="math inline">\(A\)</span>. The spectral decomposition for <span class="math inline">\(A+H\)</span> is</p><p><span class="math display">\[A+H=\sum_{i=1}^n\mu_iv_iv_i^*\]</span></p><p>We want to know</p><ul><li><ol type="1"><li>the error bound for eigenvalues <span class="math inline">\(\lambda_i\)</span> and <span class="math inline">\(\mu_i\)</span>;</li></ol></li><li><ol start="2" type="1"><li>the error bound for eigenvectors <span class="math inline">\(u_i\)</span> and <span class="math inline">\(v_i\)</span>.</li></ol></li></ul><p>It is worth thinking whether above two problems are <strong>well-defined</strong>. In fact, a critical thing is how to 'match' the original eigen-value/vector and the corresponding perturbed eigen-value/vector. For the first problem, we can sort the eigenvalues and match them according to the order. For the second problem, however, the situation is more intricate: think about two different eigenvalues that become equal after perturbation. Then any vector in the two-dimensional eigenspace will be an eigenvector corresponding to the new eigenvalue. Therefore, it is natural to place some assumptions on eigengaps to separate different eigenspaces.</p><p>Define <span class="math display">\[P=\sum_{i=1}^ku_iu_i^*=UU^*\]</span> to be the orthogonal projection operator to the <span class="math inline">\(k\)</span>-dimensional eigenspace spanned by <span class="math inline">\(u_1,\cdots,u_k\)</span>. Let <span class="math display">\[P_\perp=I-P=\sum_{i=k+1}^nu_iu_i^*=U_\perp U_\perp^*\]</span> Similarly, define <span class="math display">\[Q=\sum_{i=1}^kv_iv_i^*=VV^*\]</span> and <span class="math inline">\(Q_\perp=I-Q\)</span>. The second problem can be generalized to</p><ul><li>(2*) the error bound for projections <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</li></ul><p>We focus on the generalized problem.</p><h2 id="principal-angles">Principal Angles</h2><p>Recall that for nonzero vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> we define their angles to be</p><p><span class="math display">\[\angle x,y = \text{arccos} \frac{x^*y}{\|x\|\|y\|}\]</span></p><p>The range for angles between two vectors is <span class="math inline">\([0,\pi]\)</span>. Recall that in affine geometry we have defined angles between lines and planes: we define the angle between <span class="math inline">\(\mathbb{R}x\)</span> and <span class="math inline">\(\mathbb{R}y\)</span> to be</p><p><span class="math display">\[\angle \mathbb{R}x,\mathbb{R}y = \text{arccos} \frac{|x^*y|}{\|x\|\|y\|} = \inf_{a,b\in\mathbb{R}}\text{arccos} \frac{(ax)^*(by)}{\|ax\|\|by\|}\]</span></p><p>Suppose <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span> span a two-plane <span class="math inline">\(\mathbb{R}\{y,z\}\)</span>. The angle between <span class="math inline">\(\mathbb{R}x\)</span> and <span class="math inline">\(\mathbb{R}\{y,z\}\)</span> is defined as</p><p><span class="math display">\[\angle \mathbb{R}x,\mathbb{R}\{y,z\} = \angle \mathbb{R}x,\mathbb{R}(yy^*+zz^*)x = \inf_{a,b,c\in\mathbb{R}}\text{arccos}\frac{(ax)^*(by+cz)}{\|ax\|\|by+cz\|}\]</span></p><p>Now we want to define angles between two vector spaces. Suppose <span class="math inline">\(E=\mathbb{C}\{e_1,\cdots,e_r\}\)</span> and <span class="math inline">\(F=\mathbb{C}\{f_1,\cdots,f_s\}\)</span> are two vector spaces, we define</p><p><span class="math display">\[\angle E,F = \inf_{e\in E,f\in F}\text{arccos}\frac{e^*f}{\|e\|\|f\|}\]</span></p><p>With a bit abuse of notation, we let <span class="math inline">\(E\)</span> denote the matrix <span class="math inline">\([e_1,\cdots,e_r]\)</span> and similarly let <span class="math inline">\(F\)</span> denote <span class="math inline">\([f_1,\cdots,f_s]\)</span>. If we write</p><p><span class="math display">\[e=E\alpha,\quad f=F\beta\]</span></p><p>where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are unit vectors. Then we see that</p><p><span class="math display">\[\angle E,F = \inf_{\alpha,\beta}\text{arccos}(\alpha^*E^*F\beta)=\text{arccos}(\lambda_{\max}(E^*F))\]</span></p><p>where <span class="math inline">\(\lambda_{\max}\)</span> denotes the largest singular value. Since we have seen that the angle between two vector spaces is related to the singular value, it will be helpful to consider all the singular values instead of the largest one. We call each <span class="math display">\[\theta_i=\text{arccos}\lambda_i(E^*F)\]</span> the principal angle between <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span>. Note that each principal angle is between <span class="math inline">\([0,\pi/2]\)</span>. Define the following matrix <span class="math display">\[\Theta_{E,F}=\text{arccos}(E^*F)\]</span></p><p>where <span class="math inline">\(\text{arccos}\)</span> acts on singular values in the singular decomposition. Though in general cases <span class="math inline">\(\Theta_{E,F}\neq \Theta_{F,E}\)</span>, in the following discussion we only focus on <span class="math inline">\(\sin(\Theta)\)</span> whose nonzero singular values are equal for <span class="math inline">\(\sin(\Theta_{E,F})\)</span> and <span class="math inline">\(\sin(\Theta_{F,E})\)</span>. Now we have prepared to look at the Davis-Kahan theorem.</p><h2 id="davis-kahans-sintheta-theorem">Davis-Kahan's <span class="math inline">\(\sin(\Theta)\)</span> Theorem</h2><p>Back to our set-up, we want to compare two projection matrices <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>. Using the Frobenius norm for matrices, we have</p><p><span class="math display">\[\|P-Q\|_{F}^2=\text{tr}((P-Q)^2)=2k-2\text{tr}(PQ)\]</span></p><p>Note that <span class="math display">\[\text{tr}(PQ)=\|U^*V\|_F^2=\|\cos(\Theta_{U,V})\|^2_F\]</span> Therefore, we have <span class="math display">\[\|P-Q\|_F=\sqrt{2}\|\sin(\Theta_{U,V})\|_F\]</span></p><p>To bound the error between two projection matrices, it suffices to bound <span class="math inline">\(\|\sin(\Theta_{U,V})\|_F\)</span>.</p><p>We write the eigen-decomposition for <span class="math inline">\(A\)</span> with respect to <span class="math inline">\(U\)</span> and <span class="math inline">\(U_\perp\)</span> as</p><p><span class="math display">\[A = [U,U_\perp]\left[\begin{array}{cc}    \Lambda &amp; 0\\    0 &amp; \Lambda_\perp\end{array}\right]\left[\begin{array}{c}    U^*\\ U^*_\perp\end{array}\right]\]</span></p><p>where <span class="math inline">\(\Lambda\)</span> is the diagonal matrix consisting of eigenvalues <span class="math inline">\(\lambda_1,\cdots,\lambda_k\)</span> whereas <span class="math inline">\(\Lambda_\perp\)</span> consists of eigenvalues <span class="math inline">\(\lambda_{k+1},\cdots,\lambda_n\)</span>. Similarly, write the eigen-decomposition for <span class="math inline">\(A+H\)</span> with respect to <span class="math inline">\(V\)</span> and <span class="math inline">\(V_\perp\)</span> as</p><p><span class="math display">\[A+H = [V,V_\perp]\left[\begin{array}{cc}    M &amp; 0\\    0 &amp; M_\perp\end{array}\right]\left[\begin{array}{c}    V^*\\ V^*_\perp\end{array}\right]\]</span></p><p>Finally, we write <span class="math inline">\(H\)</span> in the following form</p><p><span class="math display">\[H = [U,U_\perp]\left[\begin{array}{cc}    H_0 &amp; B\\    B^* &amp; H_1\end{array}\right]\left[\begin{array}{c}    U^*\\ U^*_\perp\end{array}\right]\]</span></p><p>so that under the basis <span class="math inline">\([U,U_\perp]\)</span>, we have</p><p><span class="math display">\[A+H = [U,U_\perp]\left[\begin{array}{cc}    \Lambda+H_0 &amp; B\\    B^* &amp; \Lambda_\perp+H_1\end{array}\right]\left[\begin{array}{c}    U^*\\ U^*_\perp\end{array}\right]\]</span></p><p>Since our focus is the eigenspace spanned by <span class="math inline">\(u_1,\cdots,u_k\)</span>, define the following residual</p><p><span class="math display">\[\begin{aligned}R =&amp; (A+H)U-AU=UH_0+U_{\perp} B^*\\=&amp; VM V^*U+V_\perp M_\perp V_\perp^*U-U\Lambda\end{aligned}\]</span></p><p>If we multiply <span class="math inline">\(V_\perp^*\)</span> on the left, we have</p><p><span class="math display">\[V_\perp^*R=M_\perp V_\perp^*U-V_\perp^*U\Lambda\]</span></p><p>Note that <span class="math inline">\(M_\perp\)</span> and <span class="math inline">\(\Lambda\)</span> are nonnegative diagonal matrices. For Frobenius norm we have the following lemma</p><blockquote><p>Suppose <span class="math inline">\(D\)</span> is a nonnegative diagonal matrix whose largest and smallest elements are <span class="math inline">\(d_{\max}\)</span> and <span class="math inline">\(d_{\min}\)</span>, respectively. Then we have <span class="math inline">\(d_{\min}\|X\|_F\le \|DX\|_F\le d_{\max}\|X\|_F\)</span>.</p></blockquote><p>This lemma inspires us to place some assumption on gaps between <span class="math inline">\(M_\perp\)</span> and <span class="math inline">\(\Lambda\)</span> to obtain a bound for <span class="math inline">\(R\)</span>. It coincides with our intuition: In most cases we often assume that there is some eigengap between <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\Lambda_\perp\)</span>. After a small perturbation <span class="math inline">\(H\)</span>, we may expect <span class="math inline">\(M_\perp\)</span> is close to <span class="math inline">\(\Lambda_\perp\)</span>. Now if there is still a gap between <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(M_\perp\)</span>, we believe that the original projection is close to the perturbed projection. So, we may assume the following</p><blockquote><p>Suppose there exists some <span class="math inline">\(\delta&gt;0\)</span> such that <span class="math inline">\(|\lambda_i-M_j|&gt;\delta\)</span> for all <span class="math inline">\(i=1,\cdots,k\)</span> and <span class="math inline">\(j=k+1,\cdots,n\)</span>.</p></blockquote><p>Under this assumption we see that</p><p><span class="math display">\[\|R\|_F\ge\|V_\perp^*R\|_F\ge \delta\|V_\perp^*U\|_F\]</span></p><p>However, <span class="math inline">\(\|V^*_\perp U\|_F\)</span> is nothing but <span class="math inline">\(\|\sin(\Theta_{U,V})\|_F\)</span>. To see that, we have</p><p><span class="math display">\[\|V_\perp^*U\|_F^2=\text{tr}(P(I-Q))=k-\text{tr}(PQ)=\|\sin(\Theta_{U,V})\|_F^2\]</span></p><p>Thus we obtain the so-called Davis-Kahan's <span class="math inline">\(\sin(\Theta)\)</span> theorem</p><p><span class="math display">\[\|P-Q\|_F\le \frac{\sqrt{2}\|R\|_F}{\delta}\le \frac{\sqrt{2}\|H\|_F}{\delta}\]</span></p><p>If we are interested in the spectral norm, apply the inequality <span class="math inline">\(\|R\|_F\le \sqrt{k}\|R\|_{op}\)</span>, and note that <span class="math inline">\(\|R\|_{op}\le\|H\|_{op}\)</span>. So we also have</p><p><span class="math display">\[\|P-Q\|_{op}\le \|P-Q\|_F\le \frac{\sqrt{2k}\|H\|_{op}}{\delta}\]</span></p><p>It is also possible to make assumptions on gaps between <span class="math inline">\(\Lambda_\perp\)</span> and <span class="math inline">\(M\)</span>. Consider another residual</p><p><span class="math display">\[\begin{aligned}R_\perp =&amp; (A+H)U_\perp-AU_\perp=UB+U_\perp H_1\\=&amp; VM V^*U_\perp+V_\perp M_\perp V_\perp^*U_\perp-U_\perp \Lambda_\perp\end{aligned}\]</span></p><p>Multiply <span class="math inline">\(V^*\)</span> on the left</p><p><span class="math display">\[V^*R_\perp = M V^*U_\perp-V^*U_\perp\Lambda_\perp\]</span></p><p>But what is <span class="math inline">\(\|V^*U_\perp\|_F\)</span>? The computation shows that</p><p><span class="math display">\[\|V^*U_\perp\|_F^2=\text{tr}(Q(I-P))=k-\text{tr}(PQ)=\|\sin(\Theta_{U,V})\|_F^2\]</span></p><p>Thus we come to the same conclusion.</p><h2 id="generalizations">Generalizations</h2><p>A defect in Davis-Kahan's theorem is that we need to compare either <span class="math inline">\(\Lambda\)</span> with <span class="math inline">\(M_\perp\)</span> or <span class="math inline">\(\Lambda_\perp\)</span> with <span class="math inline">\(M\)</span>. However, in practice (especially in statistics where <span class="math inline">\(A\)</span> usually stands for some population matrix and <span class="math inline">\(H\)</span> stands for random noises), it is more reasonable to assume a priori comparison between <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\Lambda_\perp\)</span>. Can we change the assumption but still keep the same error bound? The answer is YES. It is noted by <a href="http://www.statslab.cam.ac.uk/~rjs57/Biometrika-2015-Yu-315-23.pdf" target="_blank" rel="noopener">this paper</a> that the theorem also holds if we only assume an eigengap on <span class="math inline">\(A\)</span>. The proof is straightforward: without perturbation we have <span class="math inline">\(AU=U\Lambda\)</span>. After perturbation we want to know the residual <span class="math inline">\(L=AV-V\Lambda\)</span>. On the one hand,</p><p><span class="math display">\[L=AV-V\Lambda+HV-HV=V(M-\Lambda)-HV\]</span></p><p>Using <a href="http://i.stanford.edu/pub/cstr/reports/cs/tr/70/150/CS-TR-70-150.pdf" target="_blank" rel="noopener">Wielandt-Hoffman theorem</a> we have</p><p><span class="math display">\[\|L\|_F\le \|V(M-\Lambda)\|_F+\|HV\|_F\le 2\|H\|_F\]</span></p><p>On the other hand, using the eigen-decomposition for <span class="math inline">\(A\)</span>, we have</p><p><span class="math display">\[L=U\Lambda U^*V+U_\perp\Lambda_\perp U_\perp^*V-V\Lambda\]</span></p><p>Note that <span class="math display">\[V\Lambda=(P+P_\perp)V\Lambda=UU^*V\Lambda+U_\perp U_\perp^*V\Lambda\]</span> Thus, <span class="math display">\[L=U(\Lambda U^*V-U^*V\Lambda)+U_\perp(\Lambda_\perp U_\perp^*V-U_\perp^*V\Lambda)\]</span></p><p>Since <span class="math inline">\(U\)</span> and <span class="math inline">\(U_\perp\)</span> are orthogonal, we have</p><p><span class="math display">\[\|L\|_F\ge \|\Lambda_\perp U_\perp^*V-U_\perp^*V\Lambda\|_F\]</span></p><p>Now if we assume</p><blockquote><p><span class="math inline">\(|\lambda_i-\lambda_j|\ge \delta&gt;0\)</span> for all <span class="math inline">\(i=1,\cdots,k\)</span> and <span class="math inline">\(j=k+1,\cdots,n\)</span>.</p></blockquote><p>from the above discussions it holds</p><p><span class="math display">\[\delta \|\sin(\Theta_{U,V})\|_F\le \|L\|_F\le 2\|H\|_F\]</span></p><p>If we are interested in the spectral norm, control <span class="math inline">\(\|L\|_F\)</span> by <span class="math inline">\(2\sqrt{k}\|H\|_{op}\)</span>. So it holds</p><p><span class="math display">\[\|\sin(\Theta_{U,V})\|_F\le \frac{2\min\{\sqrt{k}\|H\|_{op},\|H\|_F\}}{\delta} \]</span></p><p>if <span class="math inline">\(k=1\)</span>, obviously <span class="math inline">\(\|H\|_{op}\le\|H\|_F\)</span>. Hence,</p><p><span class="math display">\[\|\sin(\Theta_{U,V})\|_F\le \frac{2\|H\|_{op}}{\delta}\]</span></p><p>One may also consider the generalization of Davis-Kahan theorem to rectangular matrices. It is not hard if we notice that for any matrix <span class="math inline">\(A\)</span>,</p><p><span class="math display">\[A^*A \text{ and } AA^*\]</span></p><p>are Hermitian matrices, and their eigenvalues are nothing but the square of singular values. The only thing to take care of is that we need to relate</p><p><span class="math display">\[\widehat{A}^*\widehat{A}-A^*A \text{ and } \widehat{A}\widehat{A}^*-AA^*\]</span></p><p>with <span class="math inline">\(\widehat{A}-A=H\)</span>. This is done by the following</p><p><span class="math display">\[\begin{aligned}    \|\widehat{A}^*\widehat{A}-A^*A\|_{op}=&amp;\|(\widehat{A}-A)^*\widehat{A}+A^*(\widehat{A}-A)\|_{op}\\    \le &amp; (\|\widehat{A}\|_{op}+\|A\|_{op})\|\widehat{A}-A\|_{op}\\    \le &amp; (2\|A\|_{op}+\|A-\widehat{A}\|_{op})\|\widehat{A}-A\|_{op}\\    \|\widehat{A}^*\widehat{A}-A^*A\|_F=&amp;\|(\widehat{A}-A)^*\widehat{A}+A^*(\widehat{A}-A)\|_{F}\\    \le &amp; \|(\widehat{A}-A)^*\widehat{A}\|_F+\|A^*(\widehat{A}-A)\|_F\\    =&amp; \|(\widehat{A}^*\otimes I)vec(\widehat{A}-A)\|+\|(I\otimes A^*)vec(\widehat{A}-A)\|\\    \le &amp; (\|\widehat{A}\|_{op}+\|A\|_{op})\|vec(\widehat{A}-A)\|\\    \le &amp; (2\|A\|_{op}+\|A-\widehat{A}\|_{op})\|\widehat{A}-A\|_{F}\\\end{aligned}\]</span></p><p>Alternatively, for any <span class="math inline">\(A\in\mathbb{R}^{m\times n}\)</span>, we can define the <span class="math inline">\((m+n)\times (m+n)\)</span> matrix</p><p><span class="math display">\[\tilde{A}=\left[\begin{array}{cc}    0 &amp; A\\    A^*&amp;0\end{array}\right]\text{ and }\tilde{H}=\left[\begin{array}{cc}    0 &amp; H\\    H^*&amp;0\end{array}\right]\]</span></p><p>If <span class="math inline">\(Av=\sigma u\)</span> Then</p><p><span class="math display">\[\tilde{A}\left[\begin{array}{c}    u\\    v\end{array}\right]=\sigma \left[\begin{array}{c}    u\\    v\end{array}\right]\text{ and }\tilde{A}\left[\begin{array}{c}    u\\    -v\end{array}\right]=-\sigma \left[\begin{array}{c}    u\\    -v\end{array}\right]\]</span></p><p>Therefore, the non-zero eigenvalues of <span class="math inline">\(\tilde{A}\)</span> are the positive and negative of singular values of <span class="math inline">\(A\)</span>, and the eigenvectors are formed by the left and right singular vectors of <span class="math inline">\(A\)</span>. Note that <span class="math inline">\(\tilde{A}\)</span> and <span class="math inline">\(\tilde{H}\)</span> are Hermitian matrices, which implies we can use Davis-Kahan theorem directly. The only thing we should take care of is that we need to relate <span class="math inline">\(\sin(\tilde{\Theta})\)</span> with <span class="math inline">\(\sin(\Theta_u)\)</span> and <span class="math inline">\(\sin(\Theta_v)\)</span>. A typical model is stated as follows.</p><p>Let <span class="math display">\[\tilde{u}=\left[\begin{array}{c}    u_1\\    u_2\end{array}\right]\text{ and }\tilde{v}=\left[\begin{array}{c}    v_1\\    v_2\end{array}\right]\]</span></p><p>Note that <span class="math inline">\(\|u_i\|=\|v_i\|=1\)</span> so <span class="math inline">\(\|\tilde{u}\|=\|\tilde{v}\|=2\)</span>. Then</p><p><span class="math display">\[\cos^2(\tilde{u},\tilde{v})=\frac{1}{4}|\tilde{u}\cdot\tilde{v}|^2\le\frac{1}{2}\cos^2(u_1,v_1)+\frac{1}{2}\cos^2(u_2,v_2)\]</span></p><p>Therefore,</p><p><span class="math display">\[\sin^2(\tilde{u},\tilde{v})\ge \frac{1}{2}\sin^2(u_1,v_1)+\frac{1}{2}\sin^2(u_2,v_2)\]</span></p><p>The first generalization to singular spaces is carried out by <a href="https://www.semanticscholar.org/paper/Perturbation-bounds-in-connection-with-singular-Wedin/133c76f0d580e09de0ca28ff0fb599d5e57321f8" target="_blank" rel="noopener">Wedin</a>. Thus Davis-Kahan theorem is also called Davis-Kahan-Wedin theorem in literature.</p><h2 id="a-final-remark">A Final Remark</h2><p>In Davis and Kahan's <a href="https://epubs.siam.org/doi/10.1137/0707001" target="_blank" rel="noopener">original paper</a>, they actually proved four types of theorems. Except for the <span class="math inline">\(\sin(\Theta)\)</span> theorem, the remaining three are: (1) <span class="math inline">\(\sin(2\Theta)\)</span> theorem; (2) <span class="math inline">\(\tan(\Theta)\)</span> theorem; (3) <span class="math inline">\(\tan(2\Theta)\)</span> theorem. For <span class="math inline">\(2\Theta\)</span> theorems, they allow the principal angle to be close to either 0 or <span class="math inline">\(\pi/2\)</span>. It seems that <span class="math inline">\(2\Theta\)</span> theorems are more general. However, it is rather difficult to find practical applications.</p>]]></content>
      
      
      <categories>
          
          <category> Matrix-Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Perturbation Theory </tag>
            
            <tag> Matrix Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Convergence Rate of Graph Laplacian</title>
      <link href="/2020/12/17/The-Convergence-Rate-of-Graph-Laplacian/"/>
      <url>/2020/12/17/The-Convergence-Rate-of-Graph-Laplacian/</url>
      
        <content type="html"><![CDATA[<p>Amit Singer's paper <a href="https://www.math.pku.edu.cn/teachers/yaoy/Fall2011/Amit06_laplacian.pdf" target="_blank" rel="noopener"><em>From graph to manifold Laplacian: The convergence rate</em></a> presents a very clear proof of the convergence rate of graph Laplacian that everyone can follow. Techniques involved in this proof have become standard in following works in manifold learning. We review the proof in this post.</p><h2 id="setups">Setups</h2><p>Let <span class="math inline">\(x_1,\cdots,x_N\)</span> be i.i.d. samples from uniform distribution on compact submanifold <span class="math inline">\(\mathcal{M}^d\subseteq \mathbb{R}^m\)</span>. Construct the weight matrix <span class="math inline">\(W\)</span> by using heat kernel <span class="math display">\[W_{ij}=\exp\{-\|x_i-x_j\|^2/2\epsilon\}\]</span> Define the degree matrix by <span class="math display">\[D_{ii}=\sum_j W_{ij}\]</span> The graph Laplacian is given by <span class="math display">\[L=D^{-1}W-I\]</span> Let <span class="math inline">\(f:\mathcal{M}\to\mathbb{R}\)</span> be a smooth function and <span class="math inline">\(\Delta_\mathcal{M}\)</span> be the Beltrami-Laplacian operator on <span class="math inline">\(\mathcal{M}\)</span>. The convergence result states that for each <span class="math inline">\(x_i\)</span> we have <span class="math display">\[\frac{1}{\epsilon}\sum_j L_{ij}f(x_j)=\frac{1}{2}\Delta_\mathcal{M}f(x_i)+O(\frac{1}{\sqrt{N\epsilon^{1+d/2}}},\epsilon)\]</span> <span class="math inline">\(\textbf{Remark}:\)</span> In fact the convergence result holds for any <span class="math inline">\(x\in\mathcal{M}\)</span>. However, if we choose <span class="math inline">\(x\)</span> to be a fixed data point, then in the following arguments there will be <span class="math inline">\(N-1\)</span> random data and the summation should exclude <span class="math inline">\(x_i\)</span>. We follow the original paper and denote <span class="math inline">\(x_i\)</span> by <span class="math inline">\(x\)</span>.</p><h2 id="bias">Bias</h2><p>Define <span class="math display">\[\begin{aligned}&amp; F(x_j)=\exp\{-\|x-x_j\|^2/2\epsilon\}f(x_j)\\&amp; G(x_j)=\exp\{-\|x-x_j\|^2/2\epsilon\}\end{aligned}\]</span> The graph Laplacian can be written as <span class="math display">\[(Lf)(x)=\frac{\sum_j F(x_j)}{\sum_j G(x_j)}-f(x)\]</span> As we remarked above, we need to consider <span class="math display">\[\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}\]</span> As <span class="math inline">\(N\)</span> tends infinity, we may expect <span class="math display">\[\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}\to\frac{\mathbb{E}F}{\mathbb{E}G}\]</span> where <span class="math display">\[\begin{aligned}&amp; \mathbb{E}F=\int_{\mathcal{M}} \exp\{-\|x-y\|^2/2\epsilon\}f(y)\frac{\rm{d}y}{vol(\mathcal{M})}\\&amp; \mathbb{E}G=\int_{\mathcal{M}} \exp\{-\|x-y\|^2/2\epsilon\}\frac{\rm{d}y}{vol(\mathcal{M})}\end{aligned}\]</span> The Taylor expansion for <span class="math inline">\(\mathbb{E}F\)</span> is given by <span class="math display">\[\begin{aligned}&amp;\frac{1}{(2\pi \epsilon)^{d/2}}\int_\mathcal{M}\exp\{-\|x-y\|^2/2\epsilon\}f(y){\rm d}y\\=&amp;f(x)+\frac{\epsilon}{2}(E(x)f(x)+\Delta_\mathcal{M}f(x))+O(\epsilon^{2})\end{aligned}\]</span> Therefore, <span class="math display">\[\begin{aligned}\frac{\mathbb{E}F}{\mathbb{E}G}&amp;=\frac{f(x)+\frac{\epsilon}{2}(E(x)f(x)+\Delta_\mathcal{M}f(x))+O(\epsilon^{2})}{1+\frac{\epsilon}{2}E(x)+O(\epsilon^{2})}\\&amp;=f(x)+\frac{\epsilon}{2}\Delta_\mathcal{M}f(x)+O(\epsilon^{2})\end{aligned}\]</span> The bias term is given by <span class="math display">\[\frac{\mathbb{E}F}{\mathbb{E}G}-f(x)=\frac{\epsilon}{2}\Delta_\mathcal{M}f(x)+O(\epsilon^{2})\]</span> <span class="math inline">\(\textbf{Remark}\)</span>: In fact we have decomposed <span class="math inline">\(Lf\)</span> as <span class="math display">\[\begin{aligned}(Lf)(x)=&amp;\frac{\sum_j F(x_j)}{\sum_j G(x_j)}-\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}\\&amp;+\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}\\&amp;+\frac{\mathbb{E}F}{\mathbb{E}G}-f(x)\end{aligned}\]</span> For the first term, a rough estimate is given by <span class="math display">\[\begin{aligned}&amp;\frac{\sum_j F(x_j)}{\sum_j G(x_j)}-\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}\\=&amp;\frac{(\sum_j F(x_j))(\sum_{j\neq i} G(x_j))-(\sum_{j\neq i} F(x_j))(\sum_{j} G(x_j))}{\sum_j G(x_j)\sum_{j\neq i} G(x_j)}\\=&amp;\frac{F(x)(\sum_{j\neq i} G(x_j))-G(x)(\sum_{j\neq i} F(x_j))}{\sum_j G(x_j)\sum_{j\neq i} G(x_j)}\\=&amp;\frac{f(x)(\sum_{j\neq i} G(x_j))-(\sum_{j\neq i} F(x_j))}{\sum_j G(x_j)\sum_{j\neq i} G(x_j)}\\=&amp;\frac{\sum_j F(x_j)}{\sum_j G(x_j)}\frac{f(x)}{\sum_j F(x_j)}-\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}\frac{1}{\sum_j G(x_j)}\\\le&amp; C\frac{1/N}{1/N\sum_j F(x_j)}f(x)-C&#39;\frac{1/N}{1/N\sum_j G(x_j)}\\=&amp; O(\frac{1}{N\epsilon^{d/2}}) \end{aligned}\]</span> where bounds <span class="math inline">\(C\)</span> and <span class="math inline">\(C&#39;\)</span> come from the fact that convergent sequences are bounded. However, as we will see later <span class="math inline">\(O(1/N\epsilon^{d/2})\)</span> is negligible compared to bias and variance.</p><h2 id="variance">Variance</h2><p>For the variance error <span class="math display">\[\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}\]</span> we can only expect it to be small <em>in probability</em> when <span class="math inline">\(N\)</span> tends to infinity.</p><p>Recall that the <a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality" target="_blank" rel="noopener">Chebyshev inequality</a> states that for a random variable <span class="math inline">\(X\)</span> with finite mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> then for any <span class="math inline">\(t&gt;0\)</span> we have <span class="math display">\[\mathbb{P}(|X-\mu|\ge t)\le \frac{\sigma^2}{t^2}\]</span> <span class="math inline">\(\textbf{Remark}\)</span>: In the original paper the author used Chernoff inequality. However, according to the context it seems to be <a href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)" target="_blank" rel="noopener">Bernstein inequality</a>. But in the final statement the author only used variance and missed another term in the denominator. It seems Chebyshev inequality is enough for our use.</p><p>Consider the following probability <span class="math display">\[\begin{aligned}p(N,\alpha)&amp;=\mathbb{P}\left(\left|\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}\right|\ge \alpha\right)\\&amp;=p_+(N,\alpha)+p_-(N,\alpha)\\&amp;=\mathbb{P}\left(\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}\ge \alpha\right)\\&amp;+\mathbb{P}\left(\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}\le -\alpha\right)\end{aligned}\]</span> Since <span class="math inline">\(G(x_j)\)</span> are positive, it is equivalent to <span class="math display">\[\begin{aligned}p(N,\alpha)&amp;=p_+(N,\alpha)+p_-(N,\alpha)\\&amp;=\mathbb{P}\left(\sum_{j\neq i}F(x_j)\mathbb{E}G-G(x_j)\mathbb{E}F-\sum_{j\neq i}\alpha G(x_j)\mathbb{E}G\ge 0\right)\\&amp;+\mathbb{P}\left(\sum_{j\neq i}F(x_j)\mathbb{E}G-G(x_j)\mathbb{E}F+\sum_{j\neq i}\alpha G(x_j)\mathbb{E}G\le 0\right)\end{aligned}\]</span> If we define <span class="math display">\[Y_j=F(x_j)\mathbb{E}G-G(x_j)\mathbb{E}F+\alpha\mathbb{E}G(\mathbb{E}G-G(x_j))\]</span> Then <span class="math inline">\(Y_j\)</span> are i.i.d. samples with zero mean. The probability <span class="math inline">\(p(N,\alpha)\)</span> can be expressed by <span class="math display">\[p(N,\alpha)=\mathbb{P}\left(\left|\sum_{j\neq i} Y_j\right|\ge (N-1)\alpha(\mathbb{E}G)^2\right)\]</span> To apply Chebyshev inequality, we only need to compute <span class="math inline">\(\mathbb{E}Y_j^2\)</span>. Using the expansion from <span class="math inline">\(\mathbb{E}F\)</span> we have <span class="math display">\[\begin{aligned}&amp; \mathbb{E}F=\frac{(2\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(f(x)+\frac{\epsilon}{2}(E(x)f(x)+\Delta_\mathcal{M}f(x))+O(\epsilon^2)\right)\\&amp; \mathbb{E}G=\frac{(2\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(1+\frac{\epsilon}{2} E(x)+O(\epsilon^2)\right)\\&amp; \mathbb{E}F^2=\frac{(\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(f^2(x)+\frac{\epsilon}{4}(E(x)f^2(x)+\Delta_\mathcal{M}f^2(x))+O(\epsilon^2)\right)\\&amp; \mathbb{E}G^2=\frac{(\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(1+\frac{\epsilon}{4} E(x)+O(\epsilon^2)\right)\\&amp; \mathbb{E}FG=\frac{(\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(f(x)+\frac{\epsilon}{4}(E(x)f(x)+\Delta_\mathcal{M}f(x))+O(\epsilon^2)\right)\end{aligned}\]</span> Note that in the derivation of bias term, the coefficient before <span class="math inline">\(\Delta_\mathcal{M}f\)</span> is <span class="math inline">\(\epsilon/2\)</span>. Therefore, we should focus on <span class="math inline">\(\alpha\ll \epsilon\)</span>. Neglecting <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\alpha^2\)</span> term, we have <span class="math display">\[\mathbb{E}Y_j^2=\frac{2^d(\pi\epsilon)^{3d/2}}{vol(\mathcal{M})^3}\frac{\epsilon}{2}(\|\nabla_\mathcal{M} f\|^2+O(\epsilon))\]</span> Substituting into Chebyshev inequality we have <span class="math display">\[\begin{aligned}&amp;p(N,\alpha)\\\le&amp;\frac{\mathbb{E}Y_j^2}{(N-1)\alpha^2(\mathbb{E}G)^4}\\=&amp;\frac{\frac{2^d(\pi\epsilon)^{3d/2}}{vol(\mathcal{M})^3}\frac{\epsilon}{2}(\|\nabla_\mathcal{M} f\|^2+O(\epsilon))}{(N-1)\alpha^2\frac{(2\pi\epsilon)^{2d}}{vol(\mathcal{M})^4}\left(1+\frac{\epsilon}{2} E(x)+O(\epsilon^2)\right)^4}\\=&amp;\frac{vol(\mathcal{M})\epsilon(\|\nabla_\mathcal{M} f\|^2+O(\epsilon))}{2^{d+1}(\pi\epsilon)^{d/2}(N-1)\alpha^2\left(1+\frac{\epsilon}{2} E(x)+O(\epsilon^2)\right)^4}\end{aligned}\]</span> If we take <span class="math display">\[\alpha\approx\sqrt{\frac{vol(\mathcal{M})\epsilon(\|\nabla_\mathcal{M} f\|^2+O(\epsilon))}{2^{d+1}(\pi\epsilon)^{d/2}(N-1)}}\]</span> Thus the noise error is <span class="math display">\[\frac{\alpha}{\epsilon}=O(\frac{1}{\sqrt{N\epsilon^{1+d/2}}})\]</span></p>]]></content>
      
      
      <categories>
          
          <category> Manifold-Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Asymptotic Analysis </tag>
            
            <tag> Spectral Graph Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Two Applications of the Weingarten Map</title>
      <link href="/2020/07/11/Two-Applications-of-the-Weingarten-Map/"/>
      <url>/2020/07/11/Two-Applications-of-the-Weingarten-Map/</url>
      
        <content type="html"><![CDATA[<p>If <span class="math inline">\(\mathcal{M}\)</span> is an Euclidean submanifold, many intrinsic quantities can be expressed using information from the ambient Euclidean space. Here are two examples where the Weingarten map plays an important role: first is the Riemannian hessian of a smooth map, and second is the Ricci curvature tensor. Assume that <span class="math inline">\(f:\mathcal{M}\to\mathbb{R}\)</span> is a smooth function and <span class="math inline">\(\widehat{f}:\mathbb{R}^d\to\mathbb{R}\)</span> is a smooth extension of <span class="math inline">\(f\)</span>. In data science, it is much more convenient to compute the gradient or hessian of <span class="math inline">\(\widehat{f}\)</span> than those of <span class="math inline">\(f\)</span>. <a href="https://link.springer.com/chapter/10.1007/978-3-642-40020-9_39" target="_blank" rel="noopener">Absil etc.</a> presented the relations and applied them in Riemannian optimization. On the other hand, Ricci curvature is also a hot topic in manifold learning. An old paper of <a href="https://www.jstor.org/stable/2034680" target="_blank" rel="noopener">Hicks</a> proves that there is a simple relation between Ricci map and Weingarten map if the manifold is a hypersurface. Many problems concerning Ricci curvature thus descend to the estimation of the Weingarten map.</p><h3 id="riemannian-hessian">Riemannian Hessian</h3><p>Let <span class="math inline">\(\mathcal{M}\)</span> be an <span class="math inline">\(m\)</span>-dimensional submanifold in the <span class="math inline">\(d\)</span>-dimensional Euclidean space <span class="math inline">\(\mathbb{R}^d\)</span>. Let <span class="math inline">\(\pi_x:\mathbb{R}^d\to T_x\mathcal{M}\)</span> be the orthogonal projection to the tangent space <span class="math inline">\(T_x\mathcal{M}\)</span>. It turns out that the gradient of <span class="math inline">\(f\)</span> is nothing but the projection of gradient of <span class="math inline">\(\widehat{f}\)</span>. Denote <span class="math inline">\(\partial\widehat{f}\)</span> to be the usual gradient for <span class="math inline">\(\widehat{f}\)</span> and <span class="math inline">\(\text{grad}(f)\)</span> to be the Riemannian gradient for <span class="math inline">\(f\)</span>. Then we have</p><blockquote><p><span class="math inline">\(\text{grad}(f)_x=\pi_x(\partial\widehat{f}_x)\)</span></p></blockquote><p>To check this equality, note that for any tangent vector <span class="math inline">\(v\in T_x\mathcal{M}\)</span> and smooth curve <span class="math inline">\(\gamma\)</span> in <span class="math inline">\(\mathcal{M}\)</span> with <span class="math inline">\(\gamma(0)=x\)</span> and <span class="math inline">\(\gamma&#39;(0)=v\)</span> we have <span class="math display">\[\langle v,\text{grad}(f)_x\rangle=v(f)=\frac{\rm d}{\rm dt}|_0f(\gamma(t))=\frac{\rm d}{\rm dt}|_0\widehat{f}(\gamma(t))=\langle v,\pi_x(\partial\widehat{f}_x)\rangle\]</span> Let <span class="math inline">\(\nabla\)</span> be the Riemannian connection on <span class="math inline">\(\mathcal{M}\)</span>. For any vector fields <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, by definition, <span class="math display">\[\nabla_XY=\pi(\partial_XY)\]</span> Define the Riemannian hessian of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span> by <span class="math display">\[\text{Hess}f_x(v)=\nabla_v\text{grad}(f)\]</span> It is easy to check that <span class="math display">\[\langle\text{Hess}f_x(v),w\rangle=v(w(f))-\langle\nabla_vw,\text{grad}(f)\rangle=\langle\text{Hess}f_x(w),v\rangle\]</span> Thus, <span class="math inline">\(\text{Hess}f_x:T_x\mathcal{M}\to T_x\mathcal{M}\)</span> is a self-adjoint operator. However, the original definition is difficult to use in practice. Further computation shows that <span class="math display">\[\text{Hess}f_x(v)=\pi_x\partial_v(\pi\partial\widehat{f})=\pi_x((\partial_v\pi)\partial\widehat{f}_x+\partial^2\widehat{f}_x(v))\]</span> where we have used the product rule. If we understand <span class="math inline">\(\pi\)</span> as matrices then <span class="math inline">\(\partial_v\pi\)</span> contains elements which are derivatives of those in <span class="math inline">\(\pi\)</span>. From the result we see that Riemannian hessian of <span class="math inline">\(f\)</span> differs from the projection of usual hessian of <span class="math inline">\(\widehat{f}\)</span> by a term involving the covariant derivative of <span class="math inline">\(\pi\)</span>. Let <span class="math inline">\(\pi^\perp:\mathbb{R}^d\to T^\perp_x\mathcal{M}\)</span> be the projection operator to the normal space at <span class="math inline">\(x\)</span>. Using the identity <span class="math inline">\(\text{id}=\pi+\pi^\perp\)</span>, we have <span class="math display">\[\pi_x\partial_v\pi=-\pi_x\partial_v\pi^\perp\]</span> In addition, since <span class="math inline">\(\pi\pi^\perp=0\)</span>, we have <span class="math display">\[0=\pi_x\partial_v(\pi\pi^\perp)=\pi_x(\partial_v\pi^\perp)_x+\pi_x(\partial_v\pi)_x\pi^\perp_x\]</span> which implies <span class="math inline">\(\pi_x(\partial_v\pi^\perp)_x\pi_x=0\)</span> and <span class="math inline">\(\pi_x(\partial_v\pi)\partial\widehat{f}_x=-\pi_x(\partial_v\pi^\perp)_x\pi_x^\perp\partial\widehat{f}_x\)</span>.</p><p>Recall that the Weingarten map at <span class="math inline">\(x\)</span> for <span class="math inline">\(\xi\in T^\perp_x\mathcal{M}\)</span> is defined by <span class="math inline">\(A_{x,\xi}(v)=-\pi_x\partial_v\tilde{\xi}\)</span> where <span class="math inline">\(v\in T_x\mathcal{M}\)</span> and <span class="math inline">\(\tilde{\xi}\)</span> is any extension of <span class="math inline">\(\xi\)</span>. The identity implies that <span class="math display">\[A_{x,\xi}(v)=-\pi_x\partial_v(\pi^\perp\tilde{\xi})=-\pi_x(\partial_v\pi^\perp)_x\xi-\pi_x\pi_x^\perp\partial_v\tilde{\xi}=-\pi_x(\partial_v\pi^\perp)_x\xi\]</span> Utilizing the above results we have <span class="math display">\[\text{Hess}f_x(v)=\pi_x\partial^2\widehat{f}_x(v)+A_{x,\pi_x^\perp\partial\widehat{f}_x}(v)\]</span></p><h3 id="ricci-map">Ricci Map</h3><p>Suppose <span class="math inline">\(\mathcal{M}\)</span> is a <strong>hypersurface</strong> in Euclidean space. i.e. <span class="math inline">\(\mathcal{M}\)</span> is of codimension <span class="math inline">\(1\)</span>. Let <span class="math inline">\(\xi\)</span> be a local unit normal vector field. The second fundamental form can be simplified as <span class="math display">\[h(X,Y)=\partial_XY-\nabla_XY=\langle\partial_XY,\xi\rangle\xi=\langle Y,A_\xi(X)\rangle\xi\]</span> where we have used the identity <span class="math inline">\(X\langle Y,\xi\rangle=\langle\partial_XY,\xi\rangle+\langle Y,\partial_X\xi\rangle\)</span> and the definition of Weingarten map. Recall that for Euclidean submanifolds Gauss equation reads <span class="math display">\[0=\tilde{R}(X,Y)Z=R(X,Y)Z-A_{h(Y,Z)}X+A_{h(X,Z)}Y\]</span> Substituting the expression of the second fundamental form into above, we have <span class="math display">\[R(X,Y)Z=\langle A_\xi(Y),Z\rangle A_\xi(X)-\langle A_\xi(X),Z\rangle A_\xi(Y)\]</span> Let <span class="math inline">\(Z_1,\cdots,Z_m\)</span> be an orthonormal base of principal vectors of <span class="math inline">\(A_\xi\)</span>. i.e. <span class="math inline">\(A_\xi(Z_i)=k_iZ_i\)</span> where <span class="math inline">\(k_i\)</span>'s are principal curvatures. Let <span class="math inline">\(H=\sum k_i\)</span> be the mean curvature and define <span class="math inline">\(Ric:T_x\mathcal{M}\to T_x\mathcal{M}\)</span> by <span class="math inline">\(Ric(X)=\sum_{i=1}^m R(X,Z_i)Z_i\)</span>, called <strong>Ricci map</strong>. Then we have <span class="math display">\[\begin{aligned}Ric(X)&amp;=\sum_{i=1}^mR(X,Z_i)Z_i\\&amp;=\sum_{i=1}^m\langle A_\xi(Z_i),Z_i\rangle A_\xi(X)-\langle A_\xi(X),Z_i\rangle A_\xi(Z_i)\\&amp;=\sum_{i=1}^m\langle k_iZ_i,Z_i\rangle A_\xi(X)-\langle A_\xi(X),Z_i\rangle k_iZ_i\\&amp;=H A_\xi(X)-\sum_{i=1}^m\langle A_\xi(X),A_\xi(Z_i)\rangle Z_i\\&amp;=H A_\xi(X)-\sum_{i=1}^m\langle A_\xi^2(X),Z_i\rangle Z_i\\&amp;=H A_\xi(X)-A_\xi^2(X)\end{aligned}\]</span> Therefore we have proved the following identity relating the Ricci map with Weingarten map <span class="math display">\[A_\xi^2-HA_\xi+Ric=0\]</span> An obvious corollary is that every principal vector is also a Ricci vector. i.e. <span class="math inline">\(Ric\)</span> has the same eigenvectors as <span class="math inline">\(A_\xi\)</span>.</p><p><strong>Example.</strong> Let <span class="math inline">\(\mathcal{M}\)</span> be a surface in the three space. The characteristic equation for Weingarten map is <span class="math inline">\(A^2-HA+KI=0\)</span> where <span class="math inline">\(K\)</span> is Gauss curvature. Comparing both equations we have <span class="math inline">\(Ric=KI\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Local PCA with Gaussian Noise</title>
      <link href="/2020/07/08/Local-PCA-with-Gaussian-Noise/"/>
      <url>/2020/07/08/Local-PCA-with-Gaussian-Noise/</url>
      
        <content type="html"><![CDATA[<p>In noiseless case we have a clean result for the convergence rate about tangent space estimation via local PCA ( see <a href="http://yueqicao.top/2020/05/21/Tangent-Space-Estimation-Using-Local-PCA/">this post</a> ). However, for noisy data the analysis is much more involved, because noise will cause points in the local neighborhood to exit, and points from the outside to enter. The integration in normal patch no more holds due to loss of true location. People proposed many practical approaches to do local PCA with the existence of noise, but few provided with a comprehensive theory. There is one method which covers both sides, called Multiscale SVD. The details are complex and can be found in this <a href="http://dspace.mit.edu/handle/1721.1/72597" target="_blank" rel="noopener">technical report</a>, while the intuition is easy to explain and help us gain insights into other problems in manifold learning.</p><p>Let <span class="math inline">\(\mathcal{M}\subseteq\mathbb{R}^d\)</span> be an <span class="math inline">\(m\)</span> dimensional submanifold. Assume <span class="math inline">\(X\)</span> is a random vector valued in <span class="math inline">\(\mathcal{M}\)</span> with smooth density with respect to the volume measure. Let <span class="math inline">\(\eta\)</span> be a Gaussian random vector independent of <span class="math inline">\(X\)</span>, with zero mean and isotropic covariance, i.e. <span class="math inline">\(\eta\sim\mathcal{N}(0,\sigma^2I_{d\times d})\)</span>. Consider the new random vector <span class="math inline">\(Y=X+\eta\)</span>. Then the i.i.d samples from <span class="math inline">\(Y\)</span> are our noisy data. In noiseless case we expect to estimate the tangent space at a fixed point <span class="math inline">\(x\)</span> from nearby samples with identical distribution as <span class="math inline">\(X\)</span>. For noisy data, however, a fixed point <span class="math inline">\(x\)</span> on the manifold does not make sense because of noise. Instead, we are only accessible to the perturbed point <span class="math inline">\(\tilde{x}=x+\eta\)</span>, and the nearby samples are selected with respect to <span class="math inline">\(\tilde{x}\)</span>. The bandwidth, i.e. the radius of neighborhood, should be determined according to three factors:</p><ul><li><em>bias</em>: the bandwidth cannot be too large, or the nonlinearity coming from curvature will lead to terrible bias;</li><li><em>variance</em>: the bandwidth cannot be too small, or insufficient points will cause large variance due to sampling;</li><li><em>noise</em>: the bandwidth cannot be too small, or the error will be completely random due to noise.</li></ul><p>In variance though the bandwidth is required not to be 'small', as the number of sampling increases the bandwidth does approach zero in noiseless case. This never happens when we take noise into consideration. When the bandwidth is much smaller than the magnitude of noise, the error is controlled by noise and independent to our change of bandwidth. Thus, we cannot expect a convergence by letting the bandwidth tend to zero. Accordingly, the multiscale SVD suggests us to look at the bandwidth varying in a suitable interval. This method especially works in intrinsic dimension estimation, where we need to investigate the eigenvalues of local covariance matrix. During this suitable interval, principal eigenvalues change differently from those coming from noise. Therefore, we are able to count these eigenvalues and estimate the dimension.</p><figure><img src="/2020/07/08/Local-PCA-with-Gaussian-Noise/mpca.jpg" alt="mpca"><figcaption aria-hidden="true">mpca</figcaption></figure><p>To understand the trade-off among bias, variance and noise, it is convenient for us to consider the following virtual case (which is also used in the analysis of multiscale SVD): suppose we first locate the nearby points and then add noise. That is, we know the true samples <span class="math inline">\(X\)</span> before corruption. Let <span class="math inline">\(K\)</span> be a differentiable function supported on <span class="math inline">\([0,1]\)</span> and denote <span class="math inline">\(K_h(u)=1/h^mK(\|u\|/h)\)</span> for <span class="math inline">\(u\in\mathbb{R}^d\)</span>. Without loss of generality, assume <span class="math inline">\(x\)</span> is placed at the origin and <span class="math inline">\(T_x\mathcal{M}\)</span> coincides with the coordinate space spanned by the first <span class="math inline">\(m\)</span> standard basis <span class="math inline">\(e_1,\cdots,e_m\)</span>. Then the population covariance matrix is defined as <span class="math display">\[\begin{aligned}\mathbb{H}&amp;=\mathbb{E}[YY^tK_h(X)]\\&amp;=\mathbb{E}[XX^tK_h(X)]+\mathbb{E}[X\eta^tK_h(X)]+\mathbb{E}[\eta X^tK_h(X)]+\mathbb{E}[\eta\eta^tK_h(X)]\end{aligned}\]</span></p><p>The first matrix is known to have the following decomposition <span class="math display">\[\mathbb{E}[XX^tK_h(X)]=\begin{bmatrix}ch^2I+O(h^4)&amp; 0\\0&amp;0\end{bmatrix}+\begin{bmatrix}0&amp;O(h^4)\\ O(h^4)&amp;O(h^4)\end{bmatrix}\]</span> For the second and third matrix, using the independence of <span class="math inline">\(X\)</span> and <span class="math inline">\(\eta\)</span>, they vanish in the sum. For the fourth term, using geodesic coordinates, we have <span class="math display">\[\begin{aligned}\mathbb{E}[\eta\eta^tK_h(X)]=c\sigma^2I\end{aligned}\]</span> Therefore, the bias consists of two parts: the curvature of the manifold and noise. While we can eliminate the impact of curvature by choosing smaller and smaller neighborhoods, the impact of noise always exists.</p><p>Now let <span class="math inline">\(Y_1,Y_2,\cdots,Y_n\)</span> be i.i.d. samples. By substituting the expectation by empirical mean, we obtain the empirical covariance matrix</p><p><span class="math display">\[\begin{aligned}H&amp;=\frac{1}{n}\sum_{i=1}^nY_iY_i^tK_h(X_i)\\&amp;=\frac{1}{n}\sum_{i=1}^nX_iX_i^tK_h(X_i)+\frac{1}{n}\sum_{i=1}^nX_i\eta_i^tK_h(X_i)+\frac{1}{n}\sum_{i=1}^n\eta_i X_i^tK_h(X_i)+\frac{1}{n}\sum_{i=1}^n\eta_i\eta_i^tK_h(X_i)\end{aligned}\]</span> Note that <span class="math inline">\(\mathbb{E}[H]=\mathbb{H}\)</span>. For an arbitrary element in <span class="math inline">\(H\)</span>, the variance is given by <span class="math display">\[\mathbb{E}[(\frac{1}{n}\sum_{i=1}^nY_i^kY_i^lK_h(X_i)-\mathbb{E}[Y^kY^lK_h(X)])^2]=\frac{1}{n}\mathbb{Var}(Y^kY^lK_h(X))\]</span> It suffices to estimate the second moment terms, <span class="math display">\[\begin{aligned}\mathbb{E}[(Y^k)^2(Y^l)^2K^2_h(X)]=&amp;\mathbb{E}[((X^k)^2+2X^k\eta^k+(\eta^k)^2)((X^l)^2+2X^l\eta^l+(\eta^l)^2)K^2_h(X)]\\=&amp;\mathbb{E}[\bigg((X^kX^l)^2+2(X^k)X^l\eta^l+(X^k\eta^l)^2+2X^k(X^l)^2\eta^k\\&amp;+4X^kX^l\eta^k\eta^l+2X^k\eta^k(\eta^l)^2+(\eta^kX^l)^2+2X^l\eta^l(\eta^k)^2\\&amp;+(\eta^k\eta^l)^2\bigg)K^2_h(X)]\\=&amp;\mathbb{E}[(X^kX^l)^2K^2_h(X)]+\mathbb{E}[(X^k\eta^l)^2K^2_h(X)]\\&amp;+4\mathbb{E}[X^kX^lK^2_h(X)]\mathbb{E}[\eta^k\eta^l]+\mathbb{E}[(\eta^kX^l)^2K^2_h(X)]\\&amp;+\mathbb{E}[(\eta^k\eta^l)^2K^2_h(X)]\end{aligned}\]</span> The last equality holds by independence and symmetry. The order is then given by <span class="math display">\[\mathbb{E}[(Y^kY^l)^2K^2_h(X)]=\left\{\begin{array}{l}\frac{1}{h^m}(h^4c_1+2h^2\sigma^2c_2+4\delta_{kl}h^2\sigma^2c_2+\sigma^4c_3),\quad k,l=1,2,\cdots,m\\\frac{1}{h^m}(h^8c_1+2h^4\sigma^2c_2+4\delta_{kl}h^4\sigma^2c_2+\sigma^4c_3),\quad k,l=m+1,\cdots,d\\\frac{1}{h^m}(h^6c_1+h^2\sigma^2c_2+h^4\sigma^2c_2+\sigma^4c_3),\quad \text{otherwise}\end{array}\right.\]</span> Thus with high probability, <span class="math display">\[H=\begin{bmatrix}ch^2I+O(h^4)+O(\frac{h^2+\sigma^2}{\sqrt{nh^m}})&amp; 0\\0&amp;0\end{bmatrix}+\begin{bmatrix}0&amp;O(h^4)\\ O(h^4)&amp;O(h^4+\sigma^2)\end{bmatrix}+\frac{1}{\sqrt{nh^m}}\begin{bmatrix}0&amp;O((h^2+\sigma^2)^{3/2})\\O((h^2+\sigma^2)^{3/2})&amp;O(h^4+\sigma^2) \end{bmatrix}\]</span> If <span class="math inline">\(\sigma \ll h^2\)</span>, then the error is mainly controlled by bandwidth and the noise is relatively negligible. If <span class="math inline">\(\sigma\gg h^2\)</span>, the error largely depends on noise. Define <span class="math inline">\(\gamma=\sigma/h^2\)</span>. Then <span class="math display">\[\frac{1}{h^2}H=\begin{bmatrix}cI+O(h^2)+O(\frac{1+\sigma\gamma}{\sqrt{nh^m}})&amp; 0\\0&amp;0\end{bmatrix}+\begin{bmatrix}0&amp;O(h^2)\\ O(h^2)&amp;O(\sigma\gamma)\end{bmatrix}+\frac{1}{\sqrt{nh^m}}\begin{bmatrix}0&amp;O(\sigma^2\gamma)\\O(\sigma^2\gamma)&amp;O(\sigma\gamma) \end{bmatrix}\]</span> If <span class="math inline">\(\sigma\ll \sqrt{nh^m}\)</span>, then the error is at least <span class="math inline">\(O(\sigma\gamma)\)</span> and one cannot get better result using local PCA. If <span class="math inline">\(\sigma\gg \sqrt{nh^m}\)</span>, then the situation is terrible with large noise and inadequate sampling, with error in the order <span class="math inline">\(O(\sigma^2\gamma/\sqrt{nh^m})\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> Manifold-Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
            <tag> Asymptotic Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tangent Space Estimation Using Local Polynomial Fitting</title>
      <link href="/2020/05/30/Tangent-Space-Estimation-Using-Local-Polynomial-Fitting/"/>
      <url>/2020/05/30/Tangent-Space-Estimation-Using-Local-Polynomial-Fitting/</url>
      
        <content type="html"><![CDATA[<p>Generalizations of local PCA to tangent space estimation are considered in various papers. E. Aamari and C. Levrard considered the non-asymptotic rates of local PCA in a <a href="https://arxiv.org/pdf/1512.02857.pdf" target="_blank" rel="noopener">paper</a> of manifold reconstruction. Suppose <span class="math inline">\(n\)</span> is the number of points and <span class="math inline">\(m\)</span> is the manifold dimension. Then with bandwidth chosen to be <span class="math inline">\(O(\log(n)/n)^{1/m}\)</span> the minimax rate is <span class="math inline">\(O(1/n)^{1/m}\)</span>. In their <a href="https://arxiv.org/pdf/1705.00989.pdf" target="_blank" rel="noopener">subsequent work</a>, they considered generalization using local polynomial fitting. It happens that by involving more higher terms the convergence is faster. Suppose the fitting is up to <span class="math inline">\(k\)</span>th term, then with the same bandwidth the minimax rate is <span class="math inline">\(O(1/n)^{k/m}\)</span>. However, they did not show practical algorithms. In an <a href="http://www.cse.ust.hk/~scheng/pub/j5.pdf" target="_blank" rel="noopener">early paper</a> of S.-W. Cheng and M.-K. Chiu, they proposed an algorithm using polynomial fitting up to the second order, but their analysis did not show consistency with the above result. However, their method is practical and its principle is easy to understand. We present a simple description in the following.</p><p>Let <span class="math inline">\(\mathcal{M}\subseteq \mathbb{R}^{m+1}\)</span> be a hypersurface. Suppose <span class="math inline">\(x\in\mathcal{M}\)</span> is the origin and we want to estimate <span class="math inline">\(T_x\mathcal{M}\)</span>. Firstly for the simplest case where <span class="math inline">\(T_x\mathcal{M}\)</span> coincides with the hyperplane spanned by the standard vectors <span class="math inline">\(e_1,\cdots,e_m\)</span> in <span class="math inline">\(\mathbb{R}^{m+1}\)</span>, we have the local parametrization <span class="math display">\[\mathbf{r}(u^1,\cdots,u^m)=(u^1,\cdots,u^m,f(u^1,\cdots,u^m))\]</span> By definition we deduce that <span class="math display">\[f(0)=0,\, Df(0)=0\]</span> Thus locally <span class="math inline">\(\mathcal{M}\)</span> is the level set of the function <span class="math display">\[F(\mathbf{u})=u^{m+1}-f(u^1,\cdots,u^m)=u^{m+1}-\frac{1}{2}[u^1,\cdots,u^m]D^2f(0)[u^1,\cdots,u^m]^t+O(\|\mathbf{u}\|^3)\]</span> We write the quadratic form in <span class="math inline">\(F(\mathbf{u})\)</span> as the product of two vectors <span class="math display">\[\begin{aligned}&amp;[\frac{1}{\sqrt{2}}(u^1)^2,u^1u^2,\cdots,u^1u^m,u^1u^{m+1},\frac{1}{\sqrt{2}}(u^2)^2,\cdots,u^2u^m,u^2u^{m+1},\cdots,\frac{1}{\sqrt{2}}(u^{m+1})^2]\cdot\\&amp;[\frac{1}{\sqrt{2}}\partial_1^2f(0),\partial_1\partial_2f(0),\cdots,\partial_1\partial_mf(0),0,\frac{1}{\sqrt{2}}\partial_2^2f(0),\cdots,\partial_2\partial_mf(0),0,\cdots,0]\end{aligned}\]</span> Define <span class="math inline">\(q:\mathbb{R}^{m+1}\to\mathbb{R}^{(m+1)m/2}\)</span> by <span class="math display">\[q(\mathbf{u})=[\frac{1}{\sqrt{2}}(u^1)^2,u^1u^2,\cdots,u^1u^{m+1},\frac{1}{\sqrt{2}}(u^2)^2,\cdots,u^2u^{m+1},\cdots,\frac{1}{\sqrt{2}}(u^{m+1})^2]^t\]</span> Denote the vectorized form of <span class="math inline">\(D^2f(0)\)</span> by <span class="math inline">\(Q_{m+1}\)</span>. Then <span class="math display">\[F(\mathbf{u})=\mathbf{u}^te_{m+1}-q(\mathbf{u})^tQ_{m+1}+O(\|\mathbf{u}\|^3)\]</span> In general, let <span class="math inline">\(T_x\mathcal{M}=\text{span}\{\hat{e}_1,\cdots,\hat{e}_m\}\)</span> and <span class="math inline">\([\hat{e}_1,\cdots,\hat{e}_m,\hat{e}_{m+1}]=P[e_1,\cdots,e_m,e_{m+1}]\)</span>. Let <span class="math inline">\(\mathbf{u}=\sum_{i=1}^{m+1}u^ie_i=\sum_{i=1}^{m+1}\hat{u}^i\hat{e}_i\)</span>. As in above <span class="math inline">\(F\)</span> should be a function of <span class="math inline">\(\hat{u}^1,\cdots,\hat{u}^{m+1}\)</span>, here we still express <span class="math inline">\(F\)</span> with respect to coordinate components <span class="math inline">\(u^1,\cdots,u^{m+1}\)</span>. <span class="math display">\[\begin{aligned}F(\mathbf{u})&amp;=\mathbf{u}^t\hat{e}_{m+1}-q(\mathbf{u})^tq(P)Q_{m+1}+O(\|\mathbf{u}\|^3)\\&amp;=\mathbf{u}^tPe_{m+1}-q(\mathbf{u})^tq(P)Q_{m+1}+O(\|\mathbf{u}\|^3)\\&amp;=[\mathbf{u},q(\mathbf{u})]^t\left[\begin{array}{c}Pe_{m+1}\\ q(P)Q_{m+1}\end{array}\right]+O(\|\mathbf{u}\|^3)\end{aligned}\]</span> where <span class="math inline">\(q(P)\)</span> is an orthogonal matrix in <span class="math inline">\(\mathbb{R}^{(m+1)m/2}\)</span>.</p><p>For Euclidean submanifolds <span class="math inline">\(\mathcal{M} \subseteq\mathbb{R}^{d}\)</span>, it is similar since we have <span class="math inline">\(l=d-m\)</span> functions <span class="math inline">\(F_{m+1},\cdots,F_d\)</span> as above. That is, <span class="math display">\[F_\alpha(\mathbf{u})=[\mathbf{u},q(\mathbf{u})]^t\left[\begin{array}{c}Pe_{\alpha}\\ q(P)Q_{\alpha}\end{array}\right]+O(\|\mathbf{u}\|^3)\]</span> for <span class="math inline">\(\alpha=m+1,\cdots,d\)</span>. Now suppose we have <span class="math inline">\(n\)</span> samples <span class="math inline">\(X_1,\cdots,X_n\)</span> from some distribution on the manifold. It is natural to minimize the following error <span class="math display">\[\frac{1}{n}\sum_{\alpha=m+1}^d\sum_{i=1}^n\left([X_i,q(X_i)]^t\left[\begin{array}{c}Pe_{\alpha}\\ q(P)Q_{\alpha}\end{array}\right]\right)^2\]</span> If we set <span class="math display">\[\begin{aligned}&amp;A=\left[\begin{array}{ccc}X_{11}&amp;\cdots&amp;X_{1d}\\ \vdots&amp;\ddots&amp;\vdots\\ X_{n1}&amp;\cdots&amp; X_{nd}\end{array}\right]\\&amp;B=\left[\begin{array}{cccccc}\frac{1}{\sqrt{2}}X^2_{11}&amp;X_{11}X_{12}&amp;\cdots&amp;X_{11}X_{1d}&amp;\cdots&amp;\frac{1}{\sqrt{2}}X_{1d}^2\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\ddots&amp;\vdots\\ \frac{1}{\sqrt{2}}X^2_{n1}&amp;X_{n1}X_{n2}&amp;\cdots&amp;X_{n1}X_{nd}&amp;\cdots&amp;\frac{1}{\sqrt{2}}X_{nd}^2\end{array}\right]\\&amp;\left[\begin{array}{c}Pe_{\alpha}\\ q(P)Q_{\alpha}\end{array}\right]=\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]\end{aligned}\]</span> it is equivalent to minimize <span class="math display">\[\frac{1}{n}\sum_{\alpha=m+1}^d\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]^t[A,B]^t[A,B]\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]\]</span> Note the dimension of eigenspace of <span class="math inline">\([A B]^t[A B]\)</span> corresponding to <span class="math inline">\(0\)</span> can be far larger than that of normal space. Hence it works for hypersurfaces but becomes intractable for general submanifolds. Furthermore, if one writes down the asymptotic order with respect to bandwidth it is easy to notice that in fact it does not exceed local PCA. To work out a method faster than local PCA needs further consideration.</p><p>Consider the case when the number of samples is less than <span class="math inline">\(d(d+1)/2\)</span>. we add the penalty term <span class="math display">\[\sum_{\alpha=m+1}^d(\frac{1}{n}\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]^t[A,B]^t[A,B]\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]+\gamma\|c_\alpha\|^2)\]</span> Then the objective function becomes <span class="math display">\[\sum_{\alpha=m+1}^d\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]^tZ\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]\]</span> where <span class="math display">\[Z=\left[\begin{array}{cc}\frac{1}{n}A^tA&amp;\frac{1}{n}A^tB\\\frac{1}{n}A^tB&amp;\frac{1}{n}B^tB+\gamma I\end{array}\right]=\left[\begin{array}{cc}I&amp;H_\gamma\\0&amp;I\end{array}\right]\left[\begin{array}{cc}W_\gamma&amp;0\\0&amp;U_\gamma\end{array}\right]\left[\begin{array}{cc}I&amp;0\\H_\gamma&amp;I\end{array}\right]\]</span> and <span class="math display">\[U_\gamma=\frac{1}{n}B^tB+\gamma I,\, H_\gamma=\frac{1}{n}A^tBU_\gamma^{-1},\, W_\gamma=\frac{1}{n}A^tA-H_\gamma U_\gamma H_\gamma^t\]</span> Therefore, we have <span class="math display">\[(10)=\sum_{\alpha=m+1}^dg_\alpha^tW_\gamma g_\alpha+(c_\alpha+H_\gamma^tg_\alpha)^tU_\gamma(c_\alpha+H^t_\gamma g_\alpha)\]</span> Note that <span class="math inline">\(U_\gamma\)</span> is positive definite for <span class="math inline">\(\gamma&gt;0\)</span>. Hence the minimum is achieved if <span class="math inline">\(c_\alpha+H^t_\gamma g_\alpha=0\)</span>. Let <span class="math inline">\(B=LSR\)</span> be the SVD decomposition of <span class="math inline">\(B\)</span>. Then <span class="math display">\[U_\gamma=R^t(\frac{1}{n}S^2+\gamma I)R\]</span> and <span class="math display">\[\frac{1}{n}B(U_\gamma^{-1})^tB^t=\frac{1}{n}LS(\frac{1}{n}S^2+\gamma I)^{-1}SL^t\]</span> Substituting into the expression of <span class="math inline">\(W_\gamma\)</span>, we see that <span class="math display">\[W_\gamma=\frac{1}{n}A^t(I-\frac{1}{n}B(U_\gamma^{-1})^tB^t)A=\frac{1}{n}A^tL(I-\frac{1}{n}S(\frac{1}{n}S^2+\gamma I)^{-1}S)L^tA\]</span> Note that the middle term is a diagonal matrix. Write <span class="math inline">\(W_\gamma=\gamma A^tL\Sigma_\gamma L^tA\)</span>. For an arbitrary element <span class="math inline">\(\lambda\in S\)</span> it becomes <span class="math inline">\(1/(\lambda^2+n\gamma)\)</span> in <span class="math inline">\(\Sigma_\gamma\)</span>. Therefore, the solution is given by the eigenvectors of <span class="math inline">\(W_\gamma\)</span> corresponding to the smallest eigenvalue.</p><p>In the original paper, more techniques are involved to remove the penalty parameter and to make the computation efficient. For the convergence rate, the authors only focused on small sample size (between <span class="math inline">\((m+1)m/2\)</span> and <span class="math inline">\((d+1)d/2\)</span>). Under some assumptions, the angular error is <span class="math inline">\(O(h^2)\)</span> with high probability, where <span class="math inline">\(h\)</span> is the bandwidth. The proof involves a lot of estimations of matrix elements which are used in Gershgorin circle theorem. In fact, the paper contributes a lot to the estimation of eigenvalues. For eigenvectors they used results from <a href="https://ipsen.math.ncsu.edu/ps/rr978.ps" target="_blank" rel="noopener">relative perturbation bounds for eigenspaces</a>, which are generalizations of Davis-Kahan theorem.</p><p><img src="/2020/05/30/Tangent-Space-Estimation-Using-Local-Polynomial-Fitting/1.jpg" alt="1"><img src="/2020/05/30/Tangent-Space-Estimation-Using-Local-Polynomial-Fitting/2.jpg" alt="2"></p>]]></content>
      
      
      <categories>
          
          <category> Manifold-Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
            <tag> Asymptotic Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tangent Space Estimation Using Local PCA</title>
      <link href="/2020/05/21/Tangent-Space-Estimation-Using-Local-PCA/"/>
      <url>/2020/05/21/Tangent-Space-Estimation-Using-Local-PCA/</url>
      
        <content type="html"><![CDATA[<p>In <a href="https://arxiv.org/abs/1102.0075" target="_blank" rel="noopener">this paper</a>, A.Singer and H.-T. Wu derived an explicit expression for the bias and variance of local PCA in tangent space estimation. One thing to notice: when we have estimated a basis <span class="math inline">\(\widehat{E}\)</span> and want to compare with the true basis <span class="math inline">\(E\)</span>, we can either</p><ul><li>consider the error between two projection operators <span class="math inline">\(\|\widehat{E}\widehat{E}^T-EE^T\|\)</span>,</li></ul><p>or</p><ul><li>consider the error up to rotation <span class="math inline">\(\min_{O}\|O\widehat{E}-E\|\)</span>.</li></ul><p>However, according to the original paper, the convergence rates for above two errors are different. Moreover, the authors did not claim the choice of bandwidth is optimal. So other choices of bandwidth may also by possible.</p><p>Let <span class="math inline">\(\mathcal{M}\)</span> be an <span class="math inline">\(m\)</span>-dimensional compact submanifold in a Euclidean space <span class="math inline">\(\mathbb{R}^d\)</span>. Fix <span class="math inline">\(x\in\mathcal{M}\)</span>. Without loss of generality assume that <span class="math inline">\(x\)</span> is placed at the origin and the tangent space <span class="math inline">\(T_x\mathcal{M}\)</span> coincides with the coordinate space spanned by the first <span class="math inline">\(m\)</span> standard vectors <span class="math inline">\(\{e_1,\cdots,e_m\}\)</span> in <span class="math inline">\(\mathbb{R}^d\)</span>. Suppose <span class="math inline">\(X\)</span> is a random vector valued in <span class="math inline">\(\mathcal{M}\)</span> with a smooth positive density function <span class="math inline">\(f:\mathcal{M}\to \mathbb{R}\)</span>. Let <span class="math inline">\(K:\mathbb{R}\to\mathbb{R}\)</span> be a smooth function supported on <span class="math inline">\([0,1]\)</span> and <span class="math inline">\(h&gt;0\)</span> be a positive number. To estimate the tangent space at <span class="math inline">\(x\)</span>, we start by finding a direction <span class="math inline">\(v\)</span> such that the projection to <span class="math inline">\(v\)</span> is maximized <span class="math display">\[\mathbb{E}[((X-x)\cdot v)^2K_h(X-x)]\]</span> where <span class="math inline">\(K_h:\mathbb{R}^d\to\mathbb{R}\)</span> is the function <span class="math inline">\(K_h(u)=1/h^mK(\|u\|/h)\)</span>. Therefore, we consider the following Lagrange function <span class="math display">\[f(v)=\mathbb{E}[((X-x)\cdot v)^2K_h(X-x)]-\lambda(\|v\|-1)\]</span> Setting <span class="math inline">\(df/dv=0\)</span> we have <span class="math inline">\(\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]v=\lambda v\)</span> and the objective function is <span class="math inline">\(\lambda\)</span>. Thus we choose the eigenvector of <span class="math inline">\(\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]\)</span> with respect to the largest eigenvalue.</p><p>Next, we proceed by finding another direction <span class="math inline">\(v&#39;\)</span> orthogonal to <span class="math inline">\(v\)</span> such that the same objective function is maximized. The result turns out that the optimal <span class="math inline">\(v&#39;\)</span> is the eigenvector of <span class="math inline">\(\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]\)</span> of the second largest eigenvalue. Iteratively, to estimate the tangent space <span class="math inline">\(T_x\mathcal{M}\)</span> we attempt to find the first <span class="math inline">\(m\)</span> eigenvectors of the matrix <span class="math inline">\(\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]\)</span>. We set <span class="math display">\[\mathbb{H}=\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]\]</span> and call it the <strong>population covariance matrix</strong>. In practice, <span class="math inline">\(\mathbb{H}\)</span> is not available. Suppose <span class="math inline">\(X_1,\cdots,X_n\)</span> are i.i.d. samples of <span class="math inline">\(X\)</span>. Then we can replace <span class="math inline">\(\mathbb{H}\)</span> by the empirical mean, <span class="math display">\[H=\frac{1}{n}\sum_{i=1}^n(X_i-x)(X_i-x)^tK_h(X_i-x)\]</span> which is called the <strong>empirical covariance matrix</strong>. Note that these are not in fact covariance matrices since the mean is forced to be <span class="math inline">\(x\)</span>. But it will bring technical convenience for our analysis. We first give a detailed analysis of the covariance matrices.</p><p>For <span class="math inline">\(h\)</span> small enough, the contributed points will lie in a normal patch of <span class="math inline">\(x\)</span>. Hence we can use the exponential map. Suppose <span class="math inline">\(X=\exp_x(s_X\theta_X)\)</span> where <span class="math inline">\(\|\theta_X\|=1\)</span>. Then <span class="math display">\[X-x=s_X\theta_X+\frac{1}{2}\Pi(\theta_X,\theta_X)s_X^2-\frac{1}{6}A_{\Pi(\theta_X,\theta_X)}(\theta_X)s^3+\frac{1}{6}(\nabla_{\theta_X}\Pi)(\theta_X,\theta_X)s^3+o(s^3)\]</span> Thus we have the following expansion for <span class="math inline">\(\mathbb{H}\)</span>, <span class="math display">\[\begin{aligned}\mathbb{H}=&amp;\underbrace{\mathbb{E}[(\theta_X\theta_X^ts_X^2-\frac{1}{6}(\theta_XA_\Pi^t+A_\Pi^t\theta_X)s_X^4+o(s^4_X))K_h(X-x)]}_{\mathbf{H}}\\+&amp;\underbrace{\mathbb{E}[(\frac{1}{2}(\theta_X\Pi^t+\Pi\theta_X^t)s_X^3+\frac{1}{6}(\theta_X(\nabla\Pi)^t+(\nabla\Pi)\theta^t)s_X^4+o(s_X^4))K_h(X-x)]}_{\mathbf{B}_r}\\+&amp;\underbrace{\mathbb{E}[(\frac{1}{4}\Pi\Pi^ts^4)K_h(X-x)]}_{\mathbf{B}_s}\end{aligned}\]</span> Note that <span class="math inline">\(\mathbf{H}\)</span>, <span class="math inline">\(\mathbf{B}_r\)</span> and <span class="math inline">\(\mathbf{B}_s\)</span> are blocked matrices. i.e. <span class="math display">\[\mathbf{H}=\left[\begin{array}{cc}*&amp;0\\0&amp;0\end{array}\right],\mathbf{B}_r=\left[\begin{array}{cc}0&amp;*\\*&amp;0\end{array}\right],\mathbf{B}_s=\left[\begin{array}{cc}0&amp;0\\0&amp;*\end{array}\right]\]</span> Call <span class="math inline">\(\mathbf{H}\)</span> the true covariance matrix since the eigenvectors of <span class="math inline">\(\mathbf{H}\)</span> will give the true basis of the tangent space. <span class="math inline">\(\mathbf{B}_r\)</span> and <span class="math inline">\(\mathbf{B}_s\)</span> will called the basis matrices and we see from the expansion that the bias comes from the curvature of the manifold <span class="math inline">\(\mathcal{M}\)</span>.</p><p>Integration in the normal patch gives the leading terms of each matrices. Note that odd terms of <span class="math inline">\(\theta\)</span> will vanish. Therefore, <span class="math display">\[\mathbf{H}=\left[\begin{array}{cc}ch^2I+O(h^4)&amp;0\\0&amp;0\end{array}\right],\mathbf{B}_r=\left[\begin{array}{cc}0&amp;O(h^4)\\O(h^4)&amp;0\end{array}\right],\mathbf{B}_s=\left[\begin{array}{cc}0&amp;0\\0&amp;O(h^4)\end{array}\right]\]</span> Thus the bias deviates from the true matrix will be of order <span class="math inline">\(O(h^4)\)</span>.</p><p>Then we consider the variance coming from <span class="math inline">\(\mathbb{H}\)</span> and <span class="math inline">\(H\)</span>. By definition, <span class="math inline">\(\mathbb{E}[H]=\mathbb{H}\)</span>. Thus we may write <span class="math display">\[H=\mathbb{H}+V\]</span> where <span class="math inline">\(\mathbb{E}[V]=0\)</span>. For each element in <span class="math inline">\(V\)</span>, the variance is <span class="math display">\[\mathbb{Var}[V(i,j)]=\mathbb{E}[(H(i,j)-\mathbb{H}(i,j))^2]\le\frac{1}{n}\mathbb{E}[(X-x)_i^2(X-x)^2_jK^2_h(X-x)]\]</span> By computation the second moments are given by <span class="math display">\[\mathbb{E}[(X-x)_i^2(X-x)_j^2K_h(X-x)]=\left\{\begin{array}{l}O(\frac{h^4}{h^m})\quad i,j=1,\cdots,m\\ O(\frac{h^8}{h^m})\quad i,j=m+1,\cdots,d\\ O(\frac{h^6}{h^m})\quad \text{otherwise}\end{array}\right.\]</span> Therefore, the variance matrix is given by <span class="math display">\[\mathbb{Var}[V(i,j)]=\left\{\begin{array}{l}O(\frac{h^4}{nh^m})\quad i,j=1,\cdots,m\\ O(\frac{h^8}{nh^m})\quad i,j=m+1,\cdots,d\\ O(\frac{h^6}{nh^m})\quad \text{otherwise}\end{array}\right.\]</span> Using Chebyshev's inequality, we see that for any <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math display">\[\mathbb{Pr}\{|H(i,j)-\mathbb{H}(i,j)|&gt;\epsilon\}\le\frac{\mathbb{Var}(V(i,j))}{\epsilon^2}\]</span> If we set the right hand side to be <span class="math inline">\(O(1)\)</span>, then with high probability <span class="math inline">\(V(i,j)=O(\epsilon)\)</span>. Following this principle, with high probability, <span class="math inline">\(V\)</span> is in the following form <span class="math display">\[V=\frac{h^2}{\sqrt{nh^m}}\left[\begin{array}{cc}O(1)&amp; O(h)\\O(h)&amp;O(h^2)\end{array}\right]\]</span></p><p>Overall, we have shown that with high probability, <span class="math display">\[\begin{aligned}\frac{1}{h^2}H=&amp;c\left[\begin{array}{cc}I&amp;0\\0&amp;0\end{array}\right]+h^2\left[\begin{array}{cc}O(1)&amp;O(1)\\O(1)&amp;O(1)\end{array}\right]+\frac{1}{\sqrt{nh^m}}\left[\begin{array}{cc}O(1)&amp;O(h)\\O(h)&amp;O(h^2)\end{array}\right]\end{aligned}\]</span> If we set <span class="math inline">\(O(\text{bias})=O(\text{variance})\)</span>, the <em>optimal bandwidth</em> <span class="math inline">\(h=O(n^{-1/(m+4)})\)</span>. In this case we have <span class="math display">\[H=h^2c\left[\begin{array}{cc}I+h^2A&amp;h^2C\\h^2C^t&amp;h^2B\end{array}\right]=h^2c(S+h^2T)\]</span> where <span class="math display">\[S=\left[\begin{array}{cc}I&amp;0\\0&amp;0\end{array}\right], T=\left[\begin{array}{cc}A&amp;C\\C^t&amp;B\end{array}\right]\]</span> Let <span class="math inline">\(\mathbf{v}\)</span> be an eigenvector of <span class="math inline">\(S+h^2T\)</span> with respect to the eigenvalue <span class="math inline">\(\mathbf{\Lambda}\)</span>. Using the following expansion on both sides of <span class="math inline">\(S\mathbf{v}=\mathbf{\Lambda}\mathbf{v}\)</span>, <span class="math display">\[\begin{aligned}&amp;\mathbf{v}=v_0+h^2v_1+h^4v_2+\cdots\\&amp;\mathbf{\Lambda}=\lambda_0+h^2\lambda_1+h^4\lambda_2+\cdots\end{aligned}\]</span> and compare the coefficients on both sides. We have <span class="math display">\[Sv_0=\lambda_0v_0\]</span> which means that <span class="math inline">\(\lambda_0=1\)</span> and <span class="math inline">\(v_0\)</span> is in the form <span class="math inline">\([\bar{v}_0,0]^t\)</span>. For the first order term we have <span class="math display">\[Sv_1+Tv_0=\lambda_0v_1+\lambda_1v_0\]</span> Thus if we write <span class="math inline">\(v_1=[\bar{v}_1,\tilde{v}_1]\)</span> then <span class="math inline">\(A\bar{v}_0=\lambda_1\bar{v}_0\)</span> and <span class="math inline">\(C^t\bar{v}_0=\tilde{v}_1\)</span>. For the second term, we have <span class="math display">\[Sv_2+Tv_1=v_2+\lambda_1v_1+\lambda_2v_0\]</span> Then <span class="math inline">\(A\bar{v}_1+C\tilde{v}_1=\lambda_1\bar{v}_1+\lambda_2\bar{v}_0\)</span>. Nothing will vanish after this step. Therefore, we have <span class="math inline">\(\mathbf{v}=[\bar{v}_0,0]+O(h^2)\)</span> and <span class="math inline">\(\mathbf{\Lambda}=1+h^2\lambda_A+O(h^4)\)</span>.</p><p>However, if we make the leading term of <span class="math inline">\(A\)</span> different from <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>, that is, <span class="math inline">\(O(\text{variance})&gt;O(\text{bias})\)</span> and the error mainly comes from sampling, the result will be different. As in the original paper we set <span class="math inline">\(O(\text{variance})=O(h)\)</span> which gives the sampling rate <span class="math inline">\(h=O(n^{-1/(m+2)})\)</span>. In this case we have <span class="math display">\[S=\left[\begin{array}{cc}I&amp;0\\0&amp;0\end{array}\right], T_1=\left[\begin{array}{cc}A&amp;0\\0&amp;0\end{array}\right], T_2=\left[\begin{array}{cc}0&amp;C\\C^t&amp;B\end{array}\right]\]</span> By <a href="https://www.math.usm.edu/lambers/mat610/sum10/lecture13.pdf" target="_blank" rel="noopener">Wielandt-Hoffman theorem</a>, the eigenvalues satisfy the following inequality <span class="math display">\[\sum_{i=1}^d(\lambda_i(T_1+hT_2)-\lambda_i(T_1))^2\le\|hT_2\|_F^2\]</span> Hence if we set the eigenvalues in descending order <span class="math inline">\(\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_d\)</span>, then <span class="math inline">\(\lambda_k(T_1+hT_2)=\lambda_k^A+O(h)\)</span> for <span class="math inline">\(k=1,\cdots,m\)</span> and <span class="math inline">\(\lambda_l(T_1+hT_2)=O(h)\)</span> for <span class="math inline">\(k=m+1,\cdots,d\)</span>.</p><p>For eigenvectors we use the same method as above. Suppose <span class="math inline">\(\mathbf{v}\)</span> is a unit eigenvector of <span class="math inline">\(S+hT_1+hT_2\)</span> corresponding to the eigenvalue <span class="math inline">\(\mathbf{\Lambda}\)</span>. Using Taylor expansions for <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{\Lambda}\)</span>, <span class="math display">\[\begin{aligned}&amp;\mathbf{v}=v_0+hv_1+h^2v_2+h^3v_3+h^4v_4+O(h^5)\\&amp;\mathbf{\Lambda}=\lambda_0+h\lambda_1+h^2\lambda_2+h^3\lambda_3+h^4\lambda_4+O(h^5)\end{aligned}\]</span> and by the relation <span class="math display">\[(S+hT_1+h^2T_2)\mathbf{v}=\mathbf{\Lambda}\mathbf{v}\]</span> we compare the coefficients on both sides. For the constants we have <span class="math display">\[\begin{aligned}&amp;Sv_0=\lambda_0v_0\end{aligned}\]</span> which implies <span class="math inline">\(\lambda_0=1\)</span> and <span class="math inline">\(v_0\)</span> is in the form <span class="math inline">\([\bar{v}_0,0]^t\)</span>. For the first order term we have <span class="math display">\[Sv_1+T_1v_0=v_1+\lambda_1v_0\]</span> which implies <span class="math inline">\(v_1\)</span> is also in the form <span class="math inline">\([\bar{v}_1,0]^t\)</span>, and <span class="math inline">\(A\bar{v}_0=\lambda_1\bar{v}_0\)</span>. We see that <span class="math inline">\(\bar{v}_0\)</span> is an eigenvector of <span class="math inline">\(A\)</span>. For the second order term we have <span class="math display">\[T_2v_0+T_1v_1+Sv_2=v_2+\lambda_1v_1+\lambda_2v_0\]</span> Note that <span class="math inline">\(T_2v_0=[0,C^t\bar{v}_0]^t\)</span>. We see that the left side is <span class="math inline">\([A\bar{v}_1+\bar{v}_2,C^t\bar{v}_0]^t\)</span> and the right side is <span class="math inline">\([\lambda_1\bar{v}_1+\lambda_2\bar{v}_0,0]^t+[\bar{v}_2,\tilde{v}_2]^t\)</span>. This gives <span class="math inline">\(\tilde{v}_2=C^t\bar{v}_0\)</span> and <span class="math display">\[A\bar{v}_1=\lambda_1\bar{v}_1+\lambda_2\bar{v}_0\]</span> Multiplying both sides by <span class="math inline">\(\bar{v}_0^t\)</span>, we have <span class="math display">\[\lambda_1\bar{v}_0^t\bar{v}_1=\lambda_1\bar{v}_0^t\bar{v}_1+\lambda_2\bar{v}_0^t\bar{v}_0\]</span> Thus, <span class="math inline">\(\lambda_2=0\)</span> and <span class="math inline">\(A\bar{v}_1=\lambda_1\bar{v}_1\)</span>. If we assume the eigenvalues of <span class="math inline">\(A\)</span> are simple (as said in original paper it holds almost surely), then <span class="math inline">\(\bar{v}_1=0\)</span>. For the third order term we have <span class="math display">\[Sv_3+T_1v_2+T_2v_1=v_3+\lambda_1v_2+\lambda_2v_1+\lambda_3v_0\]</span> The left side is <span class="math inline">\([\bar{v}_3+A\bar{v}_2,0]^t\)</span> and the right side is <span class="math inline">\([\bar{v}_3+\lambda_1\bar{v}_2+\lambda_3\bar{v}_0,\tilde{v}_3+\lambda_1\tilde{v}_2]\)</span>. Hence <span class="math inline">\(\tilde{v}_3=-\lambda_1\tilde{v}_2\)</span> and <span class="math inline">\(A\bar{v}_2=\lambda_1\bar{v}_2+\lambda_3\bar{v}_0\)</span>. Using the same method as before we know that <span class="math inline">\(\lambda_3=0\)</span> and <span class="math inline">\(\bar{v}_2=0\)</span>. For the fourth term we have <span class="math display">\[Sv_4+T_1v_3+T_2v_2=v_4+\lambda_1v_3+\lambda_4v_0\]</span> The left side is <span class="math inline">\([\bar{v}_4+A\bar{v}_3+C\tilde{v}_2,B\tilde{v}_2]^t\)</span> and the right side is <span class="math inline">\([\bar{v}_4+\lambda_1\bar{v}_3+\lambda_4\bar{v}_0,\lambda_1\tilde{v}_3+\tilde{v}_4]^t\)</span>. Hence <span class="math inline">\(\tilde{v}_4=B\tilde{v}_2-\lambda_1\tilde{v}_3\)</span> and <span class="math display">\[A\bar{v}_3+CC^t\bar{v}_0=\lambda_1\bar{v}_3+\lambda_4\bar{v}_0\]</span> We cannot cancel anything in this step. Overall we have shown that <span class="math display">\[\begin{aligned}&amp;\mathbf{v}=v_0+h^2v_2+h^3v_3+O(h^4)=[\bar{v}_0+\bar{v}_3h^3+O(h^4),\tilde{v}_2h^2+O(h^3)]\\&amp;\mathbf{\Lambda}=1+h\lambda_1+h^4\lambda_4+O(h^5)\end{aligned}\]</span> Suppose the first <span class="math inline">\(m\)</span> eigenvectors of <span class="math inline">\(H\)</span> are <span class="math inline">\(u_1,\cdots,u_m\)</span> and the eigenvectors of <span class="math inline">\(A\)</span> are <span class="math inline">\(w_1,\cdots,w_m\)</span>. We have <span class="math inline">\(u_k=[w_k+O(h^3),O(h^2)]^t\)</span> for <span class="math inline">\(k=1,\cdots,m\)</span>. Let <span class="math inline">\(\mathbf{U}=[u_1,\cdots,u_m]\)</span> be the <span class="math inline">\(d\times m\)</span> matrix and <span class="math inline">\(\mathbf{O}=[w_1,\cdots,w_m]^t\)</span> be the <span class="math inline">\(m\times m\)</span> orthogonal matrix. Furthermore, let <span class="math inline">\(\mathbf{\Theta}=[e_1,\cdots,e_m]\)</span> be the matrix consisting of <span class="math inline">\(m\)</span> standard basis of <span class="math inline">\(\mathbb{R}^d\)</span>. We measure the deviation of the estimated <span class="math inline">\(m\)</span>-plane spanned by <span class="math inline">\(u_k\)</span>'s from <span class="math inline">\(T_x\mathcal{M}\)</span> by <span class="math display">\[\min_{O\in O(m)} \|\mathbf{U}^t\mathbf{\Theta}-O\|_F\le\|\mathbf{U}^t\mathbf{\Theta}-\mathbf{O}\|_F=O(h^3)\]</span> If we choose the optimal bandwidth, the order will be <span class="math inline">\(O(h^2)\)</span>. Thus we see that by choosing 'non-optimal' bandwidth the convergence even become faster. However, from another perspective, if we want to find a rotation <span class="math inline">\(\hat{O}\)</span> such that the basis <span class="math inline">\(Q=\mathbf{\Theta}\hat{O}\)</span> is closest to <span class="math inline">\(\mathbf{U}\)</span>, i.e. the error is measured by <span class="math display">\[\min_{\hat{O}\in O(m)}\|\mathbf{U}-\mathbf{\Theta}\hat{O}\|_F\]</span> then the error is bounded by <span class="math display">\[\min_{\hat{O}\in O(m)}\|\mathbf{U}-\mathbf{\Theta}\hat{O}\|_F\le O(h^2)\]</span> Then different bandwidths yield the same order of convergence. Note that the difference comes from the estimation of eigenvectors.</p><p>A final remark is that the method used above by comparing coefficients for different orders is not very strict, since it automatically assumes that there are no fractional order terms. Generalization to fractional order convergence needs further investigation.</p><hr><p><strong>Update:</strong> In this <a href="https://arxiv.org/pdf/1512.02857.pdf" target="_blank" rel="noopener">paper</a> (appendix D theorem 38), a useful version of <a href="https://arxiv.org/pdf/1405.0680.pdf" target="_blank" rel="noopener">Davis-Kahan</a> theorem is stated as follows</p><blockquote><p>Let <span class="math inline">\(O\in\mathbb{R}^{d\times d}\)</span>, <span class="math inline">\(B\in\mathbb{R}^{m\times m}\)</span> be positive semi-definite symmetric matrices such that <span class="math display">\[O=\left[\begin{array}{cc}B&amp;0\\ 0&amp;0\end{array}\right]+E\]</span> Let <span class="math inline">\(T_0\)</span> be the vector space spanned by the first <span class="math inline">\(m\)</span> vectors of canonical basis and <span class="math inline">\(T\)</span> be the eigenspace spanned by the first <span class="math inline">\(m\)</span> eigenvectors of <span class="math inline">\(O\)</span>. Then <span class="math display">\[\angle (T_0,T)\le \frac{\sqrt{2}\|E\|_{op}}{\lambda_{\min}(B)}\]</span></p></blockquote><p>Now we return back to the expression of <span class="math inline">\(1/h^2H\)</span>, consider the decomposition <span class="math display">\[\frac{1}{h^2}H=\left[\begin{array}{cc}cI+O(h^2)+O(\sqrt{nh^m})&amp;0\\0&amp;0\end{array}\right]+\left[\begin{array}{cc}0&amp;O(h^2+\frac{h}{\sqrt{nh^m}})\\O(h^2+\frac{h}{\sqrt{nh^m}})&amp;O(h^2)\end{array}\right]\]</span> Note <span class="math inline">\(\lambda_{\min}(cI+O(\cdot))\approx c\)</span>. Thus the convergence rate depends on the operator norm of the second matrix. If we set <span class="math inline">\(1/\sqrt{nh^m}=h^\alpha\)</span> where <span class="math inline">\(0&lt;\alpha&lt;1\)</span>, then the rate is <span class="math inline">\(O(h^{1+\alpha})\)</span>. If <span class="math inline">\(\alpha\ge 1\)</span>, the rate is <span class="math inline">\(O(h^2)\)</span>.</p><hr><p><strong>Update:</strong> <a href="https://djalil.chafai.net/blog/2011/12/03/the-hoffman-wielandt-inequality/" target="_blank" rel="noopener">This blog</a> gives a very nice introduction to Wielandt-Hoffman inequality. The proof is short and readable. I also write a new blog on Davis-Kahan's <span class="math inline">\(\sin(\Theta)\)</span> theorem. See <a href="http://yueqicao.top/2021/01/12/Davis-Kahan-s-Theorem/">this page</a> for more details.</p>]]></content>
      
      
      <categories>
          
          <category> Manifold-Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
            <tag> Asymptotic Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Extrinsic Quantities of Euclidean Submanifolds</title>
      <link href="/2020/05/16/Extrinsic-Quantities-of-Euclidean-Submanifolds/"/>
      <url>/2020/05/16/Extrinsic-Quantities-of-Euclidean-Submanifolds/</url>
      
        <content type="html"><![CDATA[<p>Consider an <span class="math inline">\(m\)</span>-dimensional compact Riemannian submanifold <span class="math inline">\(\mathcal{M}\)</span> of a Euclidean space <span class="math inline">\(\mathbb{R}^n\)</span> with <span class="math inline">\(m&lt;n\)</span>. There are two aspects to investigate the geometry and topology about the submanifold. On the one hand, one can look at the intrinsic quantities defined on <span class="math inline">\(\mathcal{M}\)</span>, say Riemannian metric tensor, geodesics, Jacobian fields and sectional curvature. This has often been done in <a href="https://arxiv.org/pdf/1909.12057.pdf" target="_blank" rel="noopener">Lie Group Learning</a>, <a href="http://math.ucr.edu/home/baez/information/" target="_blank" rel="noopener">Information Geometry</a> and so on. On the other hand, extrinsic quantities related to the embedding of the submanifold into the Euclidean space, which often include normal vectors (or spaces), the second fundamental form and mean curvature etc. , are also useful and important. For data scientists who always confront with the situation where the underlying manifold is unknown and only random samples are available, to look extrinsically is the only choice. Here we list some extrinsic quantities and show their influences on the geometry and topology of the submanifold.</p><h2 id="reach-and-medial-axis">Reach and Medial Axis</h2><p>Let <span class="math inline">\(\mathcal{NM}\)</span> be the normal bundle of <span class="math inline">\(\mathcal{M}\)</span>. Consider the map given by <span class="math display">\[\begin{aligned}E:\mathcal{NM}&amp;\to \mathbb{R}^n\\(x,\xi)&amp;\mapsto x+\xi\end{aligned}\]</span> A tubular neighborhood <span class="math inline">\(\mathcal{M}^\tau\)</span> is the diffeomorphic image under <span class="math inline">\(E\)</span> of some open subset <span class="math inline">\(V\subseteq \mathcal{NM}\)</span> of the form <span class="math display">\[V=\{(x,\xi)\in\mathcal{NM}:\|\xi\|&lt;\tau(x)\}\]</span> where <span class="math inline">\(\tau:\mathcal{M}\to \mathbb{R}\)</span> is some positive function. That is, <span class="math inline">\(\mathcal{M}^\tau=E(V)\)</span>. It is a fact that every embedded submanifold of <span class="math inline">\(\mathbb{R}^n\)</span> has a tubular neighborhood (See <a href="https://www.amazon.com/Introduction-Smooth-Manifolds-Graduate-Mathematics/dp/1441999817" target="_blank" rel="noopener">Theorem 6.17 of Lee</a>. Moreover, if <span class="math inline">\(\mathcal{M}\)</span> is compact, <span class="math inline">\(\tau\)</span> can be chosen to be a constant. The projection <span class="math inline">\(\pi^\tau:\mathcal{M}^\tau\to\mathcal{M}\)</span> sends a point <span class="math inline">\(y\in\mathcal{M}^\tau\)</span> to the closest point <span class="math inline">\(\pi^\tau(y)\in\mathcal{M}\)</span>. The <a href="https://arxiv.org/pdf/1705.00989.pdf" target="_blank" rel="noopener">reach</a> <span class="math inline">\(\tau_{\mathcal{M}}\)</span> of <span class="math inline">\(\mathcal{M}\)</span> is the supreme radius for which the tubular neighborhood can be defined.</p><p>In computational geometry, the <a href="http://people.cs.uchicago.edu/~niyogi/papersps/NiySmaWeiHom.pdf" target="_blank" rel="noopener">medial axis</a> is a commonly used notion which is defined as the closure of the set <span class="math display">\[G=\{x\in\mathbb{R}^n|\exists p\neq q\in\mathcal{M}\text{ s.t. }d(x,\mathcal{M})=\|x-p\|=\|x-q\|\}\]</span> For any <span class="math inline">\(p\in\mathcal{M}\)</span>, the local feature size <span class="math inline">\(\sigma(p)\)</span> is the distance of <span class="math inline">\(p\)</span> to <span class="math inline">\(\bar{G}\)</span>. By definition we have <span class="math display">\[\tau_{\mathcal{M}}=\inf_{p\in\mathcal{M}}\sigma(p)\]</span></p><h2 id="the-second-fundamental-form">The Second Fundamental Form</h2><p>Fix a point <span class="math inline">\(x\in\mathcal{M}\)</span>. Let <span class="math inline">\(\Pi:T_x\mathcal{M}\times T_x\mathcal{M}\to T_x^\perp\mathcal{M}\)</span> be the second fundamental form at <span class="math inline">\(x\)</span>. For any unit normal <span class="math inline">\(\xi\in T_x^\perp\mathcal{M}\)</span>, let <span class="math inline">\(A_\xi\)</span> be the shape operator such that <span class="math display">\[\langle\Pi(u,v),\xi\rangle=\langle A_\xi(u),v\rangle\]</span> Let <span class="math inline">\(\lambda_\xi\)</span> be the largest eigenvalue of <span class="math inline">\(A_\xi\)</span>. It turns out that the operator norm of the shape operator and thus the second fundamental form can be uniformly bounded by the reciprocal of reach.</p><blockquote><p>For all <span class="math inline">\(x\in\mathcal{M}\)</span> and all <span class="math inline">\(\xi\in T_x^\perp\mathcal{M}\)</span>, we have <span class="math inline">\(\lambda_\xi\le 1/\tau_{\mathcal{M}}\)</span>. Therefore, <span class="math inline">\(\|A_\xi\|_{op}\le1/\tau_{\mathcal{M}}\)</span>. If we define <span class="math inline">\(\|\Pi\|=\sup_{\|u\|=\|v\|=1}\|\Pi(u,v)\|\)</span>, then <span class="math inline">\(\|\Pi\|\le 1/\tau_{\mathcal{M}}\)</span>.</p></blockquote><p>Suppose there exists <span class="math inline">\(x\)</span> and <span class="math inline">\(\xi\in T_x^\perp\mathcal{M}\)</span> such that <span class="math inline">\(\lambda_\xi&gt;1/\tau_\mathcal{M}\)</span>. There exists some <span class="math inline">\(t&lt;\tau_{\mathcal{M}}\)</span> such that <span class="math inline">\(\lambda_\xi&gt;1/t&gt;1/\tau_{\mathcal{M}}\)</span>. Note that <span class="math display">\[\lambda_\xi=\sup_{\|u\|=1}\langle\Pi(u,u),\xi\rangle\]</span> Let <span class="math inline">\(\gamma(s)\)</span> be a geodesic parametrized by the arc length with <span class="math inline">\(\gamma(0)=x\)</span> and <span class="math inline">\(\dot{\gamma}(0)=u\)</span>. Then <span class="math inline">\(\Pi(u,u)=\ddot{\gamma}(0)\)</span>. Consider the point <span class="math inline">\(q=x+t\xi\)</span>. By assumption, <span class="math inline">\(x\)</span> is the closest point to <span class="math inline">\(q\)</span> in <span class="math inline">\(\mathcal{M}\)</span>. Thus <span class="math display">\[f(s):=\|\gamma(s)-q\|^2\ge f(0)=t^2\]</span> Direct computation shows that <span class="math display">\[\begin{aligned}&amp;\dot{f}(s)=2\langle\dot{\gamma}(s),\gamma(s)-q\rangle\\&amp;\ddot{f}(s)=2(\langle\ddot{\gamma}(s),\gamma(s)-q\rangle+1)\end{aligned}\]</span> Note that <span class="math inline">\(\dot{f}(0)=0\)</span> and <span class="math inline">\(\ddot{f}(0)=2(-t\langle\Pi(u,u),\xi\rangle+1)&lt;0\)</span>. By continuity, there exists <span class="math inline">\(s_0\)</span> such that <span class="math inline">\(f(s_0)&lt;f(0)\)</span>, which contradicts to our assumption.</p><p>Since <span class="math inline">\(\|\Pi(u,v)\|=\sup_{\|\xi\|=1}\langle\Pi(u,v),\xi\rangle=\sup_{\|\xi\|=1}\langle A_\xi(u),v\rangle\)</span> and for any <span class="math inline">\(\xi\)</span> the operator norm <span class="math inline">\(\|A_\xi\|_{op}\)</span> is uniformly bounded, we see that <span class="math inline">\(\|\Pi\|\le1/\tau_{\mathcal{M}}\)</span>.</p><h2 id="local-parametrizations">Local Parametrizations</h2><h3 id="exponential-map">Exponential Map</h3><p>The second fundamental form is closely related to the injective radius, as proved by <a href="https://arxiv.org/pdf/math/0511570.pdf" target="_blank" rel="noopener">Bishop and Alexander</a>: If <span class="math inline">\(\|\Pi\|\le C\)</span>, then <span class="math inline">\(inj_\mathcal{M}\ge\pi/C\)</span>. Thus the exponential map can be defined on the ball of radius <span class="math inline">\(\pi/C\)</span> in <span class="math inline">\(T_x\mathcal{M}\)</span> for all <span class="math inline">\(x\)</span>. Fix <span class="math inline">\(x\in\mathcal{M}\)</span>, if we transform <span class="math inline">\(\mathcal{M}\)</span> isometrically in <span class="math inline">\(\mathbb{R}^n\)</span> so that <span class="math inline">\(x\)</span> is the origin and the first <span class="math inline">\(m\)</span> standard basis <span class="math inline">\(\{e_1,\cdots,e_m\}\)</span> spans the tangent space <span class="math inline">\(T_x\mathcal{M}\)</span>. Consider the coordinate components of the exponential map <span class="math display">\[\exp(u^1,\cdots,u^m)=(r^1(u^1,\cdots,u^m),\cdots,r^n(u^1,\cdots,u^m))\]</span> We can express the derivatives of component functions using the geometrical terms defined on <span class="math inline">\(\mathcal{M}\)</span>. Firstly, since the tangent map of <span class="math inline">\(\exp\)</span> at origin is the identity, we have <span class="math display">\[d\exp_0(e_i)=(\partial_ir^1(0),\cdots,\partial_ir^n(0))=e_i\]</span> We obtain that <span class="math inline">\(\partial_ir^j(0)=\delta^j_i\)</span> for <span class="math inline">\(i,j=1,\cdots,m\)</span> and <span class="math inline">\(\partial_ir^\alpha(0)=0\)</span> for <span class="math inline">\(\alpha=m+1,\cdots,n\)</span>.</p><p>For the second derivatives, let <span class="math inline">\(u=\sum_{i=1}^m u^ie_i\)</span> and <span class="math inline">\(\gamma(s)=\exp(su)\)</span>. we have <span class="math display">\[\ddot{\gamma}(0)=(\sum_{i,j}\partial_{i}\partial_jr^1(0)u^iu^j,\cdots,\sum_{i,j}\partial_i\partial_jr^n(0)u^iu^j)=\Pi(u,u)\]</span> Thus, <span class="math inline">\(\partial_i\partial_jr^k(0)=0\)</span> for <span class="math inline">\(i,j,k=1,\cdots,m\)</span>. Let <span class="math inline">\(\Pi_\alpha(u,v)=\langle\Pi(u,v),e_\alpha\rangle=\langle A_{e_\alpha}(u),v\rangle\)</span>. We see that <span class="math inline">\([\partial_i\partial_jr^\alpha(0)=\langle A_{e_\alpha}e_i,e_j\rangle]_{i,j}\)</span> is the matrix representation for shape operator <span class="math inline">\(A_{e_\alpha}\)</span> for <span class="math inline">\(\alpha=m+1,\cdots,n\)</span>.</p><p>By Weingarten formula, we have <span class="math display">\[\dddot{\gamma}=-A_{\Pi(\dot{\gamma},\dot{\gamma})}(\dot{\gamma})+\nabla_{\dot{\gamma}}^\perp\Pi(\dot{\gamma},\dot{\gamma})\]</span> If we define <span class="math inline">\(\nabla\Pi(u,v,w)=\nabla_w^\perp\Pi(u,v)-\Pi(\nabla_wu,v)-\Pi(u,\nabla_wv)\)</span>. Then we have <span class="math display">\[\dddot{\gamma}(0)=-A_{\Pi(u,u)}(u)+(\nabla\Pi)(u,u,u)\]</span> Comparing the components on both sides we obtain that <span class="math display">\[\begin{aligned}&amp;\partial_i\partial_j\partial_kr^l(0)=-\langle A_{\Pi(e_i,e_j)}(e_k),e_l\rangle\\&amp;\partial_i\partial_j\partial_kr^\alpha(0)=\langle(\nabla_{e_i}\Pi)(e_j,e_k),e_l\rangle\end{aligned}\]</span> for <span class="math inline">\(i,j,k,l=1,\cdots,m\)</span> and <span class="math inline">\(\alpha=m+1,\cdots,n\)</span>. We have also deduced the following tailor expansion for the exponential map <span class="math display">\[\exp_x(u)=x+u+\frac{1}{2}\Pi(u,u)-\frac{1}{6}A_{\Pi(u,u)}(u)+\frac{1}{6}(\nabla_u\Pi)(u,u)+O(\|u\|^4)\]</span> Let <span class="math inline">\(y=\exp_x(su)\)</span> where <span class="math inline">\(\|u\|=1\)</span>. We can compare the Euclidean distance <span class="math inline">\(\|y-x\|\)</span> and the Riemannian distance <span class="math inline">\(s=d_\mathcal{M}(x,y)\)</span>. In fact, we have <span class="math display">\[\|y-x\|^2=s^2+\frac{1}{4}\|\Pi(u,u)\|^2s^4-\frac{1}{3}\langle A_{\Pi(u,u)}(u),u\rangle s^4+o(s^4)\]</span> Therefore, <span class="math display">\[\|y-x\|=s-\frac{1}{24}\|\Pi(u,u)\|^2s^3+o(s^3)\]</span></p><h3 id="graph-parametrization">Graph Parametrization</h3><p>Consider a parametrized surface <span class="math inline">\(r(u,v)=(x(u,v),y(u,v),z(u,v))\)</span>. Without loss of generality assume that <span class="math inline">\(\left[\begin{array}{cc}x_u&amp; y_u\\ x_v&amp;y_v\end{array}\right]\)</span> is non-singular at <span class="math inline">\(p\)</span>. Then by implicit function theorem there will be an open neighborhood near <span class="math inline">\(p\)</span> such that <span class="math inline">\(u=u(x,y),v=v(x,y)\)</span>. Then <span class="math inline">\(r(x,y)=(x,y,z(x,y))\)</span> is the graph of function <span class="math inline">\(z=z(x,y)\)</span>.</p><p>Generally for any submanifold there exists local parametrization such that it is just the graph of some functions. Furthermore, if we place <span class="math inline">\(p\)</span> at the origin and let <span class="math inline">\(T_p\mathcal{M}\)</span> coincide with the <span class="math inline">\(m\)</span>-coordinate space, assume that the parametrization is given by <span class="math display">\[(u^1,\cdots,u^m)\mapsto(u^1,\cdots,u^m,f^1(u^1,\cdots,u^m),\cdots,f^{n-m}(u^1,\cdots,u^m))\]</span> Then by assumption <span class="math inline">\(\partial_if^j(0)=0\)</span> for <span class="math inline">\(i,j=1,\cdots,m\)</span>. The normal vector field can be explicitly given by <span class="math display">\[\xi^\alpha=(-\partial_1f^\alpha,\cdots,-\partial_mf^\alpha,0,\cdots,\overset{\alpha}{1},\cdots,0)\]</span> Thus the matrix representation of the shape operator <span class="math inline">\(A_{e_\alpha}\)</span> is <span class="math inline">\([\partial_i\xi^\alpha\cdot e_j]_{i,j}=Hess(f^\alpha)\)</span>. Note that <span class="math inline">\(Hess(f^\alpha)\)</span> also plays the role as the second fundamental form of the hypersurface <span class="math display">\[(u^1,\cdots,u^m,f^\alpha(u^1,\cdots,u^m))\subseteq \mathbb{R}^{m+1}\]</span> Although the asymptotic of graph parametrization and the exponential map parametrization has a lot in common, they are actually different. For the graph parametrization, the inverse is simply given by the projection onto the tangent space, while for the exponential map this cannot be the case.</p>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basics about Riemannian Submanifolds</title>
      <link href="/2020/04/25/Basics-about-Riemannian-Submanifolds/"/>
      <url>/2020/04/25/Basics-about-Riemannian-Submanifolds/</url>
      
        <content type="html"><![CDATA[<h2 id="the-second-fundamental-form">The Second Fundamental Form</h2><p>Let <span class="math inline">\((\tilde{M},\tilde{g})\)</span> be a Riemannian manifold and <span class="math inline">\(M\subset \tilde{M}\)</span> be a submanifold. By restricting <span class="math inline">\(\tilde{g}\)</span> on <span class="math inline">\(M\)</span> the submanifold is assigned with a natural Riemannian metric. At each point <span class="math inline">\(p\)</span> we have the orthogonal decomposition with respect to <span class="math inline">\(\tilde{g}_p\)</span>, <span class="math display">\[T_p\tilde{M}=T_pM\oplus T_p^\perp M\]</span> where <span class="math inline">\(T_p^\perp M\)</span>, called the normal space at <span class="math inline">\(p\)</span>, denotes the orthogonal complement of <span class="math inline">\(T_pM\)</span> in <span class="math inline">\(T_p\tilde{M}\)</span>. Let <span class="math inline">\(T^\perp M=\cup_{p\in M}T_p^\perp M\)</span>. It can be verified that <span class="math inline">\(T^\perp M\)</span> is a vector bundle on <span class="math inline">\(M\)</span>, called the normal bundle. The idea to study Riemannian submanifolds is straightforward: <strong>we differentiate the vector fields along <span class="math inline">\(M\)</span> and look at its tangent components and normal components respectively</strong>.<a id="more"></a> Let <span class="math inline">\(\tilde{\nabla}\)</span> be the metric connection on <span class="math inline">\(\tilde{M}\)</span> and <span class="math inline">\(X\in\Gamma(TM)\)</span> be a tangent vector field on <span class="math inline">\(M\)</span>. On the one hand, for any vector fields <span class="math inline">\(X,Y\in \Gamma(TM)\)</span>, we have <span class="math display">\[\tilde{\nabla}_XY=(\tilde{\nabla}_XY)^\top+(\tilde{\nabla}_XY)^\perp\]</span> The first term is nothing but the covariant derivative of <span class="math inline">\(Y\)</span> on <span class="math inline">\(M\)</span>. Let <span class="math inline">\(\nabla\)</span> be the metric connection on <span class="math inline">\(M\)</span> with respect to the induced Riemannian metric. Then <span class="math inline">\((\tilde{\nabla}_XY)^\top=\nabla_XY\)</span>. Define <span class="math display">\[h(X,Y)=(\tilde{\nabla}_XY)^\perp=\tilde{\nabla}_XY-\nabla_XY\]</span> It can be verified that <span class="math inline">\(h\)</span> is a normal-bundle-valued symmetric tensor field on <span class="math inline">\(M\)</span>, called <strong>the second fundamental form</strong>. Equation (3) is called <strong>the Gauss formula</strong>.</p><p><strong>Remark.</strong> Given <span class="math inline">\(\xi\in \Gamma(T^\perp M)\)</span>, we can define a tensor field <span class="math inline">\(\Pi_\xi(X,Y)=\tilde{g}(h(X,Y),\xi)\)</span>. In literature <span class="math inline">\(\Pi\)</span> is also called the second fundamental form.</p><p>On the other hand, for a vector field <span class="math inline">\(\xi\in \Gamma(T^\perp M)\)</span>, we have <span class="math display">\[\tilde{\nabla}_X\xi=(\tilde{\nabla}_X\xi)^\top+(\tilde{\nabla}_X\xi)^\perp\]</span> Define the connection <span class="math inline">\(\nabla^\perp:\Gamma(T^\perp M)\times\Gamma(TM)\to \Gamma(T^\perp M)\)</span> by <span class="math inline">\((\nabla^\perp)_X\xi=(\tilde{\nabla}_X\xi)^\perp\)</span>. It can be verified that <span class="math inline">\(\nabla^\perp\)</span> is a connection on the normal bundle <span class="math inline">\(T^\perp M\)</span> which is compatible with the bundle metric, called <strong>the normal connection</strong>. Define <span class="math display">\[A_\xi(X)=-(\tilde{\nabla}_X\xi)^\top\]</span> The operator <span class="math inline">\(A_\xi:T_pM\to T_pM\)</span> is called <strong>the shape operator</strong> or <strong>the Weingarten map</strong>. The minus sign is chosen so that the following equation holds <span class="math display">\[\tilde{g}(A_\xi(X),Y)=\tilde{g}(h(X,Y),\xi)\]</span> The equation <span class="math display">\[\tilde{\nabla}_X\xi=-A_\xi(X)+\nabla^\perp_X\xi\]</span> is called <strong>the Weingarten formula</strong>.</p><p><strong>Example.</strong> Let <span class="math inline">\(M^{d-1}\subseteq \mathbb{E}^d\)</span> be a hypersurface, <span class="math inline">\(\xi\)</span> be a unit normal vector field on <span class="math inline">\(M\)</span>. <strong>The Gauss map</strong> <span class="math inline">\(g:M\to \mathbb{S}^{d-1}\)</span> is defined by <span class="math inline">\(g(p)=\xi_p\)</span>. For any <span class="math inline">\(X\in T_pM\)</span>, let <span class="math inline">\(\gamma\)</span> be a smooth curve such that <span class="math inline">\(\gamma&#39;(0)=X\)</span>. Then <span class="math display">\[A_\xi X=-(\xi(\gamma)&#39;(0))^\top=-g(\gamma)&#39;(0)=-g_*(X)\]</span> that is, <span class="math inline">\(-A_\xi=g_*\)</span> is the differential of Gauss map. It is clear from this example that for hypersurfaces the normal connection is trivial.</p><p>Let <span class="math inline">\(e_1,\cdots,e_m\)</span> be a basis of the tangent space and <span class="math inline">\(\xi_1,\cdots,\xi_{d-m}\)</span> be a basis of the normal space. Assume <span class="math display">\[A_{\xi_\alpha}e_i=\sum_{j=1}^mA_{\alpha i}^je_j\]</span> Easy computation shows that <span class="math display">\[\begin{aligned}&amp;\Pi_{\xi_\alpha}(e_i,e_j)=A_{\alpha i}^j\\&amp;h(e_i,e_j)=\sum_{\alpha=1}^{d-m}A_{\alpha i}^j\xi_\alpha\end{aligned}\]</span> Define the mean curvature vector field on <span class="math inline">\(M\)</span> by <span class="math inline">\(H=\frac{1}{m}trace(h)\)</span> where <span class="math inline">\(m\)</span> is the dimension of <span class="math inline">\(M\)</span>. By definition <span class="math inline">\(H\)</span> is independent of the choice of basis. Under the above notation we have <span class="math display">\[H=\frac{1}{m}\sum_{i=1}^m h(e_i,e_i)=\frac{1}{m}\sum_{i=1}^m\sum_{\alpha=1}^{d-m}A_{\alpha i}^i\xi_\alpha=\sum_{\alpha=1}^{d-m}(\frac{1}{m}\text{trace}(A_{\alpha})\xi_{\alpha})=\sum_{\alpha=1}^{d-m}H^\alpha\xi_\alpha\]</span> where <span class="math inline">\(H^\xi=\tilde{g}(H,\xi)\)</span> is called the mean curvature along <span class="math inline">\(\xi\)</span>. The value <span class="math inline">\(\|H\|=(\sum_\alpha\|H^\alpha\|^2)^{1/2}=\frac{1}{m}(\sum_{\alpha=1}^{d-m}\text{trace}(A_\alpha)^2)^{1/2}\)</span> is called the mean curvature. If <span class="math inline">\(M\)</span> is a hypersurface, <span class="math inline">\(\|H\|=\frac{1}{m}|\text{trace}(A_\alpha)|\)</span>. For a surface in 3-dimensional Euclidean space, this coincides with <strong>the absolute mean curvature</strong>.</p><h2 id="fundamental-equations">Fundamental Equations</h2><p>To involve the curvature tensor, we need second order derivative. Thus, we differentiate the Gauss formula and the Weingarten formula. For Gauss formula we have <span class="math display">\[\begin{aligned}\tilde{\nabla}_X\tilde{\nabla}_YZ&amp;=\tilde{\nabla}_X\nabla_YZ+\tilde{\nabla}_Xh(Y,Z)\\&amp;=\nabla_X\nabla_Y+h(X,\nabla_YZ)-A_{h(Y,Z)}(X)+\nabla^\perp_Xh(Y,Z)\end{aligned}\]</span> If we define the curvature tensor to be <span class="math inline">\(\tilde{R}(X,Y)=[\tilde{\nabla}_X,\tilde{\nabla}_Y]-\tilde{\nabla}_{[X,Y]}\)</span>, we have <span class="math display">\[\begin{aligned}\tilde{R}(X,Y)Z=&amp;R(X,Y)Z+h(X,\nabla_YZ)-h(Y,\nabla_XZ)-h([X,Y],Z)\\&amp;-A_{h(Y,Z)}(X)+A_{h(X,Z)}(Y)+\nabla^\perp_Xh(Y,Z)-\nabla^\perp_Yh(X,Z)\end{aligned}\]</span> Define the covariant differentiation of <span class="math inline">\(h\)</span> to be <span class="math display">\[(\nabla_Xh)(Y,Z)=\nabla^\perp_Xh(Y,Z)-h(\nabla_XY,Z)-h(\nabla_XZ,Y)\]</span> Then the tangent component of <span class="math inline">\(\tilde{R}(X,Y)Z\)</span> is <span class="math display">\[(\tilde{R}(X,Y)Z)^\top=R(X,Y)Z+A_{h(X,Z)}(Y)-A_{h(Y,Z)}(X)\]</span> while the normal component is <span class="math display">\[(\tilde{R}(X,Y)Z)^\perp=(\nabla_Xh)(Y,Z)-(\nabla_Yh)(X,Z)\]</span> Equation (12) is called <strong>the Gauss equation</strong> and equation (13) is called <strong>the Codazzi equation</strong>. We can rewrite the Gauss equation as <span class="math display">\[\tilde{R}(X,Y,Z,W)=R(X,Y,Z,W)+\langle h(X,Z),h(Y,W)\rangle-\langle h(Y,Z),h(X,W)\rangle\]</span> Especially the sectional curvature can be expressed as <span class="math display">\[\tilde{K}(X,Y)=K(X,Y)-\langle h(X,X),h(Y,Y)\rangle+\|h(X,Y)\|^2\]</span> If the ambient space is Euclidean, then <span class="math inline">\(\overline{R}\)</span> vanishes identically. Let <span class="math inline">\(e_i,e_j\)</span> span a two-plane <span class="math inline">\(\pi_{ij}\)</span>, then the sectional curvature of <span class="math inline">\(\pi_{ij}\)</span> is <span class="math display">\[K(\pi_{ij})=-\|h(e_i,e_j)\|^2+\langle h(e_i,e_i),h(e_j,e_j)\rangle=\sum_{\alpha=1}^{d-m}(-(A_{\alpha i}^j)^2+A_{\alpha i}^iA_{\alpha j}^j)\]</span> From <span class="math inline">\(A_\alpha\)</span> we extract the <span class="math inline">\(2\times 2\)</span> submatrix <span class="math inline">\(A_\alpha|_{\pi_{ij}}\)</span> with <span class="math inline">\(i\)</span>th and <span class="math inline">\(j\)</span>th row and column. Then <span class="math display">\[K(\pi_{ij})=\sum_{\alpha=1}^{d-m}\det(A_\alpha|_{\pi_{ij}})\]</span> <strong>Example.</strong> If <span class="math inline">\(M\)</span> is a hypersurface of <span class="math inline">\(\mathbb{E}^d\)</span> with a unit normal vector field <span class="math inline">\(\xi\)</span>, the sectional curvature is <span class="math inline">\(K(\pi_{ij})=\det(A_\xi|_{\pi_{ij}})\)</span>. If <span class="math inline">\(e_1,\cdots,e_{d-1}\)</span> is a basis that diagonalizes the shape operator with eigenvalues <span class="math inline">\(\lambda_1,\cdots,\lambda_{d-1}\)</span>, then <span class="math inline">\(K(\pi_{ij})=\lambda_i\lambda_j\)</span>. The eigenvalues are called <strong>principal curvature</strong> and eigenvectors are called <strong>principal directions</strong>. The determinant of <span class="math inline">\(A_\xi\)</span> is called <strong>the Gauss-Kronecker curvature</strong>.</p><p>Similarly we can differentiate the Weingarten formula, which yields <span class="math display">\[\begin{aligned}\tilde{\nabla}_X\tilde{\nabla}_Y\xi&amp;=-\tilde{\nabla}_XA_\xi(Y)+\tilde{\nabla}_X\nabla^\perp_Y\xi\\&amp;=-\nabla_X A_\xi(Y)-h(X,A_\xi(Y))-A_{\nabla^\perp_Y\xi}(X)+\nabla^\perp_X\nabla^\perp_Y\xi\end{aligned}\]</span> Define the curvature tensor on the normal bundle by <span class="math inline">\(R^\perp(X,Y)\xi=[\nabla^\perp_X,\nabla^\perp_Y]\xi-\nabla^\perp_{[X,Y]}\xi\)</span>, and define the covariant differentiation of the shape operator by <span class="math inline">\((\nabla_XA)_\xi(Y)=\nabla_X(A_\xi Y)-A_{\nabla^\perp_X\xi}(Y)-A_\xi(\nabla_XY)\)</span>. We obtain that <span class="math display">\[\begin{aligned}\tilde{R}(X,Y)\xi=&amp;R^\perp(X,Y)\xi-(\nabla_X A)_\xi(Y)+(\nabla_Y A)_\xi(X)\\&amp;-h(X,A_\xi(Y))+h(Y,A_\xi(X))\end{aligned}\]</span> The tangent component is <span class="math display">\[(\tilde{R}(X,Y)\xi)^\top=-(\nabla_X A)_\xi(Y)+(\nabla_Y A)_\xi(X)\]</span> while the normal component is <span class="math display">\[(\tilde{R}(X,Y)\xi)^\perp=R^\perp(X,Y)\xi+h(X,A_\xi(Y))-h(Y,A_\xi(X))\]</span> Equation (19) is called <strong>the Ricci equation</strong>. Note that equation (18) is equivalent to the Codazzi equation as follows <span class="math display">\[\begin{aligned}\langle\tilde{R}(X,Y)\xi,Z\rangle=&amp;\langle-(\nabla_X A)_\xi(Y),Z\rangle+\langle(\nabla_Y A)_\xi(X),Z\rangle\\=&amp;\langle\nabla_X(A_\xi Y)-A_{\nabla^\perp_X\xi}(Y)-A_\xi(\nabla_XY),Z\rangle\\&amp;+\langle\nabla_Y(A_\xi X)-A_{\nabla^\perp_Y\xi}(X)-A_\xi(\nabla_YX),Z\rangle\\=&amp;-\langle\nabla^\perp_X\xi,h(Y,Z)\rangle-\langle\nabla^\perp_Y\xi,h(X,Z)\rangle-\langle h(\nabla_XY,Z),\xi\rangle\\&amp;-\langle h(\nabla_YX,Z),\xi\rangle+X\langle h(Y,Z),\xi\rangle-\langle h(\nabla_XZ,Y),\xi \rangle\\&amp;+Y\langle h(X,Z),\xi\rangle-\langle h(\nabla_YZ,X),\xi\rangle\\=&amp;\langle\nabla_X^\perp h(Y,Z),\xi\rangle+\langle\nabla_Y^\perp h(X,Z),\xi\rangle-\langle h(\nabla_XY,Z),\xi\rangle\\&amp;-\langle h(\nabla_YX,Z),\xi\rangle-\langle h(\nabla_XZ,Y),\xi \rangle-\langle h(\nabla_YZ,X),\xi\rangle\\=&amp;\langle(\nabla_X h)(Y,Z)-(\nabla_Y h)(X,Z),\xi\rangle\\=&amp;\langle\tilde{R}(X,Y)Z,\xi\rangle\end{aligned}\]</span> The three equations (Gauss, Codazzi, Ricci) are called fundamental equations for a submanifold <span class="math inline">\(M\hookrightarrow\tilde{M}\)</span>.</p><h2 id="examples">Examples</h2><h3 id="planar-curves">Planar Curves</h3><p>A planar curve is a 1-dimensional manifold embedded in the 2-plane. Let <span class="math inline">\(\mathbf{t}\)</span> be the tangent vector field and <span class="math inline">\(\mathbf{n}\)</span> be the normal vector field. We have <span class="math display">\[A_{\mathbf{n}}\mathbf{t}=-(\overline{D}_{\mathbf{n}}\mathbf{t})^\top=\kappa \mathbf{t}\]</span> where <span class="math inline">\(\kappa\)</span> is the curvature. Thus the estimation can be simply given by <span class="math display">\[\hat{A}=\frac{\Delta\mathbf{n}\cdot\mathbf{t}}{\Delta p\cdot\mathbf{t}}\approx \kappa\]</span></p><h3 id="space-curves">Space Curves</h3><p>A space curve is a 1-dimensional manifold embedded in 3-space. Let <span class="math inline">\(\mathbf{t}\)</span> be the tangent vector field, <span class="math inline">\(\mathbf{n}\)</span> be the normal vector field, and <span class="math inline">\(\mathbf{b}\)</span> be the binormal vector field. We have the Frenet formula <span class="math display">\[\frac{d}{ds}\left(\begin{array}{c}\mathbf{t}\\ \mathbf{n}\\ \mathbf{b}\end{array}\right)=\left(\begin{array}{ccc}0&amp;\kappa&amp;0\\ -\kappa&amp;0&amp;\tau\\ 0&amp;-\tau&amp;0\end{array}\right)\left(\begin{array}{c}\mathbf{t}\\ \mathbf{n}\\ \mathbf{b}\end{array}\right)\]</span> where <span class="math inline">\(\kappa\)</span> is curvature and <span class="math inline">\(\tau\)</span> is torsion. Let <span class="math inline">\(\xi=\cos(\theta)\mathbf{n}+\sin(\theta)\mathbf{b}\)</span> be a unit normal vector field. We have <span class="math display">\[A_\xi\mathbf{t}=\cos(\theta)A_\mathbf{n}\mathbf{t}+\sin(\theta)A_\mathbf{b}\mathbf{t}=\cos(\theta)\kappa\mathbf{t}\]</span> Similarly, let <span class="math inline">\(\xi_\perp=-\sin(\theta)\mathbf{n}+\cos(\theta)\mathbf{b}\)</span> be the unit normal vector perpendicular to <span class="math inline">\(\xi\)</span>. Then <span class="math display">\[A_{\xi_\perp}\mathbf{t}=-\sin(\theta)\kappa\mathbf{t}\]</span> By definition the mean vector field is <span class="math inline">\(H=\cos(\theta)\kappa\xi-\sin(\theta)\kappa\xi_\perp=\kappa\mathbf{n}\)</span>. The mean curvature is <span class="math inline">\(\|H\|=\kappa\)</span>.</p><h3 id="surfaces">Surfaces</h3><p>A surface is a 2-dimensional manifold embedded in 3-space. The shape operator coincides with the differential of Gauss map (with an additional minus sign). Thus the norm of mean curvature vector <span class="math inline">\(\|H\|\)</span> is in fact the absolute value of mean curvature for surfaces. The sectional curvature is Gaussian curvature for surfaces.</p><h3 id="clifford-torus">Clifford Torus</h3><p>Let <span class="math inline">\(f:\mathbb{R}^2\to\mathbb{R}^4\)</span> be defined by <span class="math display">\[f(\theta,\phi)=\frac{1}{\sqrt{2}}(\cos(\sqrt{2}\theta),\sin(\sqrt{2}\theta),\cos(\sqrt{2}\phi),\sin(\sqrt{2}\phi))\]</span> The image of <span class="math inline">\(f\)</span> is <span class="math inline">\(\mathbb{S}^1(\sqrt{1/2})\times\mathbb{S}^1(\sqrt{1/2})\)</span>, hence a torus. By computation the tangent vector fields are <span class="math display">\[\begin{aligned}&amp; f_\theta = (-\sin(\sqrt{2}\theta),\cos(\sqrt{2}\theta),0,0)\\&amp; f_\phi = (0,0,-\sin(\sqrt{2}\phi),\cos(\sqrt{2}\phi))\end{aligned}\]</span> The Riemannian metric is given by <span class="math display">\[g = d\theta^2+d\phi^2\]</span> which is a flat metric, implying the sectional curvature is identically zero. The normal vector fields are given by <span class="math display">\[\begin{aligned}&amp; \xi = \frac{1}{\sqrt{2}}(-\cos(\sqrt{2}\theta),-\sin(\sqrt{2}\theta),\cos(\sqrt{2}\phi),\sin(\sqrt{2}\phi))\\&amp; \nu =\frac{1}{\sqrt{2}}(\cos(\sqrt{2}\theta),\sin(\sqrt{2}\theta),\cos(\sqrt{2}\phi),\sin(\sqrt{2}\phi))\end{aligned}\]</span> By definition the shape operators are given by <span class="math display">\[\begin{aligned}&amp;A_\xi[f_\theta,f_\phi]=[f_\theta,f_\phi]\left[\begin{array}{cc}1&amp;0\\0&amp;-1\end{array}\right]\\&amp;A_\nu[f_\theta,f_\phi]=[f_\theta,f_\phi]\left[\begin{array}{cc}-1&amp;0\\0&amp;-1\end{array}\right]\end{aligned}\]</span> Therefore, the mean curvature vector field is <span class="math inline">\(H=-\nu\)</span> and the mean curvature is <span class="math inline">\(1\)</span>.</p><h3 id="rotation-group">Rotation Group</h3><p>Let <span class="math inline">\(SO(2)\)</span> be the rotation group which consists of matrices in the form <span class="math display">\[\left[\begin{array}{cc}\cos(\theta)&amp;-\sin(\theta)\\ \sin(\theta)&amp; \cos(\theta)\end{array}\right]\]</span> which can be viewed as a curve in the 4-space. Consider the parametrization <span class="math display">\[f(\theta)=(\cos(\theta),\sin(\theta),-\sin(\theta),\cos(\theta))\]</span> Direct computation shows that the tangent vector field is <span class="math display">\[e_\theta=1/\sqrt{2}(-\sin(\theta),\cos(\theta),-\cos(\theta),-\sin(\theta))\]</span> and consider the following basis of normal space <span class="math display">\[\begin{aligned}&amp; \xi_1=(\cos(\theta),\sin(\theta),0,0)\\&amp; \xi_2=(0,0,\sin(\theta),-\cos(\theta))\\&amp; \xi_3=1/\sqrt{2}(-\sin(\theta),\cos(\theta),\cos(\theta),\sin(\theta))\end{aligned}\]</span> Then the shape operators are <span class="math display">\[A_{\xi_1}e_\theta=-1/\sqrt{2}e_\theta,A_{\xi_2}e_\theta=1/\sqrt{2}e_\theta,A_{\xi_3}e_\theta=0\]</span> Hence the mean vector field is <span class="math inline">\(H(\theta)=-1/\sqrt{2}\xi_1+1/\sqrt{2}\xi_2=-1/\sqrt{2}f(\theta)\)</span>.</p><h3 id="ellipsoid">Ellipsoid</h3><p>Let <span class="math inline">\(F:\mathbb{R}^{n+1}\to\mathbb{R}\)</span> be the function <span class="math display">\[F(x^1,\cdots,x^{n+1})=\sum_{i=1}^{n+1}(\frac{x^i}{a^i})^2-1\]</span> Since <span class="math inline">\(\nabla F(x)\neq 0\)</span> for all <span class="math inline">\(x\)</span> such that <span class="math inline">\(F(x)=0\)</span>, the level set <span class="math inline">\(M=F^{-1}(0)\)</span> is a <span class="math inline">\(n\)</span>-dimensional submanifold. <span class="math inline">\(\nabla F/\|\nabla F\|\)</span> serves as the unit normal vector field on <span class="math inline">\(M\)</span>. The mean curvature is given by the following formula <span class="math display">\[H=-\frac{1}{n}div(\frac{\nabla F}{\|\nabla F\|})\]</span></p>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Random Walk and Graph Spectra</title>
      <link href="/2020/03/27/Random-Walk-and-Graph-Spectra/"/>
      <url>/2020/03/27/Random-Walk-and-Graph-Spectra/</url>
      
        <content type="html"><![CDATA[<p>Random walks arise in many fields in mathematics and physics, recently in network analysis, graph learning, clustering etc. It also has a natural relation with spectral graph theory. The <a href="http://web.cs.elte.hu/~lovasz/erdos.pdf" target="_blank" rel="noopener">survey</a> written by L.Lovasz presents many beautiful results concerning random walks and graph spectra. We aim to write down some computation details missing in the original succinct paper. <a id="more"></a></p><h2 id="basic-notions">Basic Notions</h2><p>Let <span class="math inline">\(G=(V,E)\)</span> be a connected, undirected, simple graph with <span class="math inline">\(n\)</span> nodes and <span class="math inline">\(m\)</span> edges. Consider the following random walk on <span class="math inline">\(G\)</span>: start from a node <span class="math inline">\(v_0\)</span>; at the <span class="math inline">\(t\)</span>th step move to the neighbor of <span class="math inline">\(v_t\)</span> with probability <span class="math inline">\(1/d(v_t)\)</span>. i.e. It is a Markov chain with transition matrix <span class="math display">\[M=D^{-1}A\]</span> where <span class="math inline">\(A\)</span> is the adjacency matrix and <span class="math inline">\(D\)</span> is the diagonal matrix such that <span class="math inline">\(D_{ii}=d(i)=\sum_jA_{ij}\)</span> (<span class="math inline">\(\textbf{Caveat:}\)</span> in original paper <span class="math inline">\(D\)</span> is adopted as <span class="math inline">\(D^{-1}\)</span> here, thus all the indices over <span class="math inline">\(D\)</span> are reversed in this note). Let <span class="math inline">\(P_t\)</span> be the distribution at time <span class="math inline">\(t\)</span>. Then <span class="math display">\[(P_t)_i=\sum_j (P_{t-1})_jM_{ji}\]</span> If we assume <span class="math inline">\(P_t\)</span>'s are column vectors, the matrix form should be written as <span class="math display">\[P_t=M^TP_{t-1}\]</span> A random walk is called stationary if <span class="math inline">\(P_0=P_1\)</span>. We denote the stationary random walk by <span class="math inline">\(\pi\)</span>. A simple calculation shows that <span class="math inline">\(\pi=D\mathbf{1}/2m\)</span> is a stationary distribution of <span class="math inline">\(M\)</span>. The uniqueness and convergence are shown by the following.</p><h2 id="stationary-distribution">Stationary Distribution</h2><p>Before discussing about the stationary distribution of the random walk <span class="math inline">\(M\)</span>, we state the <a href="%5Bhttps://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem%5D(https://en.wikipedia.org/wiki/PerronFrobenius_theorem)">Perron-Frobenius theorem</a> which reveals some important properties about nonnegative matrices.</p><p>A nonnegative matrix <span class="math inline">\(A\)</span> is called irreducible if there does <strong>NOT</strong> exist a permutation matrix <span class="math inline">\(R\)</span> such that <span class="math display">\[RAR^{-1}=\left[\begin{array}{cc}E&amp;F\\ 0&amp;G\end{array}\right]\]</span> For an irreducible matrix <span class="math inline">\(A\)</span>, the spectral radius <span class="math inline">\(\rho(A)=r&gt;0\)</span> is an eigenvalue of <span class="math inline">\(A\)</span>, whose corresponding (left /right) eigenspace is unidimensional. Moreover, <span class="math inline">\(A\)</span> has an (left/right) eigenvector <span class="math inline">\(v\)</span> associated with <span class="math inline">\(r\)</span> whose components are all positive, and conversely, eigenvectors whose components are all positive are associated with <span class="math inline">\(r\)</span>.</p><p>Consider the special case where <span class="math inline">\(A\)</span> is the adjacency matrix of an undirected simple graph. <span class="math inline">\(A\)</span> is irreducible if and only if there does not exist a nontrivial partition <span class="math inline">\(\{1,2,\cdots, n\}=I\cup J\)</span> such that <span class="math inline">\(A_{ij}=0\)</span> for <span class="math inline">\(i\in I\)</span> and <span class="math inline">\(j\in J\)</span>. i.e. <span class="math inline">\(A\)</span> is irreducible if and only if the graph <span class="math inline">\(G\)</span> is connected. This criterion easily generalizes to any similarity matrices on graphs. For example, consider <span class="math inline">\(N=D^{-1/2}AD^{-1/2}\)</span>. It is irreducible if and only if <span class="math inline">\(G\)</span> is connected. Since <span class="math inline">\(N\)</span> is symmetric, write <span class="math inline">\(N\)</span> in spectral form <span class="math display">\[N=\sum_k\lambda_kv_kv_k^T\]</span> where <span class="math inline">\(\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_n\)</span> are eigenvalues of <span class="math inline">\(N\)</span> and <span class="math inline">\(v_1,\cdots,v_n\)</span> are corresponding eigenvectors of unit length. Note that <span class="math inline">\(A\mathbf{1}=D\mathbf{1}\)</span>, which gives <span class="math inline">\(ND^{1/2}\mathbf{1}=D^{1/2}\mathbf{1}\)</span>. i.e. <span class="math inline">\(D^{1/2}\mathbf{1}\)</span> is an eigenvector of <span class="math inline">\(N\)</span> whose components are all positive. By Perron-Frobenius theorem, <span class="math inline">\(v_1=\frac{D^{1/2}\mathbf{1}}{\sqrt{2m}}\)</span> and <span class="math inline">\(\lambda_1=1&gt;\lambda_2\ge\cdots\ge\lambda_n\ge-1\)</span>.</p><p>A simple calculation shows that <span class="math inline">\(M=D^{-1/2}ND^{1/2}\)</span> is similar to <span class="math inline">\(N\)</span>. Now we have <span class="math display">\[M^t=D^{-1/2}v_1v_1^TD^{1/2}+\sum_{k\ge 2}\lambda_k^tD^{-1/2}v_kv_k^TD^{1/2}=Q+\Lambda^t\]</span> where <span class="math inline">\(Q_{ij}=d(j)/2m=\pi(j)\)</span> is such that <span class="math inline">\(Q^TP=\pi\)</span> for any probability distribution <span class="math inline">\(P\)</span>. Hence if we can prove <span class="math inline">\(\Lambda^t\to 0\)</span> as <span class="math inline">\(t\to\infty\)</span> we obtain that <span class="math inline">\(\pi\)</span> is the stationary distribution. In addition, since <span class="math inline">\(M\)</span> is similar to <span class="math inline">\(N\)</span> and <span class="math inline">\(\lambda_1=1\)</span> is a simple eigenvalue of <span class="math inline">\(N\)</span>, we see that <span class="math inline">\(\pi\)</span> will be the only probability distribution such that <span class="math inline">\(M^T\pi=\pi\)</span>. Apparently, it suffices to exclude the case where <span class="math inline">\(\lambda_n=-1\)</span>.</p><p>Let <span class="math inline">\(L=D-A\)</span> be the Laplacian matrix, <span class="math inline">\(\mathcal{L}=D^{-1/2}LD^{-1/2}\)</span> be the normalized Laplacian. Note that <span class="math inline">\(\mathcal{L}=I-N\)</span>. Thus, <span class="math inline">\(\lambda_n(N)=1-\lambda_1(\mathcal{L})\)</span>. By Rayleigh-Ritz theorem, <span class="math display">\[\lambda_1(\mathcal{L})=\sup_f\frac{f^T\mathcal{L}f}{f^Tf}\]</span> Set <span class="math inline">\(g=D^{-1/2}f\)</span>. Then by the fact <span class="math inline">\((g_i-g_j)^2\le 2(g_i^2+g_j^2)\)</span>, <span class="math display">\[\lambda_1(\mathcal{L})=\sup_g\frac{1/2\sum_{i,j}a_{ij}(g_i-g_j)^2}{\sum_ig_i^2d_i}\le\sup_g\frac{\sum_{i,j}a_{ij}(g_i^2+g_j^2)}{\sum_ig_i^2d_i}=2\]</span> where equality holds if and only if <span class="math inline">\(g_i=-g_j\)</span> if <span class="math inline">\(a_{ij}\neq0\)</span>. Since <span class="math inline">\(g\neq 0\)</span>, the components of <span class="math inline">\(g\)</span> is partitioned into two parts. Hence <span class="math inline">\(\lambda_n(N)=-1\)</span> if and only if <span class="math inline">\(\lambda_1(\mathcal{L})=-2\)</span> if and only if <span class="math inline">\(G\)</span> is bipartite.</p><p>Overall we proved the following claim about stationary distribution.</p><blockquote><p>Let <span class="math inline">\(G\)</span> be an undirected, simple, connected, non-bipartite graph, with transition matrix <span class="math inline">\(M=D^{-1}A\)</span>. It possesses a unique stationary distribution <span class="math inline">\(\pi=D\mathbf{1}/2m\)</span> such that <span class="math inline">\(M^T\pi=\pi\)</span> and <span class="math inline">\(\lim_{t\to\infty}P_t=\pi\)</span> for any initial distribution <span class="math inline">\(P_0\)</span>.</p></blockquote><h2 id="spectra-and-access-time">Spectra and Access Time</h2><p>The access time (or hitting time) <span class="math inline">\(H_{ij}\)</span> is the expected number of steps the first time meeting node <span class="math inline">\(j\)</span> starting from node <span class="math inline">\(i\)</span>. i.e. <span class="math display">\[H_{ij}=\sum n\times \text{Prob(meet node j in n steps)}\]</span> By definition <span class="math inline">\(H_{ii}=0\)</span>. For <span class="math inline">\(i\neq j\)</span>, by conditional expectation formula <span class="math display">\[H_{ij}=\frac{1}{d_i}(\sum_{k\sim i}1+H_{kj})=1+\frac{1}{d_i}\sum_{k\sim i}H_{kj}\]</span> Let <span class="math inline">\(J=\mathbf{1}\mathbf{1}^T\)</span> be the all-one matrix. According to the above equation, the matrix <span class="math inline">\(F=J+MH-H\)</span> is a diagonal matrix. Direct computation shows that <span class="math inline">\(F^T\pi=\mathbf{1}\)</span>. Hence <span class="math inline">\(F=2mD^{-1}\)</span>. The access time matrix satisfies <span class="math display">\[(I-M)H=J-2mD^{-1}\]</span> By the above discussion, the kernel of <span class="math inline">\(I-M\)</span> is unidimensional. If <span class="math inline">\(X\)</span> is a solution of (11), then <span class="math inline">\(X+\mathbf{1}a^T\)</span> gives all the solutions of (11). By the restriction <span class="math inline">\(H_{ii}=0\)</span>, we can solve for the unique <span class="math inline">\(H\)</span>. Let <span class="math inline">\(V=[v_1,\cdots,v_n]\)</span> be the orthogonal matrix such that <span class="math display">\[N=V\left[\begin{array}{ccc}\lambda_1&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;\lambda_n\end{array}\right]V^T\]</span> Set <span class="math inline">\(U=D^{-1/2}V\)</span>. Then we have <span class="math display">\[I-M=U\left[\begin{array}{ccc}0&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;1-\lambda_n\end{array}\right]U^{-1}\]</span></p><p>Note the right side of (11) has nothing to do with <span class="math inline">\(\lambda_i\)</span>. We try to get rid of these eigenvalues. Suppose the solution <span class="math inline">\(X\)</span> has a factor <span class="math display">\[U\left[\begin{array}{ccc}0&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;\frac{1}{1-\lambda_n}\end{array}\right]U^{-1}\]</span> The multiplication with (13) gives <span class="math display">\[\sum_{k\ge 2}D^{-1/2}v_kv_k^TD^{1/2}=-D^{-1/2}v_1v_1^TD^{1/2}+I=JD/2m+I\]</span> In addition, multiplying (15) with <span class="math inline">\(J-2mD^{-1}\)</span> gives <span class="math display">\[(\frac{JD}{2m}+I)(J-2mD^{-1})=J-2mD^{-1}\]</span> Therefore one solution of (11) is <span class="math display">\[\begin{aligned}X&amp;=U\left[\begin{array}{ccc}0&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;\frac{1}{1-\lambda_n}\end{array}\right]U^{-1}(J-2mD^{-1})\\&amp;=D^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})D^{1/2}\mathbf{1}\mathbf{1}^T-2mD^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})D^{-1/2}\\&amp;=\sqrt{2m}D^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})v_1\mathbf{1}^T-2mD^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})D^{-1/2}\\&amp;=-2mD^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})D^{-1/2}\end{aligned}\]</span> Finally, we have to subtract the diagonals of <span class="math inline">\(X\)</span>. Since <span class="math display">\[X_{ii}=-2m\sum_{k\ge 2}\frac{v_{ki}^2}{(1-\lambda_k)d_i}\]</span> The access time matrix is <span class="math inline">\(H=X-\mathbf{1}[X_{11},\cdots,X_{nn}]^T\)</span>. The elements are <span class="math display">\[H_{ij}=2m\sum_{k\ge 2}\frac{1}{1-\lambda_k}(\frac{v_{kj}^2}{d_j}-\frac{v_{ki}v_{kj}}{\sqrt{d_id_j}})\]</span></p><p>This equation has many applications. For example, we can quickly find the symmetric properties of <span class="math inline">\(H\)</span>.</p><blockquote><p>For any three nodes <span class="math inline">\(u,v\)</span> and <span class="math inline">\(w\)</span>, <span class="math inline">\(H_{uv}+H_{vw}+H_{wu}=H_{uw}+H_{wv}+H_{vu}\)</span>.</p></blockquote><p>Direct computation shows that the sum <span class="math inline">\(H_{uv}+H_{vw}+H_{wu}\)</span> is <span class="math display">\[\sum_{k\ge 2}\frac{m}{1-\lambda_k}\left((\frac{v_{ku}}{\sqrt{d_u}}-\frac{v_{kv}}{\sqrt{d_v}})^2+(\frac{v_{kv}}{\sqrt{d_v}}-\frac{v_{kw}}{\sqrt{d_w}})^2+(\frac{v_{kw}}{\sqrt{d_w}}-\frac{v_{ku}}{\sqrt{d_u}})^2\right)\]</span> which is invariant under permutation.</p><blockquote><p>Let <span class="math inline">\(\pi\)</span> be the stationary walk. Then <span class="math inline">\(\sum_j\pi_jH_{ij}=\sum_{k\ge 2}\frac{1}{1-\lambda_k}\)</span>.</p></blockquote><p>Use the fact that <span class="math inline">\(\pi=D\mathbf{1}/2m\)</span> and <span class="math inline">\(v_1=D^{1/2}\mathbf{1}/\sqrt{2m}\)</span> which is orthogonal to all <span class="math inline">\(v_k\)</span>. In the summation, <span class="math display">\[\sum_j\pi_jH_{ij}=\sum_{k\ge 2}\frac{1}{1-\lambda_k}(\sum_jv_{kj}^2-\frac{v_{ki}}{\sqrt{d_i}}\sum_jv_{kj}v_{1j}\sqrt{2m})=\sum_{k\ge 2}\frac{1}{1-\lambda_k}\]</span> Note that this equality is independent of the starting point. Thus we can explain the formula as follows: in a stationary walk, no matter where we start on the graph, the average number of steps before meeting all the other nodes is a fix number which related to the graph spectra. Thus this formula nicely connects random walk with geometry of the graph.</p><blockquote><p>Let <span class="math inline">\(\pi\)</span> be the stationary walk. Then <span class="math inline">\(\sum_i\pi_iH_{ij}\ge \frac{(1-\pi_j)^2}{\pi_j}\)</span></p></blockquote><p>Using again <span class="math inline">\(v_1\)</span> is orthogonal to <span class="math inline">\(v_k\)</span>, we have <span class="math display">\[\sum_i\pi_iH_{ij}=\frac{2m}{d_j}\sum_{k\ge 2}\frac{1}{1-\lambda_k}v_{kj}^2\]</span> By Cauchy-Schwartz inequality, <span class="math display">\[\sum_{k\ge 2}\frac{v_{kj}^2}{1-\lambda_k}\sum_{k\ge 2}(1-\lambda_k)v_{kj}^2\ge(\sum_{k\ge 2}v_{kj}^2)^2\]</span> Note that <span class="math inline">\(\sum_{k\ge 2}v_{kj}^2=1-\pi(j)\)</span>, and <span class="math inline">\(\sum_{k\ge2}(1-\lambda_k)v_{kj}^2=\sum_{k}(1-\lambda_k)v_{kj}^2=1-N_{jj}=1\)</span>. Thus we obtain the claim.</p><p>To give an example where the access time is calculable, consider the <span class="math inline">\(k\)</span>-cube <span class="math inline">\(Q_k\)</span>. Let <span class="math inline">\(\mathbf{0}=(0,0,\cdots,0)\)</span> and <span class="math inline">\(\mathbf{1}=(1,1,\cdots,1)\)</span>. The nodes are elements in <span class="math inline">\(\{0,1\}^k\)</span> and two nodes are connected by an edge if they differ by one digit (see <a href="https://en.wikipedia.org/wiki/Hypercube_graph" target="_blank" rel="noopener">hypercube graph</a> ). To find all the eigenvalues and eigenvectors of <span class="math inline">\(M\)</span> (or <span class="math inline">\(A\)</span> since <span class="math inline">\(D\)</span> is a scalar matrix), L.Lovasz's paper gives a beautiful construction: for every vector <span class="math inline">\(b\in\{0,1\}^k\)</span>, define <span class="math inline">\((v_b)_x=(-1)^{b\cdot x}\)</span>. i.e. the component of <span class="math inline">\(v_b\)</span> at node <span class="math inline">\(x\)</span> is <span class="math inline">\(1\)</span> if <span class="math inline">\(b\)</span> and <span class="math inline">\(x\)</span> share even number of <span class="math inline">\(1\)</span>'s and is <span class="math inline">\(0\)</span> otherwise. Note that <span class="math display">\[\sum_ja_{ij}(v_b)_j=\sum_{j:|i-j|=1}(-1)^{b\cdot j}\]</span> Now if <span class="math inline">\(j\)</span> changes one digit of <span class="math inline">\(i\)</span> where the component of <span class="math inline">\(b\)</span> is <span class="math inline">\(0\)</span>, <span class="math inline">\(b\cdot j=b\cdot i\)</span>. This gives <span class="math inline">\(k-b\cdot\mathbf{1}\)</span> number of <span class="math inline">\((-1)^{b\cdot i}\)</span>. Otherwise (the component of <span class="math inline">\(b\)</span> is <span class="math inline">\(1\)</span>) each change from <span class="math inline">\(j\)</span> will yields <span class="math inline">\(b\cdot j=b\cdot i\pm 1\)</span>, giving <span class="math inline">\(b\cdot\mathbf{1}\)</span> number of <span class="math inline">\(-(-1)^{b\cdot i}\)</span>. Hence <span class="math inline">\(v_b\)</span> is an eigenvector of <span class="math inline">\(M\)</span> with eigenvalue <span class="math inline">\(1-\frac{2}{k}b\cdot\mathbf{1}\)</span>. Normalizing <span class="math inline">\(v_b\)</span> and using the formula for <span class="math inline">\(H\)</span> we have <span class="math display">\[\begin{aligned}H(\mathbf{0},\mathbf{1})&amp;=\sum_{b\in\{0,1\}^k,b\neq 0}\frac{k}{b\cdot\mathbf{1}}(1-(-1)^{b\cdot\mathbf{1}})\\&amp;=k\sum_{j=1}^kC_k^j\frac{1}{2j}(1-(-1)^j)\\&amp;\ge \frac{k}{2(k+1)}\sum_{j=1}^{k}C_{k+1}^{j+1}(1-(-1)^j)\\&amp;=\frac{k}{k+1}(2^k-1)\end{aligned}\]</span> To obtain an upper bound, use the identity <span class="math inline">\(C_k^j=\sum_{p=0}^{k-1}C_p^{j-1}\)</span>, where <span class="math inline">\(C_p^{j-1}=0\)</span> for <span class="math inline">\(p&lt;j-1\)</span>. We have <span class="math display">\[\begin{aligned}H(\mathbf{0},\mathbf{1})&amp;=k\sum_{j=1}^k\sum_{p=0}^{k-1}C_{p}^{j-1}\frac{1}{2j}(1-(-1)^j)\\&amp;=k\sum_{j=1}^k\sum_{p=0}^{k-1}\frac{1}{2(p+1)}C_{p+1}^{j}(1-(-1)^j)\\&amp;=k\sum_{p=0}^{k-1}\frac{1}{2(p+1)}\sum_{j=1}^kC_{p+1}^j(1-(-1)^j)\\&amp;= k\sum_{p=0}^{k-1}\frac{1}{2(p+1)}2^{p+1}\\&amp;\le k2^{k/2-1}\sum_{p&lt;k/2-1}\frac{1}{1+p}+2\sum_{p\ge k/2-1}2^p\\&amp;\le 2^{k+1}+k\log k2^{k/2}\end{aligned}\]</span> Thus for large <span class="math inline">\(k\)</span> the exact value of <span class="math inline">\(H(\mathbf{0},\mathbf{1})\)</span> is between <span class="math inline">\(2^k\)</span> and <span class="math inline">\(2^{k+1}\)</span>.</p><h2 id="spectra-and-mixing-rate">Spectra and Mixing Rate</h2><p>Let <span class="math inline">\(\lambda=\max\{|\lambda_2|,|\lambda_n|\}\)</span>. By equation (6) we have <span class="math display">\[P_t=(M^T)^tP_0=Q^TP_0+(\Lambda^T)^tP_0=\pi+(\Lambda^T)^tP_0\]</span> For a random walk starting at node <span class="math inline">\(i\)</span>, i.e. <span class="math inline">\(P_0(i)=1\)</span>, we have <span class="math display">\[P_t(j)=\pi(j)+\sum_{k\ge 2}\lambda_k^t\sqrt{\frac{d(j)}{d(i)}}v_{kj}v_{ki}\]</span> Thus by geometric-arithmetic inequality, <span class="math display">\[|P_t(j)-\pi(j)|\le\lambda^t\sqrt{\frac{d(j)}{d(i)}}\sum\frac{v_{kj}^2+v_{ki^2}}{2}\le \sqrt{\frac{d(j)}{d(i)}}\lambda^t\]</span> Moreover, for any subset <span class="math inline">\(S\subset\{1,\cdots,n\}\)</span>, by Cauchy inequality <span class="math inline">\(\sum_{s\in S}\sqrt{d(s)}v_{ks}\le\sqrt{\sum_{s\in S}d(s)}\sqrt{\sum_{s\in S}v_{ks}}\)</span>, we have <span class="math display">\[|P_t(S)-\pi(S)|\le \frac{|S|+1}{2}\sqrt{\frac{\pi(S)}{\pi(i)}}\lambda^t\]</span> Note that <span class="math inline">\(P_t(j)\)</span> is nothing but the probability of transferring from node <span class="math inline">\(i\)</span> to node <span class="math inline">\(j\)</span> in <span class="math inline">\(t\)</span> steps. Define the mixing rate <span class="math display">\[\mu=\limsup_{t\to\infty}\max_{i,j}|M^t(i,j)-\pi(j)|\]</span> From equation (25) we immediately have <span class="math inline">\(\mu=\lambda\)</span>. Thus we obtain</p><blockquote><p>The mixing rate of a random walk on a non-bipartite graph is <span class="math inline">\(\lambda=\max\{|\lambda_2|,|\lambda_n|\}\)</span>.</p></blockquote><p>To get rid of <span class="math inline">\(\lambda_n\)</span>, consider the following lazy random walk <span class="math display">\[M_L=\frac{1}{2}I+\frac{1}{2}D^{-1}A=\frac{1}{2}(I+M)\]</span> That is, we have half of the probability to stay rather than move, which is also equal to adding <span class="math inline">\(d(i)\)</span> loops at each node <span class="math inline">\(i\)</span>. Set <span class="math inline">\(N_L=D^{-1/2}(D+A)D^{-1/2}\)</span>. We have <span class="math inline">\(D^{1/2}M_LD^{-1/2}=\frac{1}{2}N_L=\frac{1}{2}(I+N)\)</span>. If <span class="math inline">\(Nv=\lambda v\)</span>, it holds <span class="math inline">\(M_LD^{-1/2}v=\frac{1+\lambda}{2}D^{-1/2}v\)</span>. Thus we have <span class="math inline">\(\lambda(M_L)=(1+\lambda(M))/2\ge0\)</span>. Furthermore, <span class="math display">\[M_L^t=D^{-1/2}v_1v_1^TD^{1/2}+\sum_{k\ge 2}(\frac{1+\lambda_k(M)}{2})^tD^{-1/2}v_kv_k^TD^{1/2}\]</span> we see that lazy random walk always converges to the stationary distribution, with rate <span class="math inline">\((1+\lambda_2(M))/2&gt;\lambda_2\)</span>, which makes sense that lazy walk is slower.</p><p>Thus the mixing rate is closely related to <span class="math inline">\(\lambda_2\)</span>, or equivalently, the spectral gap <span class="math inline">\(\lambda_1-\lambda_2=1-\lambda_2\)</span>. Let <span class="math inline">\(\emptyset\neq S\subset V\)</span> and <span class="math inline">\(\nabla(S)\)</span> be the set of edges connecting <span class="math inline">\(S\)</span> to <span class="math inline">\(V\backslash S=\bar{S}\)</span>, i.e. <span class="math inline">\(\nabla(S)\)</span> is the edge cut of the graph with respect to <span class="math inline">\(S\)</span> (see this <a href="http://yueqicao.top/2020/03/10/Spectral-Graph-Partitioning/">post</a> ). Define the conductance of the set <span class="math inline">\(S\)</span> by <span class="math display">\[\Phi(S)=\frac{|\nabla(S)|}{2m\pi(S)\pi(V\backslash S)}\]</span> Here <span class="math inline">\(\pi(S)=\sum_{s\in S}d(s)/2m=vol(S)/2m\)</span>. The conductance of the graph is defined by <span class="math display">\[\Phi=\min_S\Phi(S)\]</span> It turns out that the spectral gap is controlled by the graph conductance.</p><blockquote><p><span class="math inline">\(\frac{\Phi^2}{16}\le 1-\lambda_2\le \Phi\)</span></p></blockquote><p>Recall that the normalized Laplacian is <span class="math inline">\(\mathcal{L}=D^{-1/2}LD^{-1/2}=I-N\)</span>, which implies <span class="math inline">\(\lambda(\mathcal{L})=1-\lambda(N)=1-\lambda(M)\)</span>. The second largest eigenvalue of <span class="math inline">\(M\)</span> is the second smallest eigenvalue of <span class="math inline">\(\mathcal{L}\)</span>. By Rayleigh-Ritz theorem, <span class="math display">\[1-\lambda_2=\min_{f\perp D^{1/2}\mathbf{1}}\frac{f^T\mathcal{L}f}{f^Tf}\]</span> Let <span class="math inline">\(g=D^{-1/2}f\)</span> and set <span class="math inline">\(g^TDg=1\)</span>. By the basic property of graph Laplacian, <span class="math display">\[1-\lambda_2=\min\{\sum_{i\sim j}(x_i-x_j)^2:\sum_i d(i)x_i=0,\sum_i d(i)x_i^2=1\}\]</span> Let <span class="math inline">\(S\)</span> be the set with minimum conductance, and consider the vector given by <span class="math display">\[x_i=\left\{\begin{array}{l}a,i\in S\\ b,i\in\bar{S}\end{array}\right.\]</span> where <span class="math display">\[a=\sqrt{\frac{\pi(\bar{S})}{2m\pi(S)}},b=-\sqrt{\frac{\pi(S)}{2m\pi(\bar{S})}}\]</span> Then <span class="math display">\[\sum_{i\sim j}(x_i-x_j)^2=\sum_{i\in S,j\in\bar{S}}\frac{\pi(\bar{S})}{2m\pi(S)}+\frac{\pi(S)}{2m\pi(\bar{S})}+\frac{1}{m}=\frac{|\nabla(S)|}{2m\pi(S)\pi(\bar{S})}=\Phi\]</span> This proves the upper bound. To obtain the lower bound, we first prove the following lemma</p><blockquote><p>Let <span class="math inline">\(y\in\mathbb{R}^{|V|}\)</span> such that <span class="math inline">\(\pi(\{i:y_i&gt;0\})\le 1/2\)</span>, <span class="math inline">\(\pi(\{i:y_i&lt;0\})\le 1/2\)</span> and <span class="math inline">\(\sum_i\pi(i)|y_i|=1\)</span>. Then <span class="math inline">\(\sum_{i\sim j}|y_i-y_j|\ge m\Phi\)</span>.</p></blockquote><p>Sort the vector <span class="math inline">\(y\)</span> such that <span class="math inline">\(y_1\le y_2\le\cdots\le y_t&lt;0=y_{t+1}=\cdots=y_s&lt;y_{s+1}\le\cdots y_n\)</span>. Consider the sets defined by <span class="math inline">\(S_i=\{1,\cdots,i\}\)</span>. For a fixed node <span class="math inline">\(i\)</span>, in the sum <span class="math inline">\(\sum_{i\sim j}|y_j-y_i|\)</span> the quantity <span class="math inline">\(y_{i+1}-y_i\)</span> is counted for <span class="math inline">\(|\nabla(S_i)|\)</span> times if we substitute <span class="math inline">\(|y_j-y_i|\)</span> by <span class="math inline">\(y_{j}-y_{j-1}+\cdots+y_{i+1}-y_i\)</span>. We have <span class="math display">\[\sum_{i\sim j}|y_i-y_j|=\sum_{i=1}^{n-1}|\nabla(S_i)|(y_{i+1}-y_i)\ge2m\Phi\sum_{i=1}^{n-1}(y_{i+1}-y_i)\pi(S_i)\pi(\bar{S}_i)\]</span> Since <span class="math inline">\(\pi(S_i)\le 1/2\)</span> for <span class="math inline">\(i\le t\)</span> and <span class="math inline">\(\pi(S_i)\ge 1/2\)</span> for <span class="math inline">\(i\ge s+1\)</span>, we have <span class="math display">\[\begin{aligned}\sum_{i\sim j}|y_i-y_j|&amp;\ge m\Phi\sum_{i=1}^t(y_{i+1}-y_i)\pi(S_i)+m\Phi\sum_{i=s}^{n-1}(y_{i+1}-y_i)\pi(\bar{S}_i)\\&amp;=m\Phi\sum_{i=1}^t(-y_i)\pi(i)+m\Phi\sum_{i=s+1}^{n}y_i\pi(i)\\&amp;=m\Phi\sum_{i=1}^n|y_i|\pi(i)=m\Phi\end{aligned}\]</span> Return to the proof of lower bound. Let <span class="math inline">\(x\)</span> be any vector satisfying <span class="math display">\[\sum_i d(i)x_i=0,\sum_id(i)x_i^2=1\]</span> Assume <span class="math inline">\(x_1\ge \cdots\ge x_n\)</span>. Let <span class="math inline">\(k\)</span> be the index such that <span class="math inline">\(\pi(\{1,\cdots,k-1\})\le 1/2\)</span> and <span class="math inline">\(\pi(\{1,\cdots,k\})&gt;1/2\)</span>. Therefore, <span class="math inline">\(x_1-x_k\ge\cdots\ge x_{k-1}-x_k\ge0\ge x_{k+1}-x_k\ge\cdots\ge x_n-x_k\)</span>. Set <span class="math inline">\(z_i=\max\{0,x_i-x_k\}\)</span>. Then <span class="math inline">\(z_i=0\)</span> for <span class="math inline">\(i\ge k\)</span>. <span class="math display">\[\sum_i \pi_i(x_i-x_k)^2=\sum_{i}\pi_iz_i^2+\sum_{i&gt;k}\pi_i(x_i-x_k)^2\]</span> By replacing <span class="math inline">\(x\)</span> with <span class="math inline">\(-x\)</span> we may assume <span class="math inline">\(\sum_{i}\pi_iz_i^2\ge\sum_{i&gt;k}\pi_i(x_i-x_k)^2\)</span>. Thus we have <span class="math display">\[\sum_i\pi_iz_i^2\ge \frac{1}{2}\sum_i\pi_i(x_i-x_k)^2\ge\frac{1}{4m}\]</span> Apply the above lemma to <span class="math inline">\(y_i=z_i^2/\sum_i\pi_iz_i^2\)</span>. We obtain <span class="math display">\[\sum_{i\sim j}|z^2_i-z_j^2|\ge m\Phi\sum_i\pi_iz_i^2\]</span> On the other hand, by Cauchy inequality <span class="math display">\[(\sum_{i\sim j}|z_i^2-z_j^2|)^2\le(\sum_{i\sim j}(z_i-z_j)^2)(\sum_{i\sim j}(z_i+z_j)^2)\]</span> Furthermore, <span class="math display">\[\sum_{i\sim j}(z_i+z_j)^2\le 2\sum_{i\sim j}z_i^2+z_j^2=\sum_{i,j}a_{ij}(z_i^2+z_j^2)=4m\sum_i\pi_iz_i^2\]</span> Combining these results together, we have <span class="math display">\[\sum_{i\sim j}(z_i-z_j)^2\ge \frac{m^2\Phi^2}{4m}\sum_i\pi_iz_i^2\ge\frac{\Phi^2}{16}\]</span> The theorem follows from the fact that <span class="math inline">\(\sum_{i\sim j}(x_i-x_j)^2\ge\sum_{i\sim j}(z_i-z_j)^2\)</span>.</p><p>We remark that conductance is similar to what is so called <a href="https://en.wikipedia.org/wiki/Cheeger_constant_(graph_theory)" target="_blank" rel="noopener">Cheeger constant</a> defined as <span class="math display">\[h_G=\min_S\frac{|\nabla(S)|}{2m\min\{\pi(S),\pi(\bar{S})\}}\]</span> the lower bound lemma goes without modification for <span class="math inline">\(h_G\)</span>. Thus we can feel free to replace <span class="math inline">\(\Phi\)</span> with <span class="math inline">\(h_G\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> Network-Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Random Walk </tag>
            
            <tag> Spectral Graph Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spectral Graph Partitioning</title>
      <link href="/2020/03/10/Spectral-Graph-Partitioning/"/>
      <url>/2020/03/10/Spectral-Graph-Partitioning/</url>
      
        <content type="html"><![CDATA[<p>Informally, the problem of graph partitioning, or graph clustering, can be stated as follows: we want to partition a graph into several parts such that there are fewer edges between groups but more edges within groups. If there is a measurement on edges, i.e. the edges are weighted, then it can also be stated that we want a partition that nodes within groups are more similar. Graph partitioning can be encountered in many areas such as <a href="http://www.stat.cmu.edu/~larry/=sml/" target="_blank" rel="noopener">statistical learning</a>, <a href="http://www.leonidzhukov.net/hse/2015/networks/" target="_blank" rel="noopener">network analysis</a>, <a href="https://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">graph learning</a>. Other than data science, it is also related to graph theory and computer vision. One recalls that there is a classical problem called <a href="https://en.wikipedia.org/wiki/Minimum_cut" target="_blank" rel="noopener">minimum cut</a> in graph theory, which can be solved in polynomial time by the <a href="%5Bhttps://en.wikipedia.org/wiki/Stoer%E2%80%93Wagner_algorithm%5D(https://en.wikipedia.org/wiki/StoerWagner_algorithm)">Stoer-Wagner algorithm</a>. Graph partitioning can be regarded as a strengthened min-cut problem, since we not only want to find the minimal cut but also an 'equitable' partition.</p><p><a href="https://en.wikipedia.org/wiki/Spectral_clustering" target="_blank" rel="noopener">Spectral clustering</a> is one of the most popular approaches to solve graph partitioning. It has a long history which can date back to 1970s and it is popularized in data science as a machine learning method. Results obtained by spectral clustering often outperform traditional methods. Moreover, it is very simple to implement and the mathematics is nothing but standard linear algebra. A comprehensive <a href="https://arxiv.org/pdf/0711.0189.pdf" target="_blank" rel="noopener">tutorial</a> written by U. von Luxburg provides many different aspects to understand spectral clustering including random walk, perturbation theory and so on. Here we may cover the basics from the easiest way: linear algebra. <a id="more"></a></p><figure><img src="/2020/03/10/Spectral-Graph-Partitioning/demo.jpg" alt="demo"><figcaption aria-hidden="true">demo</figcaption></figure><p>Suppose we want to partition the graph into two parts. Let <span class="math inline">\(A\subset V\)</span> be a subset and <span class="math inline">\(\bar{A}\)</span> be the compliment of <span class="math inline">\(A\)</span> in <span class="math inline">\(V\)</span>. Define the (unnormalized) graph cut to be <span class="math display">\[cut(A,\bar{A})=\sum_{i\in A,j\in\bar{A}}w_{ij}\]</span> where <span class="math inline">\(w_{ij}\)</span> is the weight of edge <span class="math inline">\(e_{ij}\)</span>. <span class="math inline">\(w_{ij}=0\)</span> if and only if <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are not connected. Let <span class="math inline">\(s\)</span> be the indicator vector such that if <span class="math inline">\(v_i\in A\)</span> then <span class="math inline">\(s_i=1\)</span> and if <span class="math inline">\(v_i\notin A\)</span> then <span class="math inline">\(s_i=-1\)</span>. We can rewrite graph cut as <span class="math display">\[\begin{equation}\begin{aligned}cut(A,\bar{A})&amp;=\frac{1}{4}\sum_{i\in A,j\in\bar{A}}w_{ij}(s_i-s_j)^2\\&amp;=\frac{1}{8}\sum_{i,j}w_{ij}(s_i-s_j)^2\\&amp;=\frac{1}{4}\sum_{i,j}((\sum_{j}w_{ij})s_i^2\delta_{ij}-w_{ij}s_is_j)\end{aligned}\end{equation}\]</span> Let <span class="math inline">\(D\)</span> be the diagonal matrix such that <span class="math inline">\(d_{ii}=\sum_{j}w_{ij}\)</span>. The graph Laplacian is defined as <span class="math inline">\(L=D-W\)</span>. From the computation we see that <span class="math inline">\(s^TLs=\frac{1}{2}\sum_{ij}w_{ij}(s_i-s_j)^2\)</span> for any vector <span class="math inline">\(s\)</span> and the graph cut is <span class="math inline">\(cut(A,\bar{A})=\frac{1}{4}s^TLs\)</span>. By construction <span class="math inline">\(\sum_i s_i^2=n\)</span>. In addition, we want 'equitable' partition. Therefore we have the so called balanced cut constraint <span class="math inline">\(\sum_i s_i=0\)</span>. i.e. <span class="math inline">\(|A|=|\bar{A}|\)</span>. This integer minimization problem is, however, NP-hard. An accurate solution is not attainable. So we need to find ways to approximate the solution. One way is to relax the indicator vector <span class="math inline">\(s\)</span> to be any vector <span class="math inline">\(x\in\mathbb{R}^n\)</span> satisfying the constraints. Then we can deal with a much easier continuous optimization problem. <span class="math display">\[\begin{equation}\begin{aligned}\min\quad&amp; Q(x)=\frac{1}{4}x^TLx\\s.t.\quad&amp; \sum_i x_i=0,\sum_i x_i^2=n\end{aligned}\end{equation}\]</span> Let <span class="math inline">\(\mathbf{1}\)</span> be the constant vector. The first constraint <span class="math inline">\(\sum_i x_i=0\)</span> is equivalent to the condition <span class="math inline">\(x\perp \mathbf{1}\)</span>. The second constraint is equivalent to that <span class="math inline">\(x\)</span> is on a sphere. Then optimization is equivalent to<br><span class="math display">\[\begin{equation}\min_{x\perp \mathbf{1}}\frac{x^TLx}{x^Tx}\end{equation}\]</span> By Rayleigh-Ritz theorem, we just pick the eigenvector with respect to the second least eigenvalue (note that <span class="math inline">\(\mathbf{1}\)</span> corresponds to the eigenvalue 0 of <span class="math inline">\(L\)</span>).</p><p>The last step is to revert the eigenvector <span class="math inline">\(x\)</span> to the indicator vector <span class="math inline">\(s\)</span>. A simple criterion is to map positive numbers to <span class="math inline">\(1\)</span> and negative numbers to <span class="math inline">\(-1\)</span>. In general, one can apply <span class="math inline">\(k\)</span>-means clustering to <span class="math inline">\(x\)</span> (here <span class="math inline">\(k=2\)</span>), which is a common method if we want to obtain a <span class="math inline">\(k\)</span> partition.</p><p>The relaxation from integral optimization to real optimization seems to weaken the restriction <span class="math inline">\(|A|=|\bar{A}|\)</span>. For integral optimization it is equivalent to <span class="math inline">\(\sum s_i=0\)</span>. However, for real optimization <span class="math inline">\(\sum x_i=0\)</span> may not imply <span class="math inline">\(|\{x_i&gt;0\}|=|\{x_i&lt;0\}|\)</span>. One way to overcome this shortcoming is to place the balance restriction to the objective function. We define the ratio cut <span class="math display">\[\begin{equation}Rcut(A,\bar{A})=\frac{cut(A,\bar{A})}{|A|}+\frac{cut(A,\bar{A})}{|\bar{A}|}\end{equation}\]</span> Define the indicator vector <span class="math inline">\(f=(f_1,\cdots,f_n)\)</span> such that <span class="math display">\[\begin{equation}f_i=\left\{\begin{array}{l}\sqrt{|\bar{A}|/|A|},v_i\in A\\ -\sqrt{|A|/|\bar{A}|},v_i\in\bar{A} \end{array}\right.\end{equation}\]</span> Then the ratio cut is equivalent to <span class="math display">\[\begin{equation}Rcut(A,\bar{A})=\frac{1}{n}f^TLf\end{equation}\]</span> By definition <span class="math inline">\(f\)</span> satisfies <span class="math display">\[\begin{equation}\sum_i f_i=0, \|f\|=\sqrt{n}\end{equation}\]</span> When we use the same relaxation method we can find that the problem is the same as (3). Therefore, we have not made any progress than what we have done at first.</p><p>However, one can easily generalize the ratio cut to arbitrary <span class="math inline">\(k\)</span>-partition. We define <span class="math display">\[\begin{equation}Rcut(A_1,\cdots, A_k)=\sum_{i=1}^k\frac{cut(A_i,\bar{A}_i)}{|A_i|}\end{equation}\]</span> Unlike the previous methods, we define <span class="math inline">\(k\)</span> indicator vectors <span class="math inline">\(h^1,\cdots, h^k\)</span>. For each <span class="math inline">\(h^i\)</span>, <span class="math inline">\(h^i_j=1/\sqrt{|A_i|}\)</span> if <span class="math inline">\(v_j\in A_i\)</span> and <span class="math inline">\(h^i_j=0\)</span> otherwise. Let <span class="math inline">\(H=[h^1,\cdots,h^k]\)</span> be the <span class="math inline">\(n\times k\)</span> matrix. It satisfies <span class="math inline">\(H^TH=I_{k\times k}\)</span>. A direct computation shows that <span class="math display">\[\begin{equation}Rcut(A_1,\cdots,A_k)=Tr(H^TLH)\end{equation}\]</span> Therefore, we come to the optimization <span class="math display">\[\begin{equation}\begin{aligned}\min\quad&amp; Tr(H^TLH)\\s.t\quad&amp; H^TH=I_{k\times k}\end{aligned}\end{equation}\]</span> By Rayleigh-Ritz theorem, the minimizer is the matrix consisting the eigenvectors corresponding to the <span class="math inline">\(k\)</span> least eigenvalues. The last problem is that how to revert the matrix <span class="math inline">\(H\)</span> to partitions of the graph. View <span class="math inline">\(H\)</span> as <span class="math inline">\(n\)</span> points in the <span class="math inline">\(k\)</span> plane <span class="math inline">\(\mathbb{R}^k\)</span>. That is, we construct a 'dimension reduction' map <span class="math inline">\(v_i\to H_i\)</span>. Then we can cluster the points using <span class="math inline">\(k\)</span>-means clustering.</p><p>In ratio cut, the size of partition is measured by the number of vertices. One can also measure the size by the total weights of edges. Let <span class="math inline">\(vol(A)=\sum_{i\in A}d_{ii}\)</span>. The normalized cut is <span class="math display">\[\begin{equation}Ncut(A_i,\cdots,A_k)=\sum_{i=1}^k\frac{cut(A_i,\bar{A}_i)}{vol(A_i)}\end{equation}\]</span> Similarly, we define <span class="math inline">\(k\)</span> indicator vectors <span class="math inline">\(h^1,\cdots, h^k\)</span> such that <span class="math inline">\(h^i_j=1/\sqrt{vol(A_i)}\)</span> for <span class="math inline">\(v_i\in A_i\)</span> and 0 otherwise. Let <span class="math inline">\(H=[h^1,\cdots,h^k]\)</span>. Then <span class="math inline">\(H^TDH=I\)</span>, and <span class="math display">\[\begin{equation}Ncut(A_1,\cdots,A_k)=Tr(H^TLH)\end{equation}\]</span> Let <span class="math inline">\(T=D^{1/2}H\)</span> and <span class="math inline">\(L_s=D^{-1/2}LD^{-1/2}\)</span> be the normalized graph Laplacian. We want to solve <span class="math display">\[\begin{equation}\begin{aligned}\min\quad&amp; Tr(T^TL_sT)\\s.t\quad&amp; T^TT=I_{k\times k}\end{aligned}\end{equation}\]</span> which is in the same form as ratio cut. To revert to the discrete case, we apply <span class="math inline">\(k\)</span> means clustering to <span class="math inline">\(H\)</span>.</p><p>One remarkable thing about normalized cut is that it has a beautiful explanation from the perspective of random walk. Let <span class="math inline">\(p_{ij}=w_{ij}/d_i\)</span> be the one-step transition probability from node <span class="math inline">\(i\)</span> to node <span class="math inline">\(j\)</span>. If the graph is connected and non-bipartite, this random walk will always possess a stationary distribution <span class="math inline">\(\pi\)</span> such that <span class="math inline">\(\pi_i=d_i/vol(V)\)</span>. Let <span class="math inline">\(P(A|B)=P(v_i\in A|v_j\in B)\)</span> denote the probability that points in <span class="math inline">\(B\)</span> walk to <span class="math inline">\(A\)</span> in one step. Then <span class="math display">\[\begin{equation}\begin{aligned}P(A,\bar{A})&amp;=\sum_{i\in A,j\in \bar{A}}\pi_ip_{ij}\\&amp;=\sum_{i\in A,j\in \bar{A}}\frac{d_i}{vol(V)}\frac{w_{ij}}{d_i}\\&amp;=\sum_{i\in A,j\in \bar{A}}\frac{w_{ij}}{vol(V)}\end{aligned}\end{equation}\]</span> Therefore, <span class="math display">\[\begin{equation}\begin{aligned}P(A|\bar{A})&amp;=\frac{P(A,\bar{A})}{P(\bar{A})}\\&amp;=\frac{\sum_{i\in A,j\in\bar{A}}w_{ij}/vol(V)}{vol(A)/vol(V)}\\&amp;=\sum_{i\in A,j\in\bar{A}}\frac{w_{ij}}{vol(A)}\\&amp;=\frac{cut(A,\bar{A})}{vol(A)}\end{aligned}\end{equation}\]</span> We see that <span class="math inline">\(Ncut(A,\bar{A})=P(A|\bar{A})+P(\bar{A}|A)\)</span>. When we are minimizing the normalized cut we are trying to find a partition such that points have low probability that transfer from one community to the other.</p><figure><img src="/2020/03/10/Spectral-Graph-Partitioning/spec.jpg" alt="spec"><figcaption aria-hidden="true">spec</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> Network-Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spectral Graph Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Forman-Ricci Curvature on Complexes</title>
      <link href="/2020/02/16/Forman-Ricci-Curvature-on-Complexes/"/>
      <url>/2020/02/16/Forman-Ricci-Curvature-on-Complexes/</url>
      
        <content type="html"><![CDATA[<h2 id="bochner-weitzenbbfddotock-formula-on-riemannian-manifolds">Bochner-Weitzenb<span class="math inline">\(\bf\ddot{o}\)</span>ck Formula on Riemannian Manifolds</h2><p>Let <span class="math inline">\((M,g)\)</span> be an <span class="math inline">\(n\)</span>-dimensional Riemannian manifold, and <span class="math inline">\(\nabla\)</span> be the Levi-Civita connection. Recall that for a smooth function (or 0-form) <span class="math inline">\(f\in C^\infty(M)=\mathcal A^0(M)\)</span>, the <em>Laplacian of smooth functions</em> is defined as <span class="math display">\[\Delta_0(f)=\text{tr}\nabla^2(f)=\text{div}(\text{grad}(f))\]</span> The definition <span class="math inline">\(\Delta_0=\text{tr}\nabla^2\)</span> can be generalized to arbitrary <span class="math inline">\(p\)</span>-forms and is called the <em>connection Laplacian</em>. Suppose in addition <span class="math inline">\(M\)</span> is oriented. Let <span class="math inline">\(d:\mathcal A^p(M)\to\mathcal A^{p+1}(M)\)</span> be the differential operator. We can define the adjoint operator <span class="math inline">\(d^{\*}\)</span> of <span class="math inline">\(d\)</span> using the <em>Hodge star operator</em>. i.e. <span class="math inline">\(d^{\*}:\mathcal A^{p+1}(M)\to\mathcal A^p(M)\)</span> is the operator satisfying <span class="math inline">\(\langle d\alpha,\beta\rangle=\langle\alpha,d^{\*}\beta\rangle=\int_Md\alpha\wedge {\*}\beta\)</span> for <span class="math inline">\(\alpha\in\mathcal A^p(M)\)</span> and <span class="math inline">\(\beta\in\mathcal A^{p+1}\)</span>. The <em>Hodge Laplacian</em> is defined by <span class="math display">\[\Delta=dd^*+d^*d\]</span> By direct computation, we can see that <span class="math inline">\(\Delta=-\Delta_0\)</span> on <span class="math inline">\(C^\infty(M)=\mathcal A^0(M)\)</span>. However, it is much more complicated in higher dimensions.<a id="more"></a> The <em>Bochner-Weitzenb<span class="math inline">\(\ddot{o}\)</span>ck formula</em> tells that, on <span class="math inline">\(\mathcal A^p(M)\)</span> for <span class="math inline">\(p&gt;0\)</span>, a term involving curvature tensor should be added. More precisely, we have <span class="math display">\[\Delta=-\Delta_0+\sum_{i,j}\omega^i\wedge i_{E_j}R_{E_iE_j}\]</span> where <span class="math inline">\(E_1,\cdots,E_n\)</span> is a local orthonormal frame and <span class="math inline">\(w^1,\cdots,\omega^n\)</span> is the corresponding coframe. <span class="math inline">\(i_{E_j}\)</span> is the interior multiplication and <span class="math inline">\(R_{E_iE_j}=\nabla_{[E_i,E_j]}-[\nabla_{E_i},\nabla_{E_j}]\)</span> is the curvature tensor. For 1-forms the following equation is commonly presented in textbooks. <span class="math display">\[\frac{1}{2}\Delta_0|\omega|^2=|\nabla\omega|^2-\langle\Delta\omega,\omega\rangle+\text{Ric}(\omega_*,\omega_*)\]</span> Using this formula one is able to prove the following Bochner's theorem</p><blockquote><p>Let <span class="math inline">\(M\)</span> be a compact, oriented Riemannian manifold, whose Ricci curvature tensor is nonnegative and positive at one point. Then <span class="math inline">\(H^1(M;\mathbb R)\)</span>=0.</p></blockquote><p>From Hodge theorem it suffices to prove that every harmonic 1-form is zero. Let <span class="math inline">\(\omega\)</span> be a harmonic 1-form. Integrate equation (4) on both sides and note that on any compact manifold we have <span class="math inline">\(\int_M \Delta_0(f)dv=0\)</span>. We have <span class="math display">\[\nabla\omega=0 \text{ and } \text{Ric}(\omega_*,\omega_*)=0\]</span> For any vector field <span class="math inline">\(X\)</span> we have <span class="math inline">\(X|\omega|^2=2\langle\nabla_X\omega,\omega\rangle=0\)</span>. Hence <span class="math inline">\(|\omega|\)</span> is constant. If <span class="math inline">\(\omega\neq 0\)</span> at one point then <span class="math inline">\(\omega\neq 0\)</span> on <span class="math inline">\(M\)</span>. But <span class="math inline">\(\text{Ric}\)</span> is positive at one point which means at this point <span class="math inline">\(\text{Ric}(\omega_\*,\omega_\*)&gt;0\)</span>, which is a contradiction.</p><p>This 'standard' proof is not the same as Bochner's original proof. In fact, Bochner defined Laplacian in the following way. Since <span class="math inline">\(\nabla:\Gamma(\bigwedge^\*(M))\to\Gamma(\bigwedge^\*(M)\otimes T^\*M)\)</span> is a linear map between inner product spaces, it admits an adjoint <span class="math inline">\(\nabla^\*\)</span>. The <a href="https://en.wikipedia.org/wiki/Laplace_operators_in_differential_geometry" target="_blank" rel="noopener"><em>Bochner Laplacian</em></a> is defined by <span class="math inline">\(\Delta_B=\nabla^\*\nabla\)</span>, which is also called <em>rough Laplacian</em> in literature (see <a href="https://www.springer.com/gp/book/9783540653172" target="_blank" rel="noopener">Berger</a>). It is easy to see that <span class="math inline">\(\Delta_B\)</span> is nonnegative definite and <span class="math inline">\(\text{Ker}(\Delta_B)=\text{Ker}(\nabla)\)</span> whose elements are parallel forms. By computation one verifies that <span class="math inline">\(\Delta_B=-\Delta_0\)</span>. Therefore, <span class="math display">\[\Delta=\Delta_B+\text{Curv}(R)\]</span> If <span class="math inline">\(\text{Curv}(R)\)</span> is nonnegative, then <span class="math inline">\(\text{Ker}(\Delta)=\text{Ker}(\Delta_B)\cap\text{Ker}(\text{Curv}(R))\)</span>. But parallel forms are completely determined at one point. Therefore, if <span class="math inline">\(\text{Curv}(R)\)</span> is positive at one point, the harmonic forms will be identically zero.</p><h2 id="formans-discretization-of-ricci-curvature">Forman's Discretization of Ricci Curvature</h2><p>Bochner's proof is carefully studied by <a href="https://link.springer.com/article/10.1007%2Fs00454-002-0743-x" target="_blank" rel="noopener">Robin Forman</a> so as to place a combinatorial analogy in discrete cases. In 2003, a series of combinatorial invariants for quasiconvex CW complexes were proposed, named 'curvature' by Forman. In the simplest case, suppose <span class="math inline">\(M\)</span> is a simplicial complex. For each nonnegative integer <span class="math inline">\(p\)</span>, define the <span class="math inline">\(p\)</span>th curvature function by <span class="math display">\[\mathcal F_p(\alpha)=\#\{(p+1)\text{-cofaces}\}+\#\{(p-1)\text{-faces}\}-\#\{\text{parallel neighbors}\}\]</span> where parallel neighbors of <span class="math inline">\(\alpha\)</span> are <span class="math inline">\(p\)</span>-simplices sharing either a <span class="math inline">\((p+1)\)</span>-coface or <span class="math inline">\((p-1)\)</span>-face but not both. For <span class="math inline">\(p=1\)</span>, <span class="math inline">\(\mathcal F_1\)</span> is called Ricci curvature by Forman, denoted by <span class="math inline">\(\text{Ric}\)</span>. Using this analogy Forman could prove several Bochner-type theorems. For example, one has the following</p><blockquote><p>Let <span class="math inline">\(M\)</span> be a connected weighted quasiconvex CW complex with nonnegative Ricci curvature. Suppose, in addition, there exists a vertex <span class="math inline">\(v\)</span> such that <span class="math inline">\(\text{Ric}(e)&gt;0\)</span> for each coface <span class="math inline">\(e\)</span> of <span class="math inline">\(v\)</span>. Then <span class="math inline">\(H_1(M,\mathbb R)=0\)</span>.</p></blockquote><p>Forman's idea comes from a simple observation. Though the classical Bochner-Weitzenb<span class="math inline">\(\rm\ddot{o}\)</span>ck formula looks rather abstruse, with many abstract operators on Riemannian manifolds involved, but from the simplest perspective, one has <span class="math display">\[\text{Laplacian}=\text{nonnegative definite operator}+\text{curvature}\]</span></p><p>For a simplicial complex <span class="math inline">\(M\)</span> the discrete Laplacian is well defined. i.e. Let <span class="math inline">\(\partial_p:C_p(M;\mathbb R)\to C_{p-1}(M;\mathbb R)\)</span> be the <span class="math inline">\(p\)</span>th boundary operator. Place a metric on each chain vector space by declaring the simplices are orthogonal (<em>It is not necessary that they are orthonormal. Note that when simplices are assigned weights the adjoint operator is no longer the transpose</em>). The boundary operator admits an adjoint <span class="math inline">\(\partial_p^\*:C_{p-1}(M;\mathbb R)\to C_p(M;\mathbb R)\)</span>, such that <span class="math inline">\(\langle\partial_p\alpha,\beta\rangle_{p-1}=\langle\alpha,\partial_p^\*\beta\rangle_p\)</span>. The <em>combinatorial Laplacian</em> is defined by <span class="math display">\[\square_p=\partial_{p+1}\partial_{p+1}^*+\partial_p^*\partial_p:C_p(M;\mathbb R)\to C_p(M;\mathbb R)\]</span> From equation (8) we know we want to decompose <span class="math inline">\(\square_p\)</span> as a sum of a nonnegative definite matrix <span class="math inline">\(B_p\)</span> and a curvature-type matrix <span class="math inline">\(F_p\)</span>. In fact we only need to construct <span class="math inline">\(B_p\)</span>, since we do not know any property of <span class="math inline">\(F_p\)</span> so that we just put <span class="math inline">\(F_p=\square_p-B_p\)</span>.</p><p>The way Forman defined <span class="math inline">\(B_p\)</span> is quite natural. Suppose <span class="math inline">\(A\)</span> is a symmetric matrix. Then <span class="math display">\[\mathbb{B}(A)=\left\{\begin{array}{l}A_{ij},\text{ for }i\neq j\\ \sum_{j\neq i} |A_{ij}|,\text{ for }i=j\end{array}\right.\]</span> is a diagonally dominant matrix, thus is nonnegative definite. <span class="math inline">\(\mathbb B(A)\)</span> is called the Bochner matrix associated to <span class="math inline">\(A\)</span>. Thereafter <span class="math inline">\(\mathbb F(A)=A-\mathbb B(A)\)</span> is called the curvature matrix associated to <span class="math inline">\(A\)</span>. From this perspective we see that 'curvature' measures how <strong>a symmetric matrix deviates from a diagonally dominant matrix</strong>. This decomposition is useful in our case because <span class="math inline">\(\mathbb B(\square)\)</span> preserves the topological (or combinatorial) information of the simplicial complex <span class="math inline">\(M\)</span>. Specifically, if <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are parallel neighbors, then <span class="math inline">\(\square_{\alpha\beta}=\mathbb B(A)_{\alpha\beta}\neq 0\)</span> by checking definition. Let <span class="math inline">\(B\)</span> be a symmetric <span class="math inline">\(n\times n\)</span> matrix. Define an equivalence relation on <span class="math inline">\(\{1,2,\cdots,n\}\)</span> by requiring <span class="math inline">\(i\sim i\)</span> and <span class="math inline">\(i\sim j\)</span> if and only if there is a sequence <span class="math inline">\(i=k_0,k_1,\cdots,k_n=j\)</span> such that <span class="math inline">\(B(k_l,k_{l+1})\neq 0\)</span>. Let <span class="math inline">\(\mathcal C(B)\)</span> be the set of equivalence classes and <span class="math inline">\(\mathcal N(B)=|\mathcal C(B)|\)</span>. The following nontrivial property of diagonally dominant matrices is used by Forman to prove Bochner's theorem for 1-chains.</p><blockquote><p>Let <span class="math inline">\(B\)</span> be a diagonally dominant metrix, then</p><ol type="1"><li><span class="math inline">\(\text{dim(ker)}(B)\le \mathcal N(B)\)</span>;</li><li>Suppose <span class="math inline">\(v=(v_1,\cdots,v_n)\in\text{ker}(B)\)</span>, if <span class="math inline">\(B_{ij}\neq 0\)</span>, <span class="math inline">\(v_j=-sign(B_{ij})v_i\)</span>. i.e. the components in the same equivalence class are completely determined at one element.</li></ol></blockquote><p>It suffices to prove 2. Let <span class="math inline">\(c\in\mathcal C(B)\)</span> be any class. Without loss of generality, assume <span class="math inline">\(v_i=\max_{j\in c}|v_j|\ge0\)</span>. Then we have <span class="math display">\[0=(Bv)_i=\sum_{j}B_{ij}v_j=B_{ii}v_i+\sum_{j\in c,j\neq i}B_{ij}v_j\ge\sum_{j\in c,j\neq i}|B_{ij}|(v_i-|v_j|)\ge0\]</span> The equality holds if and only if <span class="math inline">\(v_j=0\)</span> for all <span class="math inline">\(j\in c\)</span>, or, <span class="math inline">\(B_{ii}=\sum_{j\neq i}|B_{ij}|\)</span> and <span class="math inline">\(v_j=-sign(B_{ij})v_i\)</span> for all <span class="math inline">\(j\neq c\)</span>.</p><p>Recall that for graph Laplacian <span class="math inline">\(L=D-A\)</span> the dimension of kernel is always equal to the connected components. This is because the constant vector <span class="math inline">\(\mathbf{1}\)</span> provides a nontrivial element in the kernel if the graph is connected. However, one can easily construct an invertible matrix <span class="math inline">\(B\)</span> with the diagonal equal to the sum of off-diagonal elements. Thus the first inequality can be strict.</p><p>Let us see how this property can be used in our case. Suppose <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are parallel <span class="math inline">\(p\)</span>-neighbors, which implies <span class="math inline">\(\square_p(\alpha,\beta)\neq 0\)</span> and thus <span class="math inline">\(\mathbb{B}(\square_p)(\alpha,\beta)\neq 0\)</span>. If there is a <span class="math inline">\(p\)</span>-chain <span class="math inline">\(c=\sum c_\gamma\gamma\in\text{Ker}(\mathbb{B}(\square_p))\)</span> with <span class="math inline">\(c_\alpha=0\)</span>, then <span class="math inline">\(c_\beta=0\)</span>. This continuation property will be important in the proof of Bochner-type theorems.</p><p>Now suppose <span class="math inline">\(\mathbb{F}(\square_p)\)</span> is nonnegative definite, which is equivalent to say <span class="math inline">\(\mathbb{F}(\square_p)_{ii}\ge 0\)</span>. Being a sum of two nonnegative definite matrices, we have <span class="math inline">\(\text{Ker}(\square_p)=\text{Ker}(\mathbb{B}(\square_p))\cap\text{Ker}(\mathbb{F}(\square_p))\)</span>. In the simplest case, suppose <span class="math inline">\(\mathbb{F}(\square_p)\)</span> is positive definite, then <span class="math inline">\(\text{Ker}(\square_p)=\{0\}\)</span>. For <span class="math inline">\(p=1\)</span>, we have the familiar statement: if <span class="math inline">\(M\)</span> has positive Ricci curvature (at present call <span class="math inline">\(\mathbb{F}(\square_1)\)</span> the Ricci curvature, which is positive if the diagonal elements are positive), then <span class="math inline">\(H_1(M;\mathbb R)\)</span> is trivial.</p><p>More generally, assume <span class="math inline">\(M\)</span> has nonnegative Ricci curvature and there is a vertex <span class="math inline">\(v\)</span> such that <span class="math inline">\(\text{Ric}(e)&gt;0\)</span> for all <span class="math inline">\(e\succ v\)</span>. Let <span class="math inline">\(c=\sum c_e e\in\text{Ker}(\square_1)=\text{Ker}(\mathbb{B}(\square_1))\cap\text{Ker(Ric)}\)</span>. Since <span class="math inline">\(c\in\text{Ker(Ric)}\)</span> we have <span class="math inline">\(c_e=0\)</span> for all <span class="math inline">\(e\succ v\)</span>. The following lemma shows that <span class="math inline">\(c\)</span> will be identically zero.</p><blockquote><p>Suppose <span class="math inline">\(c=\sum c_e e\)</span> is a 1-chain such that <span class="math inline">\(c\in\text{Ker}(\partial^\*)\cap\text{Ker}(\mathbb{B}(\square_1))\)</span>. In addition, there is a vertex <span class="math inline">\(v\)</span> with <span class="math inline">\(c_e=0\)</span> for all <span class="math inline">\(e\succ v\)</span>. Then <span class="math inline">\(c=0\)</span>.</p></blockquote><p>Define <span class="math inline">\(D:\{\text{1-simplices}\}\to\mathbb{Z}_{\ge 0}\)</span> as: (1). <span class="math inline">\(D(e)=0\)</span> for <span class="math inline">\(e\succ v\)</span>; (2). Inductively, if <span class="math inline">\(D(e)\)</span> is greater than <span class="math inline">\(k\)</span> and there is a 1-simplex <span class="math inline">\(e_1\)</span> such that <span class="math inline">\(e\cap e_1\neq\emptyset\)</span> and <span class="math inline">\(D(e_1)=k\)</span>, set <span class="math inline">\(D(e)=k+1\)</span>.</p><p>From hypothesis <span class="math inline">\(c_e=0\)</span> for <span class="math inline">\(D(e)=0\)</span>. Suppose <span class="math inline">\(c_e=0\)</span> for all <span class="math inline">\(D(e)\le k\)</span>. Let <span class="math inline">\(e\)</span> be a 1-simplex with <span class="math inline">\(D(e)=k+1\)</span>. Then there exists 1-simplex <span class="math inline">\(e_1\)</span> such that <span class="math inline">\(D(e_1)=k\)</span> and <span class="math inline">\(e\cap e_1\neq \emptyset\)</span>. If <span class="math inline">\(e\)</span> and <span class="math inline">\(e_1\)</span> are parallel, by the continuation property of <span class="math inline">\(\mathbb{B}(\square_1)\)</span>, <span class="math inline">\(c_e=0\)</span>. If <span class="math inline">\(e\)</span> and <span class="math inline">\(e_1\)</span> are not parallel, there is a 2-simplex <span class="math inline">\(f\)</span> such that <span class="math inline">\(f\succ e\)</span> and <span class="math inline">\(f\succ e_1\)</span>. Let the last edge of <span class="math inline">\(f\)</span> be <span class="math inline">\(e_2\)</span>. Since <span class="math inline">\(D(e_1)=k\)</span>, either <span class="math inline">\(D(e)=k\)</span> or <span class="math inline">\(D(e_2)=k\)</span>. By assumption <span class="math inline">\(D(e_2)=k\)</span>, thus <span class="math inline">\(c_{e_1}=c_{e_2}=0\)</span>. Then <span class="math display">\[\langle\partial f,c\rangle=\langle f,\partial^*c\rangle=0=\pm c_e\langle e,e\rangle\]</span> Hence <span class="math inline">\(c=0\)</span> by induction.</p><p>Now we give the explicit representation of <span class="math inline">\(\square_p\)</span> and the curvature function <span class="math inline">\(\mathcal F_p\)</span>.</p><p>For each <span class="math inline">\((p+1)\)</span>-simplex <span class="math inline">\(\beta\)</span> with <span class="math inline">\(\partial\beta=\sum_{\alpha\prec\beta}\epsilon_{\alpha\beta}\alpha\)</span> where <span class="math inline">\(\epsilon_{\alpha\beta}=\pm 1\)</span> according to the orientation of <span class="math inline">\(\beta\)</span>. By definition, <span class="math inline">\(\langle\partial^\*\alpha,\beta\rangle=\langle\alpha,\partial\beta\rangle=\epsilon_{\alpha\beta}w_\alpha\)</span> where we assume that <span class="math inline">\(\langle\alpha,\alpha\rangle=w_\alpha\)</span>. Thus <span class="math inline">\(\partial^\*\alpha=\sum_{\beta\succ\alpha}\epsilon_{\alpha\beta}\frac{w_\alpha}{w_\beta}\beta\)</span>. This gives <span class="math display">\[\square_p\alpha_1=\sum_{\alpha_2}[\sum_{\beta\succ\alpha_1,\alpha_2}\epsilon_{\alpha_1\beta}\epsilon_{\alpha_2\beta}\frac{w_{\alpha_1}}{w_\beta}+\sum_{\gamma\prec\alpha_1,\alpha_2}\epsilon_{\gamma\alpha_1}\epsilon_{\gamma\alpha_2}\frac{w_\gamma}{w_{\alpha_2}}]\alpha_2\]</span> We may work on orthonormal basis. Note that for different basis the definition of curvature will be different. Let <span class="math inline">\(\alpha^\*=\alpha/\sqrt{w_\alpha}\)</span>. Then <span class="math display">\[\langle\square_p\alpha_1^*,\alpha_2^*\rangle=\sum_{\beta\succ\alpha_1,\alpha_2}\epsilon_{\alpha_1\beta}\epsilon_{\alpha_2\beta}\frac{\sqrt{w_{\alpha_1}w_{\alpha_2}}}{w_\beta}+\sum_{\gamma\prec\alpha_1,\alpha_2}\epsilon_{\gamma\alpha_1}\epsilon_{\gamma\alpha_2}\frac{w_\gamma}{\sqrt{w_{\alpha_2}w_{\alpha_1}}}\]</span> Denote by <span class="math inline">\(\square_p(\alpha_1^\*,\alpha_2^\*)\)</span>. Then under orthonormal basis, the curvature matrix is <span class="math display">\[\mathbb{F}(\square_p)(\alpha_1^*,\alpha_2^*)=\left\{\begin{array}{l}0,\text{ if }\alpha_1\neq\alpha_2\\ \square_p(\alpha_1^*,\alpha_2^*)-\sum_{\alpha^*\neq\alpha_2^*}|\square_p(\alpha^*,\alpha_1^*)|,\text{ if }\alpha_1=\alpha_2\end{array}\right.\]</span> Define the <span class="math inline">\(p\)</span>th curvature function by <span class="math inline">\(\mathcal{F}_p(\alpha)=w_\alpha\mathbb{F}(\square_p)(\alpha^\*,\alpha^\*)\)</span>. Thus <span class="math display">\[\begin{aligned}\mathcal{F}_p(\alpha)=w_\alpha(&amp;\sum_{\beta\succ\alpha}\frac{w_\alpha}{w_\beta}+\sum_{\gamma\prec\alpha}\frac{w_\gamma}{w_\alpha}\\ &amp;-\sum_{\eta\neq\alpha}|\sum_{\beta\succ\alpha,\eta}\epsilon_{\alpha\beta}\epsilon_{\eta\beta}\frac{\sqrt{w_{\alpha}w_{\eta}}}{w_\beta}+\sum_{\gamma\prec\alpha,\eta}\epsilon_{\gamma\alpha}\epsilon_{\gamma\eta}\frac{w_\gamma}{\sqrt{w_{\alpha}w_{\eta}}}|)\end{aligned}\]</span> When <span class="math inline">\(p=1\)</span> and all weights are 1, the formula simplifies as what is given in the beginning.</p>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Forman Curvature </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Local-to-Global Convexity</title>
      <link href="/2020/01/26/Local-to-Global-Convexity/"/>
      <url>/2020/01/26/Local-to-Global-Convexity/</url>
      
        <content type="html"><![CDATA[<h2 id="convex-functions">Convex Functions</h2><p>Recall that a function <span class="math inline">\(f:I\to \mathbb R\)</span> is called convex if <span class="math display">\[f(tx+(1-t)y)\le tf(x)+(1-t)f(y)\]</span> for any <span class="math inline">\(t\in[0,1]\)</span> and <span class="math inline">\(x,y\in I\)</span> where <span class="math inline">\(I\)</span> is an interval in <span class="math inline">\(\mathbb R\)</span>. In some <a href="https://arxiv.org/pdf/1601.03363.pdf" target="_blank" rel="noopener">context</a>, <span class="math inline">\(f\)</span> is called <span class="math inline">\(p\)</span>-convex if <span class="math inline">\(f^p\)</span> is convex in the meaning of (1). <span class="math inline">\(f\)</span> is called quasi-convex if <span class="math display">\[f(t)\le\max\{f(a),f(b)\}\]</span> for any <span class="math inline">\(t\in [a,b]\)</span>. Note that if <span class="math inline">\(f\)</span> is convex then <span class="math inline">\(f\)</span> is necessarily quasi-convex. Quasi-convex functions are also called peakless in <a href="https://core.ac.uk/display/56641235" target="_blank" rel="noopener">Busemann's articles</a>.</p><p>If <span class="math inline">\(f\)</span> is differentiable, then we have a simple rule to determine its convexity: <span class="math inline">\(f\)</span> is convex if and only if <span class="math inline">\(f&#39;\)</span> is non-decreasing. If <span class="math inline">\(f\)</span> is twice differentiable, then <span class="math inline">\(f\)</span> is convex if and only if <span class="math inline">\(f&#39;&#39;\)</span> is non-negative. From these rules we can see that convexity is a <strong>local</strong> property for differentiable functions. i.e. say that <span class="math inline">\(f\)</span> is locally convex if for any <span class="math inline">\(x\)</span> there is a neighborhood such that <span class="math inline">\(f\)</span> is convex on that neighborhood. If <span class="math inline">\(f\)</span> is differentiable and locally convex, by the above discussions locally convexity implies global convexity.</p><a id="more"></a><p>The question is whether, without the presuppose of differentiability, local convexity implies global convexity. i.e if <span class="math inline">\(f\)</span> is defined on an interval such that any point has a neighborhood where condition (1) holds, is <span class="math inline">\(f\)</span> convex or not? Recall the following sufficient and necessary condition for <span class="math inline">\(f\)</span> being convex: <span class="math inline">\(f\)</span> is convex if and only if its epigraph is convex, where the epigraph of <span class="math inline">\(f\)</span> is defined by <span class="math display">\[epi(f)=\{(t,y)|y\ge f(t)\}\]</span> We see that local convexity is equivalent to that each point in the epigraph has a neighborhood which is convex. Thus we come to the question: Let <span class="math inline">\(X\)</span> be a locally convex set in <span class="math inline">\(\mathbb R^n\)</span>, is itself convex?</p><h2 id="tietze-nakajima-theorem">Tietze-Nakajima Theorem</h2><p>The above question is answered by the classical <a href="https://arxiv.org/pdf/math/0701745.pdf" target="_blank" rel="noopener">Tietze-Nakajima theorem</a>.</p><blockquote><p>Let <span class="math inline">\(X\)</span> be a closed subset in <span class="math inline">\(\mathbb R^n\)</span>. <span class="math inline">\(X\)</span> is convex if and only if <span class="math inline">\(X\)</span> is connected and locally convex.</p></blockquote><p>Necessity is clear since convexity implies connectedness and local convexity. Note that local convexity implies local path connectedness. Thus <span class="math inline">\(X\)</span> being connected is equivalent to <span class="math inline">\(X\)</span> being path connected. We can define a length structure on <span class="math inline">\(X\)</span> by defining admissible paths to be polygonal paths (the existence of polygonal paths is guaranteed by local convexity) and length to be the canonical length in Euclidean spaces. The induced metric on <span class="math inline">\(X\)</span> is <em>midpoint convex</em> (a special case of <em>Menger convex</em>). i.e. For any two point in <span class="math inline">\(X\)</span> there exists a midpoint (The existence of midpoint relies on the closeness and local convexity of <span class="math inline">\(X\)</span>). Since <span class="math inline">\(X\)</span> is complete, by a theorem of <a href="https://www.amazon.com/Course-Metric-Geometry-Dmitri-Burago/dp/0821821296" target="_blank" rel="noopener">BBI</a>, <span class="math inline">\(X\)</span> is strictly intrinsic. i.e. the induced metric coincides with the usual Euclidean metric and there is a shortest path, which is clearly a straight line, connecting any two point. Therefore <span class="math inline">\(X\)</span> is convex.</p><p>Tietze-Nakajima theorem implies that the convexity of functions is a local property. In contrast, quasi-convexity is <strong>not</strong> a local property. i.e if a function is local quasi-convex, it may not be a quasi-convex function globally. For a counterexample, consider the exterior of ''.</p><p>From the proof we see that the local-to-global convexity property for Euclidean spaces should have a natural generalization to distance geometry. However, one should be careful with the notion of 'convexity' and the choice of 'proper' spaces. See a series of questions discussed in StackExchange (<a href="https://math.stackexchange.com/questions/3517202/does-convexity-implies-contractibility-in-length-space" target="_blank" rel="noopener">Does convexity implies contractibility in length space?</a>; <a href="https://math.stackexchange.com/questions/479022/is-a-uniquely-geodesic-space-contractible-i" target="_blank" rel="noopener">Is a uniquely geodesic space contractible? I</a>; <a href="https://math.stackexchange.com/questions/481569/on-continuously-uniquely-geodesic-space" target="_blank" rel="noopener">On continuously uniquely geodesic space</a>; <a href="https://mathoverflow.net/questions/252605/are-small-varepsilon-balls-convex-in-geodesic-metric-spaces" target="_blank" rel="noopener">Are small <span class="math inline">\(\epsilon\)</span>-balls convex in geodesic metric spaces?</a>)</p><h2 id="generalized-hadamard-cartan-theorem">Generalized Hadamard-Cartan Theorem</h2><p>M. Gromov stated a theorem which generalized the classical <a href="https://en.wikipedia.org/wiki/Cartan%E2%80%93Hadamard_theorem" target="_blank" rel="noopener">Hadamard-Cartan theorem</a> in Riemannian geometry:</p><blockquote><p>A simply-connected, complete, locally convex geodesic space is globally convex; hence any two points are joined by a unique geodesic.</p></blockquote><p>A <strong>geodesic space</strong> is a space where any two points can be joined by a shortest path. A subset <span class="math inline">\(A\)</span> is <strong>convex</strong> if for any two scaled geodesics <span class="math inline">\(\alpha,\beta:[0,1]\to A\)</span> the function <span class="math inline">\(d(\alpha(t),\beta(t))\)</span> is convex. If <span class="math inline">\(m_{pq}\)</span> denotes the midpoint of a geodesic from <span class="math inline">\(p\)</span> to <span class="math inline">\(q\)</span>. Then convexity is equivalent to <span class="math inline">\(2d(m_{pq},m_{pr})\le d(q,r)\)</span> for any three distinct points. Thus the generalized Hadamard-Cartan theorem shows that local-to-global convexity holds for simply-connected, complete, geodesic spaces. A detailed proof of this theorem is given by <a href="https://www.researchgate.net/publication/257947117_The_Hadamard-Cartan_theorem_in_locally_convex_metric_spaces" target="_blank" rel="noopener">S. Alexander and R. Bishop</a> in 1990.</p><p>Let <span class="math inline">\(m\in X\)</span> be a fixed point in the geodesic space. Let <span class="math inline">\(\mathbb G_m\)</span> be the space of geodesics starting at <span class="math inline">\(m\)</span>, carrying the uniform distance <span class="math inline">\(\mathbf d\)</span>. <span class="math inline">\(m\)</span> has no <em>conjugate points</em> if the endpoint map <span class="math display">\[EP:\mathbb G_m\to X\]</span> defined by <span class="math inline">\(EP(\gamma)=\gamma(1)\)</span>, is a local homeomorphism. i.e. <span class="math inline">\(EP\)</span> maps a neighborhood of each <span class="math inline">\(\gamma\)</span> homeomorphically onto a neighborhood of <span class="math inline">\(\gamma(1)\)</span>. That is to say, each point has a neighborhood such that the geodesics vary continuously. The first theorem shown by AB is that</p><blockquote><p>A locally convex, complete geodesic space has no conjugate point.</p></blockquote><p>Using local convexity, it is easy to prove that <span class="math inline">\(EP\)</span> is an isometry on a neighborhood of <span class="math inline">\(\gamma\)</span> onto <strong>its image</strong>. The question is that whether this image is a neighborhood of <span class="math inline">\(\gamma(1)\)</span>. AB proved that, in fact, <span class="math inline">\(EP\)</span> is an isomorphism from a ball of <span class="math inline">\(\gamma\)</span> to a ball of <span class="math inline">\(\gamma(1)\)</span> by using an induction method.</p><p><span class="math inline">\(X\)</span> is said to have neighborhoods of bipoint uniqueness if there are neighborhoods such that any two points of which is joined by a unique shortest geodesic varying continuously with its endpoints. If <span class="math inline">\(X\)</span> is locally convex, then <span class="math inline">\(X\)</span> has neighborhoods of bipoint uniqueness by the following argument.</p><figure><img src="/2020/01/26/Local-to-Global-Convexity/bp.jpg" alt="bipoint uniqueness"><figcaption aria-hidden="true">bipoint uniqueness</figcaption></figure><p>The following lemma says <span class="math inline">\(EP\)</span> is a covering map</p><blockquote><p>Let <span class="math inline">\(\phi:\overline{M}\to M\)</span> be a local isometry. <span class="math inline">\(\overline{M}\)</span> and <span class="math inline">\(M\)</span> are complete intrinsic metric spaces and <span class="math inline">\(M\)</span> has neighborhoods of bipoint uniqueness. Then <span class="math inline">\(\phi\)</span> is a covering map.</p></blockquote><p>Since <span class="math inline">\(\mathbb G_m\)</span> is contractible thus connected and simply connected, it suffices to define a complete intrinsic metric <span class="math inline">\(\bar{\mathbf d}\)</span> on <span class="math inline">\(\mathbb G_m\)</span> so that <span class="math inline">\((\mathbb G_m,\bar{\mathbf d})\)</span> is complete and <span class="math inline">\(EP\)</span> is local isometry. One verifies that the metric induced by <span class="math inline">\(\mathbf d\)</span> is exactly <span class="math inline">\(\bar{\mathbf d}\)</span>.</p><h2 id="catkappa-spaces">CAT(<span class="math inline">\(\kappa\)</span>) Spaces</h2><p><a href="http://cncc.bingj.com/cache.aspx?q=CAT+Spaces&amp;d=4932082848371635&amp;mkt=en-US&amp;setlang=en-US&amp;w=xMBtoXdf-irC2l_HmVovaAaEXyBHa_yw" target="_blank" rel="noopener">CAT<span class="math inline">\((\kappa)\)</span> spaces</a> are spaces with curvature bounded above in the Alexandrov sense. Let <span class="math inline">\(x,y,z\)</span> be distinct points in a length space <span class="math inline">\(X\)</span> and call <span class="math inline">\(\Delta xyz\)</span> a geodesic triangle if the segments <span class="math inline">\(xy,yz,zx\)</span> are geodesics. For any <span class="math inline">\(\kappa\in\mathbb R\)</span>, the comparison space of curvature <span class="math inline">\(\kappa\)</span> is the unique simply-connected Riemannian 2-manifold of constant sectional curvature <span class="math inline">\(\kappa\)</span>, denoted by <span class="math inline">\(M_\kappa\)</span>. Define the diameter of <span class="math inline">\(M_\kappa\)</span> as <span class="math display">\[D_\kappa=\left\{\begin{array}{l}\infty, \kappa\le 0\\ \pi/\sqrt{\kappa},\kappa&gt;0\end{array}\right.\]</span> For a <span class="math inline">\(D_\kappa\)</span>-geodesic space <span class="math inline">\(X\)</span> (i.e. points within distance <span class="math inline">\(D_\kappa\)</span> are joined by a unique geodesic), a geodesic triangle <span class="math inline">\(\Delta\)</span> is said to satisfy the CAT<span class="math inline">\((\kappa)\)</span> inequality if there exists an expanding map <span class="math inline">\(f:\Delta\to M_\kappa\)</span> such that <span class="math inline">\(f|_{\partial\Delta}\)</span> is an isometry. Then <span class="math inline">\(X\)</span> is called a CAT<span class="math inline">\((\kappa)\)</span> space if every geodesic triangle with perimeter less than <span class="math inline">\(2D_\kappa\)</span> satisfies the CAT<span class="math inline">\((\kappa)\)</span> inequality.</p><p>A subset <span class="math inline">\(A\)</span> in the CAT<span class="math inline">\((\kappa)\)</span> space <span class="math inline">\(X\)</span> is called convex if for any two points <span class="math inline">\(x,y\in A\)</span> within distance <span class="math inline">\(D_\kappa\)</span>, the unique geodesic joining <span class="math inline">\(x,y\)</span> is contained in <span class="math inline">\(A\)</span>. One easily verifies that these generalize the convexity in Euclidean spaces.</p><p><a href="https://arxiv.org/pdf/1304.4147.pdf" target="_blank" rel="noopener">This paper</a> shows that local-to-global convexity holds for all CAT<span class="math inline">\((\kappa)\)</span> spaces. Specifically, the following theorem holds</p><blockquote><p>Let <span class="math inline">\(A\subset X\)</span> be a closed, connected, locally convex subset in CAT<span class="math inline">\((\kappa)\)</span> space. Denote with <span class="math inline">\(l\)</span> the induced length metric in <span class="math inline">\(A\)</span>. Then if <span class="math inline">\(diam_l(A)\le D_\kappa\)</span>, <span class="math inline">\(l\)</span> coincides with the original length distance and <span class="math inline">\(A\)</span> is convex.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Metric Convexity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gromov</title>
      <link href="/2020/01/16/Gromov%E8%AE%BF%E8%B0%88/"/>
      <url>/2020/01/16/Gromov%E8%AE%BF%E8%B0%88/</url>
      
        <content type="html"><![CDATA[<p>20141222Gromov<a href="https://www.simonsfoundation.org/2014/12/22/mikhail-gromov/" target="_blank" rel="noopener"></a>.</p><hr><p>18</p><p></p><p></p><p></p><p>ERGO</p><p>2009</p><p></p><p>MacTutor19651970</p><p></p><p>hh</p><p></p><hr><p></p><p></p><p></p><p></p><p>19</p><p></p><p>--</p><p>1986</p><p>Filling Riemannian Manifolds</p><p></p><hr><p>(ERGO)</p><p></p><p></p><p></p><p></p><p>ERGOERGOERGOERGO</p><p></p><p></p>]]></content>
      
      
      <categories>
          
          <category>  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Stability Theorems in Topological Data Analysis (4)</title>
      <link href="/2019/12/30/Stability-Theorems-in-Topological-Data-Analysis-4/"/>
      <url>/2019/12/30/Stability-Theorems-in-Topological-Data-Analysis-4/</url>
      
        <content type="html"><![CDATA[<p>Let <span class="math inline">\(\mathbb X\)</span> be a triangulable, compact metric space, <span class="math inline">\(f,g:\mathbb X\to\mathbb R\)</span> be tame functions (that is, <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> have finitely many homological critical values.) The sublevel sets of <span class="math inline">\(f\)</span> (resp. <span class="math inline">\(g\)</span>) form a filtration of spaces. Using fields as coefficient groups, It gives a finite sequence of homology groups (vector spaces) in each dimension. A homology class is born if it is not an image from preceding homology groups, and is said to be died if it merges with other classes when entering succeeding homology groups. With the notion of birth and death, in each dimension one can associate <span class="math inline">\(f\)</span> (resp. <span class="math inline">\(g\)</span>) a multiset in the two-plane called the persistence diagram, denoted by <span class="math inline">\(D_k(f)\)</span> (resp. <span class="math inline">\(D_k(g)\)</span>) for <span class="math inline">\(k=0,1,\cdots\)</span>. The bottleneck distance of <span class="math inline">\(D_k(f)\)</span> and <span class="math inline">\(D_k(g)\)</span> is defined as <span class="math display">\[d_B(D_k(f),D_k(g)):=\inf_{\gamma \text{ bijection}}\sup_{x\in D_k(f)}||x-\gamma(x)||_{\infty}\]</span> It was proved that persistence diagrams are robust to small perturbations of functions. More specifically, <a href="https://link.springer.com/article/10.1007%2Fs00454-006-1276-5" target="_blank" rel="noopener">David Cohen-Steiner, Herbert Edelsbrunner, John Harer ('2007')</a> proved that, for each <span class="math inline">\(k\)</span> one has <span class="math display">\[d_B(D_k(f),D_k(g))\le||f-g||_\infty\]</span> In analogy to Wasserstein distance, they note that bottleneck distance is a special case of a more general form. Let <span class="math inline">\(D(f)\)</span> be the union of all <span class="math inline">\(D_k(f)\)</span>. Define the degree-<span class="math inline">\(p\)</span> Wasserstein distance between <span class="math inline">\(D(f)\)</span> and <span class="math inline">\(D(g)\)</span> by <span class="math display">\[W_p(D(f),D(g))=\left[\inf_{\gamma \text{ bijections}}\sum_{x\in D(f)}||x-\gamma(x)||_\infty^p\right]^{\frac{1}{p}}\]</span> To prove a stability result for Wasserstein distance, additional conditions are required for the space <span class="math inline">\(\mathbb X\)</span> and functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>. We shall explain the details about the work of <a href="https://link.springer.com/article/10.1007%2Fs10208-010-9060-6" target="_blank" rel="noopener">David Cohen-Steiner, Herbert Edelsbrunner, John Harer, Yuriy Mileyko ('2010')</a>.</p><a id="more"></a><hr><p><strong>Size of triangulation.</strong></p><p>A crucial step in proving the main theorem is to bound the number of points in the diagram whose persistence exceeds some threshold. In fact, they observed that this quantity can be controlled by the size of triangulation. That is why we need to assume the space <span class="math inline">\(\mathbb X\)</span> is <strong>triangulable</strong>. Precisely, a triangulation of <span class="math inline">\(\mathbb X\)</span> is a finite simplicial complexes <span class="math inline">\(K\)</span> together with a homeomorphism <span class="math inline">\(\nu:|K|\to \mathbb X\)</span>. For a simplex $$ in <span class="math inline">\(K\)</span> we can define its diameter <span class="math inline">\(\text{diam}(\sigma)=\max_{x,y\in\sigma}d(\nu(x),\nu(y))\)</span>. The mesh of <span class="math inline">\(K\)</span> is defined as <span class="math inline">\(\text{mesh}(K)=\max_{\sigma\in K}\text{diam}(\sigma)\)</span>. The size of <span class="math inline">\(K\)</span> is the number of simplices in <span class="math inline">\(K\)</span>, denoted by <span class="math inline">\(\text{card}(K)\)</span>. We are interested in the following size function <span class="math display">\[N(r)=\min_{\text{mesh}(K)\le r}\text{card}(K)\]</span> A rough observation is that, <span class="math inline">\(N(r)\)</span> is a non-increasing function, and when <span class="math inline">\(r\)</span> tends to 0, <span class="math inline">\(N(r)\)</span> will tend to infinity. For 'good' spaces we can estimate that the order of divergence. For example, if <span class="math inline">\(\mathbb X\)</span> is an <span class="math inline">\(n\)</span>-dimensional ball (or cube), then the cardinality of simplices is proportional to <span class="math inline">\(\text{vol}(\mathbb X)/r^n\)</span>. For compact Riemannian manifold of dimension <span class="math inline">\(n\)</span> the order also holds true. This inspires to propose the '<strong>polynomial growth</strong>' assumption: there are constants <span class="math inline">\(C_0\)</span>, <span class="math inline">\(R_0\)</span> and <span class="math inline">\(M\)</span> such that for <span class="math inline">\(r&lt;R_{0}\)</span>, <span class="math inline">\(N(r)\le \frac{C_0}{r^M}\)</span>.</p><p>One clearly sees that the inequality cannot hold for all <span class="math inline">\(r&gt;0\)</span> since it leads to nonsense <span class="math inline">\(N(r)\to 0\)</span> as <span class="math inline">\(r\to \infty\)</span>. In fact, the value of <span class="math inline">\(N(r)\)</span> for large <span class="math inline">\(r\)</span> is the minimal triangulation of the space <span class="math inline">\(\mathbb X\)</span>. Even for the simplest case, say, an <span class="math inline">\(n\)</span>-dimensional cube, the exact value of the minimal triangulation is not completely known. (See <a href="https://math.stackexchange.com/questions/474857/find-the-smallest-triangulation-of-the-n-dimensional" target="_blank" rel="noopener">this post</a>) .</p><p><strong>Lipschitz Functions</strong></p><p>Recall that <span class="math inline">\(f:\mathbb X\to \mathbb R\)</span> is a Lipschitz function if there is a constant <span class="math inline">\(c\)</span> such that <span class="math inline">\(d(f(x),f(y))\le cd(x,y)\)</span> for all <span class="math inline">\(x,y\in \mathbb X\)</span>. The infimum over all such <span class="math inline">\(c\)</span> is called the Lipschitz norm of <span class="math inline">\(f\)</span>, denoted by <span class="math inline">\(\text{Lip}(f)\)</span>.</p><p>The reason to consider Lipschitz functions is that the sublevel sets of Lipschitz functions are well separated. For simplicity assume <span class="math inline">\(f\)</span> is non-expanding, i.e. <span class="math inline">\(\text{Lip}(f)=1\)</span>. Then for <span class="math inline">\(x\in f^{-1}(a)\)</span> and <span class="math inline">\(y\in f^{-1}(b)\)</span> with <span class="math inline">\(a&lt;b\)</span> we have <span class="math inline">\(d(x,y)\ge b-a\)</span>. Furthermore, denote <span class="math inline">\(\mathbb X_a=f^{-1}((-\infty,a])\)</span> and <span class="math inline">\(\mathbb X_b=f^{-1}((-\infty,b])\)</span>. Let <span class="math inline">\(\mathbb X_a^r=\{z\in\mathbb X|d(z,\mathbb X_a)\le r\}\)</span> be the set of points within distance <span class="math inline">\(r\)</span> to <span class="math inline">\(\mathbb X_a\)</span>. Then <span class="math inline">\(\mathbb X_a^{b-a}\subseteq \mathbb X_b\)</span>. This allows us to thicken the sublevel sets without breaking the order of inclusion.</p><p>Thickening a subset is helpful for us to find simplicial cycles in <span class="math inline">\(K\)</span> that approximate singular cycles in <span class="math inline">\(\mathbb X\)</span>. This is best illustrated in the figure (copied from paper <a href="https://link.springer.com/article/10.1007%2Fs10208-010-9060-6" target="_blank" rel="noopener">'2010'</a>)</p><figure><img src="/2019/12/30/Stability-Theorems-in-Topological-Data-Analysis-4/thick.png" alt="cycle"><figcaption aria-hidden="true">cycle</figcaption></figure><p><strong>Bounding Persistent Cycles</strong></p><p>Fix a tame non-expanding function <span class="math inline">\(f\)</span> on the triangulable, compact metric space <span class="math inline">\(\mathbb X\)</span>. Let <span class="math inline">\(P(f,\epsilon)\)</span> be the number of points in the diagrams of <span class="math inline">\(f\)</span> whose persistence exceeds <span class="math inline">\(\epsilon\)</span>. Then the <em>Persistent Cycle Lemma</em> claims that <span class="math inline">\(P(f,\epsilon)\le N(\epsilon)\)</span> for <strong>any</strong> <span class="math inline">\(\epsilon\)</span>.</p><p><strong>Proof.</strong> Let <span class="math inline">\(K\)</span> be a triangulation such that <span class="math inline">\(\text{mesh}(K)\le \epsilon\)</span>. Let <span class="math inline">\(\alpha\)</span> be a homological class in any dimension whose persistence exceeds <span class="math inline">\(\epsilon\)</span>. i.e. <span class="math inline">\(\text{per}(\alpha)=\text{d}(\alpha)-\text{b}(\alpha)\ge\epsilon\)</span>. Let <span class="math inline">\(z(\alpha)\)</span> be a closed cycle in <span class="math inline">\(\mathbb X_{\text{b}(\alpha)}\)</span>. By separation of Lipschitz function, <span class="math inline">\(\mathbb X_{\text{b}(\alpha)}^{\text{b}(\alpha)+\epsilon}\subset\mathbb X_{\text{b}(\alpha)+\epsilon}\)</span>. Thus, there is a closed cycle <span class="math inline">\(\bar{z}(\alpha)\)</span> in <span class="math inline">\(K\)</span> which is homologous to <span class="math inline">\(z(\alpha)\)</span> in <span class="math inline">\(\mathbb X_{\text{b}(\alpha)+\epsilon}\)</span>.</p><p>Suppose there are <span class="math inline">\(m\)</span> points in the diagram of dimension <span class="math inline">\(l\)</span> whose persistence exceeds <span class="math inline">\(\epsilon\)</span>. By construction, we can find <span class="math inline">\(l\)</span> dimensional cycles <span class="math inline">\(\bar{z}_1,\bar{z}_2,\cdots,\bar{z}_m\)</span>. Assume the birth time are ordered <span class="math inline">\(\text{b}_1\le\text{b}_2\le\cdots\le \text{b}_m\)</span>. We claim that these cycles are <strong>independent</strong>. Suppose <span class="math inline">\(\bar{z}_i\)</span> is not independent of its predecessors. i.e. <span class="math inline">\(\bar{z}_i\sim\sum_{j=1}^{i-1}\lambda_j\bar{z}_j\)</span>. In <span class="math inline">\(\mathbb X_{\text{b}_i+\epsilon}\)</span> we have <span class="math inline">\(z_i\sim\sum_{j=1}^{i-1}\lambda_jz_j\)</span>. Without loss of generality we assume <span class="math inline">\(\text{b}_i&gt;\text{b}_j\)</span> for <span class="math inline">\(i&gt;j\)</span> (or we can put the same <span class="math inline">\(\text{b}_i\)</span> together). By definition, this means that <span class="math inline">\(z_i\)</span> is dead before <span class="math inline">\(\text{b}_i+\epsilon\)</span>, contradiction. By induction, these cycles are independent. Note that the number of independent cycles is bounded by the number of <span class="math inline">\(l\)</span> dimensional simplices. Summing over we see that <span class="math inline">\(P(\epsilon,f)\)</span> is bounded by <span class="math inline">\(N(\epsilon)\)</span>.</p><p><strong>Estimating Persistence Moments</strong></p><p>View a persistence diagram as a sum of Dirac measures. We want to estimate the moments <span class="math display">\[\text{Per}_k(f)=\sum_{x\in D(f)}\text{per}(x)^k\]</span> Call it <em>degree-k total persistence</em> of <span class="math inline">\(f\)</span>. More generally, we consider the following function <span class="math display">\[\text{Per}_k(f,t)=\sum_{\text{per}(x)&gt;t}\text{per}(x)^k\]</span> From this point of view, <span class="math inline">\(P(t,f)\)</span> is the measure of <span class="math inline">\(\{x|\text{per}(x)&gt;t\}\)</span>. For simplicity, Let us project the diagram to the <span class="math inline">\(t\)</span>-axis so that all functions are treated as functions on <span class="math inline">\(\mathbb R\)</span>. Then <span class="math inline">\(P(t,f)\)</span> is a <strong>decreasing stair function</strong>. The derivative of <span class="math inline">\(P(t,f)\)</span> with respect to <span class="math inline">\(t\)</span> is the negative of Dirac function. Hence <span class="math display">\[\text{Per}_k(f,t)=\int_{\epsilon&gt;t}-P(\epsilon,f)&#39;\epsilon^kd\epsilon\]</span> Integrating by parts we obtain <span class="math display">\[\text{Per}_k(f,t)=t^kP(t,f)+k\int_t^{\text{Amp}(f)}P(\epsilon,f)\epsilon^{k-1}d\epsilon\]</span> where <span class="math inline">\(\text{Amp}(f)=\max f-\min f\le \text{diam}(\mathbb X)\)</span>.</p><p>But <span class="math inline">\(P(t,f)\)</span> is bounded by <span class="math inline">\(N(t)\)</span> and <span class="math inline">\(N(t)\)</span> is assume to has a polynomial growth when <span class="math inline">\(t\)</span> is small. Suppose there are constants <span class="math inline">\(C_{\mathbb X}\)</span> and <span class="math inline">\(M\)</span> such that when <span class="math inline">\(t\le \text{Amp}(f)\)</span>, <span class="math inline">\(N(t)\le C_{\mathbb X}/t^M\)</span>. We see that <span class="math display">\[\text{Per}_k(f,t)\le t^kC_{\mathbb X}/t^M+k\int_t^{\text{Amp}(f)}C_{\mathbb X}\epsilon^{k-1-M}d\epsilon\]</span> when <span class="math inline">\(k=M+\delta\)</span> with <span class="math inline">\(\delta&gt;0\)</span>, <span class="math display">\[\text{Per}_k(f,t)\le C_{\mathbb X}\text{diam}(\mathbb X)^\delta+\frac{M+\delta}{\delta}C_{\mathbb X}\text{diam}(\mathbb X)^\delta=\frac{M+2\delta}{\delta}C_{\mathbb X}\text{diam}(\mathbb X)^\delta\]</span> Thus we see that there is a uniform bound on <span class="math inline">\(\text{Per}_k(f,t)\)</span> which only depends on the space <span class="math inline">\(\mathbb X\)</span>. Therefore, we abstract the following definition</p><p><strong>Definition.</strong> A metric space <span class="math inline">\(\mathbb X\)</span> implies bounded degree-k total persistence if <span class="math inline">\(\text{Per}_k(f)\)</span> is uniformly bounded for all non-expanding functions.</p><p>From the discussion above, a triangulable, compact metric space with polynomial growth size of order <span class="math inline">\(M\)</span> implies bounded degree-<span class="math inline">\((M+\delta)\)</span> total persistence.</p><p><strong>Wasserstein Stability</strong></p><p>Let <span class="math inline">\(\mathbb X\)</span> be a triangulable, compact metric space which implies bounded degree-k total persistence. <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are two tame non-expanding functions on <span class="math inline">\(\mathbb X\)</span>. Then <span class="math display">\[W_p(D(f),D(g))\le C_{\mathbb X}||f-g||_\infty^{1-k/p}\]</span> where <span class="math inline">\(p\ge k\)</span> and <span class="math inline">\(C_{\mathbb X}\)</span> is a constant depending only on <span class="math inline">\(\mathbb X\)</span>.</p><p><strong>Proof.</strong> Let <span class="math inline">\(||f-g||_\infty=\epsilon\)</span>. Let <span class="math inline">\(\gamma:D(f)\to D(g)\)</span> be a bijection such that <span class="math inline">\(d_B(D(f),D(g))=||f-g||_\infty\)</span>. Note that <span class="math inline">\(||x-\gamma(x)||\le 1/2(\text{per}(x)+\text{per}(\gamma(x)))\)</span>. Thus, <span class="math display">\[W_p(D(f),D(g))^p\le \epsilon^{p-k}\sum ||x-\gamma(x)||_\infty^k\le\epsilon^{p-k}/2\sum (\text{per}(x)^k+\text{per}(\gamma(x))^k)\]</span> by the convexity of <span class="math inline">\(x^k\)</span>. The stability inequality follows.</p><hr><p><strong>Wasserstein Stability for Point Clouds</strong></p><p>Recall that for a finite metric space <span class="math inline">\(X=\{x_1,x_2,\cdots,x_n\}\)</span> with <span class="math inline">\(n\)</span> points. We have an isometric embedding <span class="math display">\[\rho:X\to\mathbb (R^n,||\cdot||_\infty)\]</span> by <span class="math inline">\(\rho(x_i)=(d_X(x_1,x_i),\cdots,d_X(x_n,x_i))\)</span>. If <span class="math inline">\(X,Y\)</span> are finite metric spaces with Gromov-Hausdorff distance <span class="math inline">\(\epsilon\)</span>, we can find a compact metric space <span class="math inline">\(Z\)</span> such that <span class="math inline">\(X,Y\)</span> isometrically embed into <span class="math inline">\(Z\)</span> with Hausdorff distance <span class="math inline">\(\epsilon\)</span>. Then we can embed <span class="math inline">\(X\cup Y\)</span> into <span class="math inline">\(\mathbb R^{n_X+n_Y}\)</span>. The Cech filtration of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are the sublevel set filtration of <span class="math inline">\(\delta_X\)</span> and <span class="math inline">\(\delta_Y\)</span>. Note that <span class="math inline">\(||\delta_X-\delta_Y||_\infty\le\epsilon\)</span>. However, the space <span class="math inline">\(\mathbb R^{n_X+n_Y}\)</span> is not compact. The Wasserstein stability theorem cannot apply directly.</p><p>One way to overcome this difficulty is to restrict <span class="math inline">\(\delta_X\)</span> and <span class="math inline">\(\delta_Y\)</span> to a subset of the total space. It is easy to check that <span class="math inline">\(X\cup Y\)</span> is bounded by a cube of radius <span class="math inline">\(a=\min\{\text{diam}(X),\text{diam}(Y)\}+2\epsilon\)</span>. We see that this cube satisfies the polynomial growth condition <span class="math inline">\(N(r)\le C_n\frac{a^n}{r^n}\)</span> where <span class="math inline">\(n=n_X+n_Y\)</span> (just divide the cube into <span class="math inline">\(a^n/r^n\)</span> small cubes where each cube can be triangulated by <span class="math inline">\(C_n\)</span> simplices). Hence the cube implies <span class="math inline">\(k\)</span>-total persistence with constant <span class="math inline">\(C_n a^{k}\)</span> where <span class="math inline">\(k&gt;n\)</span>. At this moment we can use Wasserstein stability to obtain a bound by the Gromov-Hausdorff distance of point clouds.</p><p>However, this result is not of much importance since the stability depends on the number of point clouds. The problem is that the point clouds we are considering are too general while the Wasserstein stability for spaces are restrictive. A reasonable assumption is to consider finite samples from a <span class="math inline">\(k\)</span>-dimensional Riemannian manifold (or Finsler manifold). But one should be careful with Nerve theorem since it will not be true for general open covers.</p><p>To start up, let us consider the simplest case where point clouds are <em>Euclidean</em>, i.e. finite samples from Euclidean spaces. Now the embedding is fixed and Nerve theorem is true for any convex cover. The only question is to find a suitable metric for point clouds.</p><p>Let <span class="math inline">\(X,Y\)</span> be compact subsets of <span class="math inline">\(\mathbb R^d\)</span> where <span class="math inline">\(d\)</span> is fixed. Let <span class="math inline">\(\mathcal E(d)\)</span> be the Euclidean group acting on <span class="math inline">\(\mathbb R^d\)</span>. Consider the distance given by <span class="math display">\[d^{\mathbb R^n}_\mathcal{EH}(X,Y)=\inf_{T\in \mathcal E(d)}d^{\mathbb R^n}_{\mathcal H}(X,T(Y))\]</span> This distance defines a metric on the set of isometric classes of compact subsets in <span class="math inline">\(\mathbb R^n\)</span> (see <a href="https://people.math.osu.edu/memolitechera.1/papers/dgh-euclidean.pdf" target="_blank" rel="noopener">Memoli</a>). This distance is also commonly used in image/point cloud registration problem. It is often be approximated by Iterative Closet Poing (ICP) algorithm.</p><p>Obviously <span class="math inline">\(d_\mathcal{GH}\le d_\mathcal{EH}\)</span> and it is often the case where the inequality is strict. However, it is proved that <span class="math inline">\(d_\mathcal{EH}\le cd_\mathcal{GH}^{1/2}\)</span> thus two distances are equivalent. For our case <span class="math inline">\(d_\mathcal{EH}\)</span> is more convenient. Let <span class="math inline">\(a=d_\mathcal{EH}\)</span>. For any <span class="math inline">\(\epsilon&gt;0\)</span> we can find an isometry <span class="math inline">\(T\in \mathcal E(d)\)</span> such that the Hausdorff distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(T(Y)\)</span> is less than <span class="math inline">\(a+\epsilon\)</span>. Therefore, <span class="math inline">\(X\cup T(Y)\)</span> lies in a ball of radius <span class="math inline">\(2(\max\{\text{diam}(X),\text{diam}(Y)\}+a+\epsilon)\)</span>. This ball clearly satisfies the polynomial growth condition and the distance function <span class="math inline">\(\delta_X\)</span> restricted on the ball has the same filtration homotopy of <span class="math inline">\(\delta_X\)</span> on the whole space. According to the Nerve theorem, the sublevel filtration of <span class="math inline">\(\delta_X\)</span> is the Cech filtration of point clouds. Now apply Wasserstein Stability theorem and we obtain an inequality between the Wasserstein distance of Cech persistence diagrams and Euclidean-Hausdorff distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/11/28/%E6%88%91%E6%98%AF%E5%A6%82%E4%BD%95%E5%8F%91%E7%8E%B0%E6%80%AA%E7%90%83%E7%9A%84/"/>
      <url>/2019/11/28/%E6%88%91%E6%98%AF%E5%A6%82%E4%BD%95%E5%8F%91%E7%8E%B0%E6%80%AA%E7%90%83%E7%9A%84/</url>
      
        <content type="html"><![CDATA[<p>John Milnor<a href="https://www.maths.ed.ac.uk/~v1ranick/papers/milnor.pdf" target="_blank" rel="noopener">Classification of <span class="math inline">\((n-1)\)</span>-connected <span class="math inline">\(2n\)</span>-dimensional manifolds and the discovery of exotic spheres</a></p><hr><p>50<span class="math inline">\(n-1\)</span><span class="math inline">\(2n\)</span><span class="math inline">\(2n\)</span><span class="math inline">\(M^{2n}\)</span><span class="math inline">\(n\)</span><span class="math inline">\(2n\)</span><span class="math inline">\(e^{2n}\)</span><span class="math inline">\(\partial e^{2n}\)</span><span class="math inline">\(n\)</span> <span class="math display">\[M^{2n}\simeq (S^n\vee \cdots\vee S^n)\cup_{f}e^{2n}\]</span> <span class="math inline">\(f\)</span><span class="math inline">\(\pi_{2n-1}(S^n\vee \cdots\vee S^n)\)</span><span class="math inline">\(M^{2n}\)</span> <span class="math display">\[H^0(M^{2n})\cong\mathbb{Z},\, H^n(M^{2n})\cong\mathbb{Z}\oplus\cdots\oplus\mathbb{Z},\, H^{2n}(M^{2n})\cong\mathbb{Z},\]</span> <span class="math inline">\(f\)</span><span class="math inline">\(H^n\otimes H^n\)</span><span class="math inline">\(n\)</span><span class="math inline">\(n\)</span><span class="math inline">\(\pm 1\)</span><span class="math inline">\(n\)</span><span class="math inline">\(n\)</span>[7][5]<em></em></p><p> <span class="math display">\[p_i\in H^{4i}(M).\]</span> 50.50 <span class="math display">\[\Pi_n=\pi_{n+k}(S^k)\quad(k&gt;n+1)\]</span> 50<span class="math inline">\(4m\)</span> <span class="math display">\[H^{2m}(M^{4m};\mathbb{R})\otimes H^{2m}(M^{4m};\mathbb{R})\to H^{4m}(M^{4m};\mathbb{R})\cong\mathbb{R}\]</span>  <span class="math display">\[\text{signature}(M^4)=\frac{1}{3}p_1[M^4]\]</span>  <span class="math display">\[\text{signature}(M^8)=\frac{1}{45}(7p_2-(p_1)^2)[M^8].\]</span> </p><p><span class="math inline">\(n-1\)</span><span class="math inline">\(2n\)</span><span class="math inline">\(n=2m\)</span><span class="math inline">\(2m\)</span><span class="math inline">\(4m\)</span><span class="math inline">\(4m\)</span> <span class="math display">\[S^{2m}\cup e^{4m}\simeq M^{4m}\]</span> --</p><p><span class="math inline">\(M^{4m}\)</span><span class="math inline">\(2m\)</span><span class="math inline">\(m&gt;1\)</span><span class="math inline">\(S^{2m}\subset M^{4m}\)</span><span class="math inline">\(2m\)</span><span class="math inline">\(E^{4m}\)</span><span class="math inline">\(2m\)</span><span class="math inline">\(D^{2m}_+\)</span><span class="math inline">\(D^{2m}_-\)</span><span class="math inline">\(S^{2m-1}\)</span> <span class="math display">\[E^{4m}=(D^{2m}_+\times D^{2m})\cup_{F}(D^{2m}_-\times D^{2m})\]</span> <span class="math inline">\(F(x,y)=(x,f(x)y)\)</span><span class="math inline">\(D^{2m}_+\cap D^{2m}_-\)</span><span class="math inline">\(D^{2m}\)</span><span class="math inline">\(f:S^{2m-1}\to SO(2m)\)</span><span class="math inline">\(2m\)</span><span class="math inline">\(\pi_{2m-1}(SO(2m))\)</span></p><p><span class="math inline">\(4m=4\)</span><span class="math inline">\(S^2\)</span><span class="math inline">\(D^2\)</span><span class="math inline">\(\pi_1(SO(2))\cong\mathbb{Z}\)</span><span class="math inline">\(S^4\)</span><span class="math inline">\(D^4\)</span><span class="math inline">\(\pi_3(SO(4))\)</span><span class="math inline">\(SO(4)\)</span><span class="math inline">\(\pi_3(SO(4))\cong\mathbb{Z}\oplus\mathbb{Z}\)</span><span class="math inline">\(S^3\)</span><span class="math inline">\(S^3\)</span><span class="math inline">\((f)\in\pi_3(SO(4))\)</span><span class="math inline">\(f(x)y=x^iyx^j\)</span><span class="math inline">\(x\)</span><span class="math inline">\(y\)</span><span class="math inline">\((i,j)\in\mathbb{Z}\oplus\mathbb{Z}\)</span></p><p><span class="math inline">\((i,j)\)</span><span class="math inline">\(M^7=\partial E^8\)</span><span class="math inline">\(S^7\)</span><span class="math inline">\(i\)</span><span class="math inline">\(j\)</span><span class="math inline">\(S^7\)</span><span class="math inline">\(M^7\)</span><span class="math inline">\(S^7\)</span><span class="math inline">\(i+j\)</span><span class="math inline">\(\pm 1\)</span><span class="math inline">\(i+j=+1\)</span><span class="math inline">\(i\)</span><span class="math inline">\(i\)</span><span class="math inline">\(j=1-i\)</span><span class="math inline">\(M^7=\partial E^8\)</span><span class="math inline">\(S^4\)</span><span class="math inline">\(S^3\)</span><span class="math inline">\(S^7\)</span><span class="math inline">\(S^7\)</span></p><p>-<span class="math inline">\((p_1)^2\)</span><span class="math inline">\(p_2\)</span><span class="math inline">\(\pm 1\)</span><span class="math inline">\(\pm 1\)</span><span class="math inline">\(+1\)</span><span class="math inline">\(H^4(M^8)\)</span><span class="math inline">\(H^4(S^4)\)</span><span class="math inline">\(p_1\)</span><span class="math inline">\(i\)</span><span class="math inline">\(j\)</span><span class="math inline">\(p_1\)</span><span class="math inline">\(2(i-j)=2(2i-1)\)</span><span class="math inline">\(H^4(M^8)\)</span><span class="math inline">\(p_1^2[M^8]=4(2i-1)^2\)</span><span class="math inline">\(p_2\)</span><span class="math inline">\(p_2[M^8]\)</span> <span class="math display">\[p_2[M^8]=\frac{p_1^2[M^8]+45}{7}=\frac{4(2i-1)^2+45}{7}\]</span> <span class="math inline">\(i=1\)</span><span class="math inline">\(p_2[M^8]=7\)</span><span class="math inline">\(i=2\)</span><span class="math inline">\(p_2[M^8]=\frac{81}{7}\)</span><span class="math inline">\(p_2\)</span><span class="math inline">\(p_2[M^8]\)</span></p><p><span class="math inline">\(p_1\)</span><span class="math inline">\(p_2[M]\)</span><span class="math inline">\(M^7=\partial E^8\)</span><span class="math inline">\(E^8\)</span><span class="math inline">\(M^7\)</span><span class="math inline">\(M^7\)</span></p><p><span class="math inline">\(M^7\)</span><span class="math inline">\(S^4\)</span><span class="math inline">\(S^3\)</span><span class="math inline">\(M^7\)</span><span class="math inline">\(S^7\)</span><span class="math inline">\(k\)</span><span class="math inline">\(M^7\)</span></p><p> <span class="math display">\[M^7=D^7_+\cup_fD^7_-\]</span> <span class="math inline">\(D_{\pm}^7\)</span><span class="math inline">\(f:S^6\to S^6\)</span><em></em><span class="math inline">\(S^6\)</span><span class="math inline">\(M^7\)</span></p>]]></content>
      
      
      <categories>
          
          <category>  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>The Space of Persistence Diagrams</title>
      <link href="/2019/11/24/The-Space-of-Persistence-Diagrams/"/>
      <url>/2019/11/24/The-Space-of-Persistence-Diagrams/</url>
      
        <content type="html"><![CDATA[<p><strong>Notation.</strong> In the following <span class="math inline">\(\mathbb R^2\)</span> is equipped with <span class="math inline">\(\infty\)</span>-norm. Let <span class="math inline">\(\Delta=\{(x,x)|x\in\mathbb R\}\)</span> denote the diagonal, <span class="math inline">\(\Delta_+=\{(x,y)|y&gt;x\}\)</span> denote the upper off-diagonal points, and <span class="math inline">\(\overline{\Delta}_+=\Delta\cup\Delta_+\)</span> is the closure of <span class="math inline">\(\Delta_+\)</span>. Set <span class="math inline">\(\overline{\mathbb N}=\{1,2,\cdots\}\cup\{\infty\}\)</span>.</p><hr><p><strong>Definition.</strong> A persistence diagram <span class="math inline">\(\mathcal T\)</span> is a pair <span class="math inline">\((S_{\mathcal T},i_{\mathcal T})\)</span> where <span class="math inline">\(S_{\mathcal T}\)</span> is a subset of <span class="math inline">\(\mathbb R^2\)</span> and <span class="math inline">\(i_{\mathcal T}:S_{\mathcal T}\to \overline{\mathbb N}\)</span> is a function such that</p><ol type="1"><li><span class="math inline">\(\Delta\subset S_{\mathcal T}\subset \overline{\Delta}_+\)</span>;</li><li><span class="math inline">\(i_{\mathcal T}(\Delta)=\{\infty\}\)</span>.</li></ol><p><span class="math inline">\(S_{\mathcal T}\)</span> is called the support of <span class="math inline">\(\mathcal T\)</span> while <span class="math inline">\(i_{\mathcal T}\)</span> is called the multiplicity function of <span class="math inline">\(\mathcal T\)</span>.</p><p>Two persistence diagrams are said to be equal if they have the same support and multiplicity function. The graph of <span class="math inline">\(i_{\mathcal T}\)</span>, defined by <span class="math inline">\(G(i_{\mathcal T})=\{^k{\bf x}:=({\bf x},k)\in S_{\mathcal T}\times \overline{\mathbb N}|k\le i_{\mathcal T}({\bf x})\}\)</span>, is called the bundle space over <span class="math inline">\(\mathcal T\)</span>. The projection <span class="math inline">\(\pi_{\mathcal T}:G(i_{\mathcal T})\to S_{\mathcal T}\)</span> to the first factor defines a map from the bundle space to the support with fiber <span class="math inline">\(\pi_{\mathcal T}^{-1}({\bf x})=\{1,2,\cdots,i_{\mathcal T}({\bf x})\}\)</span> for each point <span class="math inline">\({\bf x}\in S_{\mathcal T}\)</span>. It is an easy consequence that two persistence diagrams are equal if and only if they have the same bundle spaces.</p><a id="more"></a><p><strong>Definition.</strong> Let <span class="math inline">\(\mathcal T_1\)</span> and <span class="math inline">\(\mathcal T_2\)</span> be two persistence diagrams. Their sum <span class="math inline">\(\mathcal T_1+\mathcal T_2\)</span> is a persistence diagram where <span class="math inline">\(S_{\mathcal T_1+\mathcal T_2}=S_{\mathcal T_1}\cup S_{\mathcal T_2}\)</span> is the union of two supports and <span class="math inline">\(i_{\mathcal T_1+\mathcal T_2}=i_{\mathcal T_1}+i_{\mathcal T_2}\)</span> is the natural extension on <span class="math inline">\(S_{\mathcal T_1+\mathcal T_2}\)</span>.</p><p>Let <span class="math inline">\(0_{\Delta}\)</span> be the trivial persistence diagram whose support is <span class="math inline">\(\Delta\)</span> and multiplicity function is the constant function <span class="math inline">\(\infty\)</span>. It is clear that <span class="math inline">\(0_{\Delta}\)</span> is the neutral element for addition. Therefore, the set of persistence diagrams together with addition forms a monoid.</p><p><strong>Definition.</strong> Let <span class="math inline">\(\lambda\ge0\)</span> be a nonnegative number. Define the scalar product <span class="math inline">\(\lambda \mathcal T\)</span> to be the persistence diagram with <span class="math inline">\(S_{\lambda\mathcal T}=\lambda S_{\mathcal T}=\{\lambda{\bf x}|{\bf x}\in S_{\mathcal T}\}\)</span> and <span class="math inline">\(i_{\lambda\mathcal T}(\lambda{\bf x})=i_{\mathcal T}({\bf x})\)</span>.</p><p><strong>Caveat.</strong> It is easy to check that <span class="math inline">\(\lambda(\mathcal T_1 +\mathcal T_2)=\lambda\mathcal T_1+\lambda\mathcal T_2\)</span>. However, for <span class="math inline">\(\lambda_1,\lambda_2\neq 0\)</span>, <span class="math inline">\((\lambda_1+\lambda_2)\mathcal T\)</span> may not equal to <span class="math inline">\(\lambda_1\mathcal T+\lambda_2\mathcal T\)</span>.</p><p><strong>Definition.</strong> Let <span class="math inline">\(\mathcal T_1\)</span> and <span class="math inline">\(\mathcal T_2\)</span> be two persistence diagrams and <span class="math inline">\(p\ge 1\)</span>. The <span class="math inline">\(p\)</span>th Wasserstein distance between <span class="math inline">\(\mathcal T_1\)</span> and <span class="math inline">\(\mathcal T_2\)</span> is defined as <span class="math display">\[W_p(\mathcal T_1,\mathcal T_2)=(\inf_{\gamma}\{\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma(^k{\bf x}))||_{\infty}^p\})^{1/p}\]</span> where <span class="math inline">\(\gamma\)</span> ranges over all the bijections from <span class="math inline">\(G(i_{\mathcal T_1})\)</span> to <span class="math inline">\(G(i_{\mathcal T_2})\)</span>. The summation is interpreted as integration with respect to the counting measure on <span class="math inline">\(G(i_{\mathcal T_1})\)</span>. If such a bijection does not exist, the distance is defined to be <span class="math inline">\(\infty\)</span>.</p><p><span class="math inline">\(W_p\)</span> is not a metric on the space of persistence diagrams, since <span class="math inline">\(W_p(\mathcal T_1,\mathcal T_2)=0\)</span> does not imply <span class="math inline">\(\mathcal T_1=\mathcal T_2\)</span>. For example, consider <span class="math inline">\(\mathcal T_1\)</span> with support <span class="math inline">\(\{(0,y)|y\in(0,1]\}\cup\Delta\)</span> and for each <span class="math inline">\((0,y)\)</span> the multiplicity is <span class="math inline">\(1\)</span>. Excise the point <span class="math inline">\((0,1)\)</span> and we obtain another persistence diagram <span class="math inline">\(\mathcal T_2\)</span> such that <span class="math inline">\(W_p(\mathcal T_1,\mathcal T_2)=0\)</span>. However, <span class="math inline">\(\mathcal T_1\)</span> and <span class="math inline">\(\mathcal T_2\)</span> are not equal.</p><p><strong>Definition.</strong> Let <span class="math inline">\(\mathcal T\)</span> be a persistence diagram and <span class="math inline">\(p\ge 1\)</span>. The <span class="math inline">\(p\)</span>th Wasserstein norm of <span class="math inline">\(\mathcal T\)</span> is defined as <span class="math inline">\(||\mathcal T||_{W_p}:=W_p(\mathcal T, 0_{\Delta})\)</span>. The <span class="math inline">\(p\)</span>th finite persistence space is defined as the collection of persistence diagrams with finite <span class="math inline">\(p\)</span>th Wasserstein norm, i.e. <span class="math inline">\(\mathcal D_p=\{\mathcal T|||\mathcal T||_{W_p}&lt; \infty\}\)</span>.</p><p><strong>Example.</strong> Let <span class="math inline">\({\bf x}=(x,y)\in\mathbb R^2\)</span> with <span class="math inline">\(y&gt;x\)</span>. Define the indicator diagram at <span class="math inline">\({\bf x}\)</span> to be the diagram$\chi^{{\bf x}}_1$ with support $\Delta\cup\{{\bf x}\}$ and $i_{\chi^{\bf x}_1}({\bf x})=1$. Then $||\chi^{{\bf x}}_1||_{W_p}=(y-x)/2$. Similarly, we can define $\chi_k^{{\bf x}}=\sum^k_{r=1}\chi^{\bf x}_1$, called the indicator diagram at ${\bf x}$ in multiplicity $k$. Then $||\chi_k^{\bf x}||_{W_p}=k^{1/p}(y-x)/2$. More generally, we have the following lemma.</p><p><strong>Lemma.</strong> Let $\mathcal T=\sum_{r=1}^n\chi^{{\bf x}_r}_{1}$ be a finite sum of indicator diagrams. Then $||\mathcal T||_{W_p}=(\sum_{r=1}^n||\chi_1^{{\bf x}_r}||^p_{W_p})^{1/p}$. As a consequence, for any <span class="math inline">\(\mathcal T\in\mathcal D_p\)</span> and <span class="math inline">\({\bf x}\in S_{\mathcal T}\backslash\Delta\)</span>, the multiplicity of <span class="math inline">\({\bf x}\)</span> is finite. i.e. <span class="math inline">\(i_{\mathcal T}({\bf x})&lt;\infty\)</span>.</p><p><strong>Remark.</strong> In convention, the degree-<span class="math inline">\(p\)</span> total persistence of a persistence diagram <span class="math inline">\(\mathcal T\)</span> is defined to be <span class="math inline">\((2W_p(\mathcal T,0_{\Delta}))^p\)</span>. Thus a persistence diagram has finite <span class="math inline">\(p\)</span>th Wasserstein norm if and only if its degree-<span class="math inline">\(p\)</span> total persistence is finite. This is why we call <span class="math inline">\(\mathcal D_p\)</span> <span class="math inline">\(p\)</span>th finite persistence space.</p><p><strong>Theorem.</strong> <span class="math inline">\(W_p\)</span> is a metric on the space <span class="math inline">\(\mathcal D_p\)</span> for <span class="math inline">\(p\ge 1\)</span>.</p><p><strong>Proof.</strong> Symmetry is obvious from definition. Let <span class="math inline">\(\mathcal T_1,\mathcal T_2\in\mathcal D_p\)</span>. Assume <span class="math inline">\(W_p(\mathcal T_1,\mathcal T_2)=0\)</span>. Let <span class="math inline">\(^k{\bf z}\in G(i_{\mathcal T_1})\)</span> and <span class="math inline">\({\bf z}\)</span> be the corresponding point in <span class="math inline">\(S_{\mathcal T_1}\)</span>. If <span class="math inline">\({\bf z}\in\Delta\)</span>, then it is clear <span class="math inline">\(^k{\bf z}\in G(i_{\mathcal T_2})\)</span>. Suppose <span class="math inline">\({\bf z}\notin\Delta\)</span>. For any <span class="math inline">\(\epsilon&gt;0\)</span>, there is a bijection <span class="math inline">\(\gamma_{\epsilon}\)</span> from <span class="math inline">\(G(i_{\mathcal T_1})\)</span> to <span class="math inline">\(G(i_{\mathcal T_2})\)</span> such that <span class="math display">\[\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma_{\epsilon}(^k{\bf x}))||_{\infty}^p&lt;\epsilon\]</span> Therefore we have <span class="math inline">\(||{\bf z}-\pi_{\mathcal T_2}(\gamma_{\epsilon}(^k{\bf z}))||_{\infty}^p&lt;\epsilon\)</span>. Take <span class="math inline">\(\epsilon=1/2^{np}\)</span> for <span class="math inline">\(n=1,2,\cdots\)</span> and denote <span class="math inline">\(\pi_{\mathcal T_2}(\gamma_n(^k{\bf z}))\)</span> by <span class="math inline">\({\bf z}_n\)</span>. If <span class="math inline">\({\bf z}_n={\bf z}\)</span> for some large <span class="math inline">\(n\)</span>, then <span class="math inline">\({\bf z}\in S_{\mathcal T_2}\)</span> and <span class="math inline">\(i_{\mathcal T_1}({\bf z})=i_{\mathcal T_2}(\bf z)\)</span>. If <span class="math inline">\({\bf z}_n\)</span>'s are distinct from <span class="math inline">\({\bf z}\)</span>, we have <span class="math inline">\(||{\bf z}-{\bf z}_n||_{\infty}&lt;1/2^n\)</span>. Writing in coordinates we find that $||\chi^{{\bf z}_n}_1||_{W_p}>||\chi^{\bf z}_1||_{W_p}-1/2^n$. Then $||\mathcal T_2||_{W_p}\ge (\sum_{r=n}^N||\chi_1^{{\bf z}_r}||_{W_p}^p)^{1/p}\ge (N-n)^{1/p}||\chi^{\bf z}_1||_{W_p}-(N-n)^{1/p-1}\frac{1}{2^{n-1}}$. The last inequality holds since <span class="math inline">\(x^p\)</span> is convex for <span class="math inline">\(p\ge 1\)</span>. Let <span class="math inline">\(N\)</span> tend to infinity and we have a contradiction that <span class="math inline">\(||\mathcal T_2||_{W_p}=\infty\)</span>. Over all, we conclude that <span class="math inline">\(^k{\bf z}\in G(i_{\mathcal T_2})\)</span>. Hence <span class="math inline">\(G(i_{\mathcal T_1})\subset G(i_{\mathcal T_2})\)</span> and symmetry implies that <span class="math inline">\(G(i_{\mathcal T_2})\subset G(i_{\mathcal T_1})\)</span>. i.e. <span class="math inline">\(\mathcal T_1=\mathcal T_2\)</span>.</p><p>Let <span class="math inline">\(\mathcal T_1,\mathcal T_2,\mathcal T_3\in\mathcal D_p\)</span>. If <span class="math inline">\(W_p(\mathcal T_1,\mathcal T_2)=\infty\)</span> or <span class="math inline">\(W_p(\mathcal T_2,\mathcal T_3)=\infty\)</span>, then triangle inequality trivially holds. Otherwise, for any <span class="math inline">\(\epsilon&gt;0\)</span>, there are bijections <span class="math inline">\(\gamma_1:G(i_{\mathcal T_1})\to G(i_{\mathcal T_2})\)</span> and <span class="math inline">\(\gamma_2:G(i_{\mathcal T_2})\to G(i_{\mathcal T_3})\)</span> such that <span class="math display">\[\begin{aligned}&amp;(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))||_{\infty}^p)^{1/p}&lt;W_p(\mathcal T_1,\mathcal T_2)+\epsilon/2\\&amp;(\sum_{^k{\bf y}\in G(i_{\mathcal T_2})}||\pi_{\mathcal T_2}(^k{\bf y})-\pi_{\mathcal T_3}(\gamma_2(^k{\bf y}))||_{\infty}^p)^{1/p}&lt;W_p(\mathcal T_2,\mathcal T_3)+\epsilon/2\end{aligned}\]</span> Note that <span class="math inline">\(\gamma_2\circ\gamma_1\)</span> is a bijection from <span class="math inline">\(G(i_{\mathcal T_1})\)</span> to <span class="math inline">\(G(i_{\mathcal T_3})\)</span>. Therefore, we have <span class="math display">\[\begin{aligned}&amp; W_p(\mathcal T_1,\mathcal T_3)\le(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_3}(\gamma_2\circ\gamma_1(^k{\bf x}))||_{\infty}^p)^{1/p}\\&amp;\le(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}(||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))||_{\infty}+||\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))-\pi_{\mathcal T_3}(\gamma_2\circ\gamma_1(^k{\bf x}))||_{\infty})^p)^{1/p}\\&amp;\le(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}(||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))||_{\infty}^p)^{1/p}+(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}(||\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))-\pi_{\mathcal T_3}(\gamma_2\circ\gamma_1(^k{\bf x}))||_{\infty}^p)^{1/p}\\&amp;\le W_p(\mathcal T_1,\mathcal T_2)+W_p(\mathcal T_2,\mathcal T_3)+\epsilon\end{aligned}\]</span> where the third line used Minkowski's inequality. Since <span class="math inline">\(\epsilon\)</span> is arbitrary, the triangle inequality holds.</p><p><strong>Corollary.</strong></p><ol type="1"><li><span class="math inline">\(W_p(\mathcal T_1,\mathcal T_2)&lt;\infty\)</span> for <span class="math inline">\(\mathcal T_1,\mathcal T_2\in \mathcal D_p\)</span>;</li><li>Note that symmetry and triangle inequality holds for any persistence diagrams, but positive definiteness requires finiteness assumption;</li><li>An off-diagonal point of $\mathcal T\in \mathcal D_p$ can not be a cluster point. i.e. for any off-diagonal point ${\bf x}$ in $S_{\mathcal T}$ there is an open ball $B_{\epsilon}({\bf x})$ such that $B_{\epsilon}({\bf x})\cap S_{\mathcal T}=\{{\bf x}\}$. As a consequence, for any $\mathcal T\in\mathcal D_p$, $G(i_{\mathcal T})\backslash G(0_{\Delta})$ is at most countable.</li></ol><p><strong>Definition.</strong> A persistence diagram <span class="math inline">\(\mathcal T\in\mathcal D_p\)</span> is called constructible if <span class="math inline">\(S_{\mathcal T}\backslash\Delta\)</span> consists of finitely many points. The collection of all constructible persistence diagrams is denoted by <span class="math inline">\(\mathcal C_p\)</span>.</p><p><strong>Remark.</strong> The name 'constructible' comes from the fact that any constructible persistence diagram can be obtained as a persistence diagram of a finite filtration of simplicial complexes.</p><p><strong>Theorem.</strong> Let <span class="math inline">\(\mathcal T\in\mathcal D_p\)</span> be arbitrary and <span class="math inline">\(\epsilon&gt;0\)</span> be any given positive number. There exists <span class="math inline">\(\mathcal T_c\in\mathcal C_p\)</span> and <span class="math inline">\(\mathcal T_s\in\mathcal D_p\)</span> such that <span class="math inline">\(\mathcal T=\mathcal T_c+\mathcal T_s\)</span> with <span class="math inline">\(||\mathcal T_s||_{W_p}&lt;\epsilon\)</span>.</p><p><strong>Proof.</strong> Enumerate the off-diagonal points $\{{\bf x}_1,{\bf x}_2,\cdots\}=S_{\mathcal T}\backslash\Delta$. By definition, <span class="math display">\[||\mathcal T||_{W_p}=(\sum_{n=1}^{\infty}||\chi^{\bf x_n}_{k_n}||_{W_p}^p)^{1/p}=(\lim_{l\to\infty}\sum_{||\chi^{\bf x_n}_1||_{W_p}\ge\frac{1}{l}}||\chi^{\bf x_n}_{k_n}||_{W_p}^p)^{1/p}&lt;\infty\]</span> Thus, for <span class="math inline">\(\epsilon&gt;0\)</span>, there exists <span class="math inline">\(l&gt;0\)</span> such that <span class="math inline">\(||\mathcal T||_{W_p}^p-\sum_{||\chi^{\bf x_n}_1||_{W_p}\ge\frac{1}{l}}||\chi^{\bf x_n}_{k_n}||_{W_p}^p&lt;\epsilon^p\)</span>. Let <span class="math inline">\(\mathcal T_u\)</span> be the persistence diagram such that <span class="math inline">\(S_{\mathcal T_u}\)</span> consists of those points in <span class="math inline">\(\mathcal T\)</span> with <span class="math inline">\(||\chi^{\bf x}_1||_{W_p}\ge 1/l\)</span> and <span class="math inline">\(i_{\mathcal T_u}\)</span> is the restriction of <span class="math inline">\(i_{\mathcal T}\)</span> on <span class="math inline">\(S_{\mathcal T_u}\)</span>, <span class="math inline">\(\mathcal T_s\)</span> be such that <span class="math inline">\(S_{\mathcal T_s}\)</span> consists of points with <span class="math inline">\(||\chi^{\bf x}_1||_{W_p}&lt;1/l\)</span> and <span class="math inline">\(i_{\mathcal T_s}\)</span> is the restriction of <span class="math inline">\(i_{\mathcal T}\)</span> on <span class="math inline">\(S_{\mathcal T_s}\)</span>. It is immediate that <span class="math inline">\(\mathcal T_u\in\mathcal C_p\)</span> and <span class="math inline">\(||\mathcal T_s||_{W_p}&lt;\epsilon\)</span> and <span class="math inline">\(\mathcal T=\mathcal T_u+\mathcal T_s\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Some Applications of Wasserstein Distances</title>
      <link href="/2019/11/09/Some-Applications-of-Wasserstein-Distances/"/>
      <url>/2019/11/09/Some-Applications-of-Wasserstein-Distances/</url>
      
        <content type="html"><![CDATA[<h2 id="omt-i">OMT I</h2><p>The original setting for optimal mass transportation (OMT) consists of three objects.</p><ol type="1"><li>Two probability spaces <span class="math inline">\((X,\mu)\)</span> and <span class="math inline">\((Y,\nu)\)</span>. i.e. <span class="math inline">\(\mu(X)=\nu(Y)=1\)</span>;</li><li>A measurable map <span class="math inline">\(T:X\to Y\)</span> such that <span class="math inline">\(T_*\mu=\nu\)</span>. i.e. for any Borel set <span class="math inline">\(B\subset Y\)</span> it holds <span class="math inline">\(\nu(B)=\mu(T^{-1}(B))\)</span>.</li><li>A cost function <span class="math inline">\(c:X\times Y\to \mathbb R_+\cup\{\infty\}\)</span> such that for any <span class="math inline">\(T\)</span> in 2. the map <span class="math inline">\(c_T(x)=c(x,T(x))\)</span> is measurable. The integration <span class="math inline">\(\int_X c(x,T(x))d\mu\)</span> is called the total cost.</li></ol><a id="more"></a><p>We require that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are probability spaces. However, in practicing many examples do not arise naturally as probability spaces. In those cases we require <span class="math inline">\(\mu(X)=\nu(Y)\)</span> for finite or infinite measures. In the second setting note that we use the <strong>pushforward</strong> of <span class="math inline">\(\mu\)</span> but not the pullback of <span class="math inline">\(\nu\)</span>. This has a practical meaning, see <a href="https://bookstore.ams.org/gsm-58/" target="_blank" rel="noopener">Villani</a> for a reference.</p><p>The original problem is raised by Monge that one needs to find a 'transference plan' <span class="math inline">\(T\)</span> to minimize the total cost <span class="math inline">\(\int_X c(x,T(x))d\mu\)</span>.</p><p>Despite solving Monge's problem, consider the special case where <span class="math inline">\((X,d)\)</span> is a metric space. <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span> are two measures on <span class="math inline">\(X\)</span> with finite <span class="math inline">\(p\)</span>th moments, i.e. <span class="math inline">\(\int_Xd(x,x_0)^pd\mu&lt;\infty,\int_Xd(x,x_0)^pd\nu&lt;\infty\)</span> for any <span class="math inline">\(x_0\in X\)</span>. Let <span class="math inline">\(T:X\to X\)</span> range over all measure preserving map so that we can obtain the quantity <span class="math inline">\((\inf\{\int_Xd(x,T(x))^p\})^{1/p}\)</span> which is called the <span class="math inline">\(p\)</span>th Wasserstein distance between <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span>, denoted by <span class="math inline">\(W_p(\mu,\nu)\)</span>. It can be proved that <span class="math inline">\(W_p\)</span> is a metric on the space of measures with finite <span class="math inline">\(p\)</span>th order on <span class="math inline">\(X\)</span>.</p><h2 id="shape-classification">Shape Classification</h2><p>Here is an example that Wasserstein distance is used to classify shapes. See <a href="https://www.researchgate.net/publication/305887246_Surface-based_shape_classification_using_Wasserstein_distance" target="_blank" rel="noopener">this paper</a> for details of methods, and <a href="https://www.researchgate.net/publication/332180388_A_Geometric_View_of_Optimal_Transportation_and_Generative_Model" target="_blank" rel="noopener">this paper</a> for details of proofs.</p><p>Let <span class="math inline">\(\mathbb D\)</span> be the unit disk in <span class="math inline">\(\mathbb R^2\)</span> (generally any compact and convex set in <span class="math inline">\(\mathbb R^n\)</span> can be considered), equipped with Lebesgue measure <span class="math inline">\(m\)</span>. Let <span class="math inline">\(Y=\{y_1,\cdots,y_k\}\)</span> be a discrete subset in <span class="math inline">\(\mathbb R^2\)</span> with weighted counting measure <span class="math inline">\(\delta=\sum_{i=1}^k b_i\delta_{y_i}\)</span> such that <span class="math inline">\(\delta(Y)=\pi\)</span>. In other words, we have two measures on <span class="math inline">\(\mathbb R^2\)</span> where the first is supported on <span class="math inline">\(\mathbb D\)</span> and the second is supported on <span class="math inline">\(Y\)</span>. Monge's problem states that a map <span class="math inline">\(f:\mathbb D\to Y\)</span> such that <span class="math inline">\(f_*m=\nu\)</span> is to be find to minimize <span class="math inline">\(\int_{\mathbb D}|x-f(x)|^2dx\)</span>, where we use the standard metric on <span class="math inline">\(\mathbb R^2\)</span>. Note that <span class="math inline">\(Y\)</span> is discrete. This problem is equivalent to partition <span class="math inline">\(\mathbb D\)</span> into <span class="math inline">\(k\)</span> subsets <span class="math inline">\(D_1\cup\cdots\cup D_k\)</span> such that <span class="math inline">\(m(D_i)=b_i\)</span>. Therefore, we can use techniques in <a href="https://www.fmf.uni-lj.si/~lavric/hug&amp;weil.pdf" target="_blank" rel="noopener">convex geometry</a>.</p><p>Let <span class="math inline">\({\bf h}=(h_1,\cdots,h_k)\in\mathbb R^k\)</span>. Define a piecewise linear convex function <span class="math inline">\(u_{\bf h}(x)=\max\{x\cdot y_i+h_i\}\)</span>. Let <span class="math inline">\(G({\bf h})\)</span> be the graph of <span class="math inline">\(u_{\bf h}\)</span>. Then <span class="math inline">\(G({\bf h})\)</span> is a piecewise hyperplane. The projection of <span class="math inline">\(G({\bf h})\)</span> to <span class="math inline">\(\mathbb D\)</span> gives a cell decomposition <span class="math inline">\(\mathbb D=\cup_{i=1}^k W_i\)</span>, where each <span class="math inline">\(W_i\)</span> corresponds to a piece of plane <span class="math inline">\(\{(x,x\cdot y_i+h_i)|x\in W_i\}\)</span>. Assign each <span class="math inline">\(W_i\)</span> to <span class="math inline">\(y_i\)</span>. By moving <span class="math inline">\({\bf h}\)</span> we can find the request assignment with <span class="math inline">\(m(W_i)=b_i\)</span>. The fact is that, this is the unique map minimizing the total cost (see <a href="https://www.researchgate.net/publication/227632352_Polar_Factorization_and_Monotone_Rearrangement_of_Vector-Valued_Functions" target="_blank" rel="noopener">Brenier</a>, <a href="https://www.researchgate.net/publication/220616351_Power_Diagrams_Properties_Algorithms_and_Applications" target="_blank" rel="noopener">Aurenhammer</a>). Therefore, the problem reduces to find <span class="math inline">\({\bf h}\)</span> in some suitable space <span class="math inline">\(H_0\)</span>. This is done by using Newton's method after defining an objective function <span class="math inline">\(E({\bf h})\)</span> which is twice differentiable and the hessian is given in a closed form.</p><p>For two arbitrary surfaces <span class="math inline">\(M_1,M_2\)</span> in <span class="math inline">\(\mathbb R^3\)</span> (represented by meshes), we use conformal maps to map <span class="math inline">\(M_i\)</span> to unit disks <span class="math inline">\(\mathbb D_i\)</span>. For the first disk <span class="math inline">\(\mathbb D_1\)</span> we equip it with Lebesgue measure. For the second disk <span class="math inline">\(\mathbb D_2\)</span> we assign each point (projected from <span class="math inline">\(M_2\)</span>) a value so that it is equipped with a weighted counting measure. The Wasserstein distance from <span class="math inline">\(\mathbb D_1\)</span> to <span class="math inline">\(\mathbb D_2\)</span> (in fact. <span class="math inline">\(m\)</span> to <span class="math inline">\(\delta\)</span>) measures the difference between <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span>.</p><h2 id="omt-ii">OMT II</h2><p>Consider the following example: <span class="math inline">\(X=\{x_1,\cdots,x_n\}\)</span> and <span class="math inline">\(Y=\{y_1,\cdots,y_n\}\)</span> are finite sets with the same cardinality. Equip <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with counting measure. Then a measure preserving map <span class="math inline">\(f:X\to Y\)</span> is exactly a permutation. In many cases it is not adequate to use 'maps' only, since each <span class="math inline">\(x_i\)</span> cannot be 'split' under maps. If we are permitted to partition each <span class="math inline">\(x_i\)</span> into pieces, then a transference plan is a matrix <span class="math inline">\(\Pi=[\pi_{ij}]\)</span> where <span class="math inline">\(\sum_j \pi_{ij}=1\)</span> for each <span class="math inline">\(i\)</span> and <span class="math inline">\(\sum_{i} \pi_{ij}=1\)</span> for each <span class="math inline">\(j\)</span>, i.e. <span class="math inline">\(\Pi\)</span> is a bistochastic matrix. This idea was raised by Kantorovich and the more general setting for OMT is</p><ol type="1"><li><span class="math inline">\((X,\mu)\)</span> and <span class="math inline">\((Y,\nu)\)</span> are probability spaces;</li><li><span class="math inline">\(\pi\)</span> is a probability measure on <span class="math inline">\(X\times Y\)</span> such that the maginal distributions are <span class="math inline">\(\mu\)</span> on <span class="math inline">\(X\)</span> and <span class="math inline">\(\nu\)</span> on <span class="math inline">\(Y\)</span> respectively;</li><li><span class="math inline">\(c:X\times Y\to \mathbb R_+\cup\{\infty\}\)</span> is a <span class="math inline">\(\pi\)</span> measurable function. The integration <span class="math inline">\(\int_{X\times Y}c(x,y)d\pi\)</span> is called the total cost.</li></ol><p>Kantorovich's problem is to find a probability measure <span class="math inline">\(\pi\)</span> so that the total cost is minimized. Note that it is a linear programming problem so we can formulate this in a dual form. The Kantorovich dual problem is <span class="math display">\[W_c(\mu,\nu)=\max_{\phi,\psi}\{\int_X\phi(x)d\mu+\int_Y\psi(y)d\nu\}\]</span> where <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\psi\)</span> are functions on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> respectively and <span class="math inline">\(\phi(x)+\psi(y)\le c(x,y)\)</span>. Define <span class="math inline">\(\phi^c(y)=\inf_{x\in X}\{c(x,y)-\phi(x)\}\)</span>. Then (1) is equivalent to <span class="math display">\[W_c(\mu,\nu)=\max_{\phi}\{\int_X\phi(x)d\mu+\int_Y\phi^c(y)d\nu \}\]</span></p><h2 id="wgan">WGAN</h2><p>In <a href="http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf" target="_blank" rel="noopener">this paper</a> Wasserstein distance is first introduced to Generative Adversarial Network and the model is called WGAN. A resent <a href="https://www.researchgate.net/publication/332180388_A_Geometric_View_of_Optimal_Transportation_and_Generative_Model" target="_blank" rel="noopener">paper</a> by Gu Xianfeng etc. used geometry to interprete the role of Wasserstein distance in WGAN.</p><p>A GAN consists of a generator (G) and a discriminator (D). (G) generates artificial data and (D) discriminate them from real data. Suppose real data lies in a high dimensional space <span class="math inline">\(\Chi\)</span> and its distribution <span class="math inline">\(\nu\)</span> supports around a low dimensional manifold <span class="math inline">\(\mathcal M\)</span>. A local chart <span class="math inline">\((U,\tau)\)</span> is an open set in <span class="math inline">\(\mathcal M\)</span> together with a map <span class="math inline">\(\tau:U\to Z\)</span> from <span class="math inline">\(U\)</span> to the latent space <span class="math inline">\(Z\)</span>. The inverse of <span class="math inline">\(\tau\)</span> is called a parametrization. If <span class="math inline">\(\mu\)</span> is a distribution on <span class="math inline">\(Z\)</span> then a local parametrization pushes forward <span class="math inline">\(\mu\)</span> to be a distribution on <span class="math inline">\(\mathcal M\)</span>. i.e. the local parametrization <strong>is</strong> generator (G). Assume the training parameter for (G) is <span class="math inline">\(\theta\)</span> and denote the parametrization by <span class="math inline">\(g_\theta\)</span>. (G) generates data on <span class="math inline">\(\mathcal M\)</span> whose distribution is <span class="math inline">\((g_\theta)_*\mu\)</span>. But how to discriminate it from the real distribution <span class="math inline">\(\nu\)</span>? The answer is Wasserstein distance <span class="math inline">\(W((g_\theta)_*,\nu)\)</span>. More specifically, by Kantorovich dual we need to compute <span class="math display">\[W_c((g_\theta)_*\mu,\nu)=\max_{\phi}\{\int_Z\phi(g_\theta(z))d\mu+\int_Y\phi^c(y)d\nu\}\]</span> Suppose the training parameter for (D) is <span class="math inline">\(\xi\)</span>. Then we can rewrite (3) as <span class="math display">\[W_c((g_\theta)_*\mu,\nu)=\max_{\xi}(\mathbb E_{z\sim \mu}\phi(g_\theta(z))+\mathbb E_{y\sim \nu}\phi^c(y))\]</span> Therefore, the discriminator (D) <strong>is</strong> a calculator computing the Wasserstein distance. Above all, we can write down the objective function of a WGAN <span class="math display">\[\min_{\theta}\max_{\xi}(\mathbb E_{z\sim \mu}\phi(g_\theta(z))+\mathbb E_{y\sim \nu}\phi^c(y))\]</span></p>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Wasserstein Distances </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stability Theorems in Topological Data Analysis (3)</title>
      <link href="/2019/10/02/Stability-Theorems-in-Topological-Data-Analysis-3/"/>
      <url>/2019/10/02/Stability-Theorems-in-Topological-Data-Analysis-3/</url>
      
        <content type="html"><![CDATA[<p>Perhaps the most important case in TDA is point cloud data, i.e. finite metric spaces. The collection of all compact metric spaces makes up a metric space under <a href="%5Bhttps://en.wikipedia.org/wiki/Gromov%E2%80%93Hausdorff_convergence%5D(https://en.wikipedia.org/wiki/GromovHausdorff_convergence)">Gromov-Hausdorff metric</a>. If we construct Rips filtrations for point clouds, we obtain a map between metric spaces by sending each point cloud to its persistence diagram. It is natural to ask which properties will this map have. From the work of <a href="http://fodava.gatech.edu/files/reports/FODAVA-09-24.pdf" target="_blank" rel="noopener">F. Memoli etc.</a>, it is in fact a <strong>Lipschitz</strong> map. Therefore, the bottleneck distance between two persistence diagrams is bounded by the Gromov-Hausdorff distance between two point clouds. This stability shows that one can use TDA to classify different point clouds, which is a major object in shape analysis.</p><p>Let <span class="math inline">\((Z,d_Z)\)</span> be a (compact) metric space and <span class="math inline">\(X,Y\)</span> be subsets in <span class="math inline">\(Z\)</span>. The Hausdorff distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as <span class="math display">\[d_H^Z(X,Y)=\max\{\max_{x\in X}\min_{y\in Y}d_Z(x,y),\max_{y\in Y}\min_{x\in X}d_Z(x,y)\}.\]</span> The intuition is follows: For each <span class="math inline">\(x\in X\)</span>, we can compute the distance between <span class="math inline">\(x\)</span> and <span class="math inline">\(Y\)</span>, which is <span class="math inline">\(\min_{y\in Y}d_Z(x,y)\)</span>. Then we take the largest value as the distance from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>. The distance is the so called one-sided Hausdorff distance. Symmetrically, we compute the distance from <span class="math inline">\(Y\)</span> to <span class="math inline">\(X\)</span>. The maximum is the so called (two-sided) Hausdorff distance.</p><a id="more"></a><p>If <span class="math inline">\((X,d_X)\)</span> and <span class="math inline">\((Y,d_Y)\)</span> are different metric spaces, we cannot compare the distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> unless they are subspaces of another space <span class="math inline">\((Z,d_Z)\)</span>. Therefore, we try to embed <span class="math inline">\((X,d_X)\)</span> and <span class="math inline">\((Y,d_Y)\)</span> into a larger space so that they can be compared using Hausdorff distance. This motivates the following definition of Gromov-Hausdorff distance<br><span class="math display">\[d_{GH}((X,d_X),(Y,d_Y))=\inf_{X,Y\hookrightarrow Z}d_H^Z(X,Y).\]</span> Let <span class="math inline">\(\mathcal{X}=\{(X,d_X):X\text{ is compact metric space}\}\)</span>. Define an equivalence relation on <span class="math inline">\(\mathcal{X}\)</span>: <span class="math inline">\((X,d_X)\sim(Y,d_Y)\)</span> if they are isometric. The quotient space is still denoted by <span class="math inline">\(\mathcal{X}\)</span>. Then <span class="math inline">\((\mathcal{X},d_{GH})\)</span> is a complete metric space (see <a href="https://www.amazon.com/Course-Metric-Geometry-Dmitri-Burago/dp/0821821296" target="_blank" rel="noopener">this book</a> for reference). Furthermore, the infimum is in fact a minimum, that is, we can always find <span class="math inline">\(Z\)</span> for compact spaces.</p><p>Given a finite metric space <span class="math inline">\((X,d_X)\)</span> and a parameter <span class="math inline">\(\alpha&gt;0\)</span>, the Rips complex <span class="math inline">\(R_\alpha(X,d_X)\)</span> is an abstract simplicial complex with vertex set <span class="math inline">\(X\)</span>. A <span class="math inline">\(k\)</span>-simplex is in <span class="math inline">\(R_\alpha(X,d_X)\)</span> if and only if the diameter is no more than <span class="math inline">\(2\alpha\)</span>. (<strong>Caveat.</strong> The factor 2 makes a difference in the main theorem. Note that it is a classical issue that there is no universal parametrization on Rips complex.) When <span class="math inline">\(\alpha\)</span> ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(\infty\)</span>, the nested family is called Rips filtration, denoted by <span class="math inline">\(\mathcal{R}(X,d_X)\)</span>.</p><p>Moreover, if <span class="math inline">\(X\)</span> is a finite subset of <span class="math inline">\((\mathbb{R}^n,l^\infty)\)</span>, there is another construction called <span class="math inline">\(\check{C}ech\)</span> complex which is the nerve of <span class="math inline">\(l^\infty\)</span> balls, denoted by <span class="math inline">\(C_\alpha(X,l^\infty)\)</span>. It happens that the two constructions are the same.</p><hr><p><strong>Lemma</strong>. For any finite set <span class="math inline">\(X\)</span> in <span class="math inline">\((\mathbb{R}^n,l^\infty)\)</span> and <span class="math inline">\(\alpha&gt;0\)</span>, <span class="math inline">\(C_\alpha(X,l^\infty)=R_\alpha(X,l^\infty)\)</span>.</p><p><strong>proof</strong>. If <span class="math inline">\(\{x_1,\cdots,x_k\}\)</span> is in <span class="math inline">\(C_\alpha\)</span>, then there is <span class="math inline">\(\bar{x}\)</span> such that <span class="math inline">\(|x_i-\bar{x}|_\infty\le \alpha\)</span>. By triangle inequality <span class="math inline">\(|x_i-x_j|_\infty\le 2\alpha\)</span>. <span class="math inline">\(\{x_1,\cdots,x_k\}\)</span> is in <span class="math inline">\(R_\alpha\)</span>. On the other hand, if <span class="math inline">\(|x_i-x_j|_\infty\le 2\alpha\)</span>, for each coordinate <span class="math inline">\(l\)</span> we take <span class="math inline">\(\bar{x}^{(l)}=1/2(x^{(l)}_{\max}+x^{(l)}_{\min})\)</span>. It is easy to verify <span class="math inline">\(|x_i-\bar{x}|_\infty\le \alpha\)</span>. <span class="math inline">\(\{x_1,\cdots,x_k\}\)</span> is in <span class="math inline">\(C_\alpha\)</span>.</p><hr><p>Another useful lemma can be found in the <a href="https://www.amazon.com/Course-Metric-Geometry-Dmitri-Burago/dp/0821821296" target="_blank" rel="noopener">book</a> which says that any finite metric space of cardinality <span class="math inline">\(n\)</span> can be isometrically embedded into <span class="math inline">\((\mathbb{R}^n,l^\infty)\)</span>. The proof is also straightforward. Put the distances in the coordinates in <span class="math inline">\(\mathbb{R}^n\)</span>. One also finds a comprehensive introduction to embedding of finite metric spaces <a href="https://sites.cs.ucsb.edu/~suri/cs235/MatousekMetric.pdf" target="_blank" rel="noopener">here</a>.</p><p>Now we can state and prove the stability theorem.</p><hr><p><strong>Theorem</strong>. For any finite metric spaces <span class="math inline">\((X,d_X)\)</span> and <span class="math inline">\((Y,d_Y)\)</span>, for any <span class="math inline">\(k\in \mathbb{N}\)</span>, <span class="math display">\[d^\infty_B(D_k\mathcal{R}(X,d_X),D_k\mathcal{R}(Y,d_Y))\le d_{GH}((X,d_X),(Y,d_Y)).\]</span> <strong>proof</strong>. Let <span class="math inline">\(\epsilon=d_{GH}((X,d_X),(Y,d_Y))\)</span>. By definition, there is a compact metric space <span class="math inline">\((Z,d_Z)\)</span> and inclusions <span class="math inline">\(\gamma_X:X\to Z\)</span> and <span class="math inline">\(\gamma_Y:Y\to Z\)</span> such that <span class="math inline">\(d_H^Z(X,Y)\le \epsilon\)</span>. Since <span class="math inline">\(\gamma_X(X)\cup\gamma_Y(Y)\)</span> is finite, they can be isometrically embedded into <span class="math inline">\((\mathbb{R}^n,l^\infty)\)</span> for some <span class="math inline">\(n\)</span>. Denote the embedding by <span class="math inline">\(\gamma\)</span>. Let <span class="math inline">\(\delta_X\)</span> be the distance function to <span class="math inline">\(\gamma(\gamma_X(X))\)</span>. That is,<br><span class="math display">\[\delta_X(z)=\min_{x\in X}|z-\gamma(\gamma_X(x))|_\infty.\]</span> Similarly, let <span class="math inline">\(\delta_Y\)</span> be the distance function to <span class="math inline">\(\gamma(\gamma_Y(Y))\)</span>. Note that <span class="math display">\[|\delta_X(z)-\delta_Y(z)|\le d_H^\infty(\gamma(\gamma_X(X)),\gamma(\gamma_Y(Y)))=\epsilon.\]</span> (<em>I have doubt in this inequality. The closest point in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> may not correspond in Hausdorff distance</em>).</p><hr><p><em>update.</em> This inequality is true. Note that <span class="math inline">\(\delta_X(z)=d(z,X)\)</span>. For any point <span class="math inline">\(y\in Y\)</span> it holds <span class="math inline">\(d(z,X)\le d(z,y)+d(y,X)\)</span>. Since <span class="math inline">\(\inf_y(d(z,y)+d(y,X))\le \inf_yd(z,y)+\sup_yd(y,X)\)</span>, we obtain <span class="math inline">\(d(z,X)-d(z,Y)\le H(X,Y)\)</span>. The other side is similar.</p><hr><p>Since <span class="math inline">\(\delta_X\)</span> and <span class="math inline">\(\delta_Y\)</span> are <span class="math inline">\(\epsilon\)</span>-close, by the theorem of <a href="https://link.springer.com/article/10.1007%2Fs00454-006-1276-5" target="_blank" rel="noopener">Edelsbrunner etc.</a>, the persistence diagrams of <span class="math inline">\(\delta_X\)</span> and <span class="math inline">\(\delta_Y\)</span> are <span class="math inline">\(\epsilon\)</span>-close.</p><p>However, the level set <span class="math inline">\(\delta_X([0,\alpha])\)</span> is nothing but the union of <span class="math inline">\(\alpha\)</span>-balls in <span class="math inline">\((\mathbb{R}^n,l^\infty)\)</span>. By persistence nerve theorem the sublevel filtration of <span class="math inline">\(\delta_X\)</span> and the <span class="math inline">\(\check{C}ech\)</span> filtration of <span class="math inline">\(\gamma(\gamma_X(X))\)</span> yield same persistence diagrams. From the above lemma, <span class="math inline">\(C_\alpha(\gamma(\gamma_X(X)),l^\infty)=R_\alpha(\gamma(\gamma_X(X)),l^\infty)=R_\alpha(\gamma_X(X),d_Z)=R_\alpha(X,d_X)\)</span>. Therefore, the persistence diagram of <span class="math inline">\(\delta_X\)</span> is exactly the persistence diagram <span class="math inline">\(D\mathcal{R}(X,d_X)\)</span>. Apply the same argument to <span class="math inline">\(Y\)</span>. Hence the inequality holds.</p><hr>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stability Theorems in Topological Data Analysis (2)</title>
      <link href="/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/"/>
      <url>/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/</url>
      
        <content type="html"><![CDATA[<p>One observes that when proving the <span class="math inline">\(L^{\infty}\)</span> stability theorem, the functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> provide nothing but an interleaved inclusion of level sets <span class="math inline">\(f^{-1}((-\infty,a])\subseteq g^{-1}((-\infty,a+\epsilon])\subseteq f^{-1}((-\infty.a+2\epsilon])\)</span>. This further gives the following commutative diagram.</p><a id="more"></a><figure><img src="/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/interleavedmodules.png" alt="interleaved modules"><figcaption aria-hidden="true">interleaved modules</figcaption></figure><p>Instead of taking functions and spaces into account, let us forget about geometry and work on algebra directly. That is, we consider two sequences of modules such that the above diagram commutes. In the terminology of <a href="https://geometry.stanford.edu/papers/ccggo-ppmd-09/ccggo-ppmd-09.pdf" target="_blank" rel="noopener">F. Chazal etc.</a>, the two sequences of modules are called <em>(weakly/strongly) <span class="math inline">\(\epsilon\)</span>-interleaved</em>. It would be no surprise that the distance between two persistence diagrams are bounded by <span class="math inline">\(\epsilon\)</span>. However, when working in an algebraic approach, we can drop many restrictions on spaces and functions. Furthermore, we generalize the definition of persistence diagrams hence the conclusion is also strengthened.</p><hr><p>Let <span class="math inline">\(A\)</span> be a subset of <span class="math inline">\(\mathbb{R}\)</span> and <span class="math inline">\(R\)</span> be a field. A <strong>persistence module</strong> <span class="math inline">\(\mathcal{F}_A\)</span> is a collection of vector spaces <span class="math inline">\(\{F_{\alpha}\}_{\alpha\in A}\)</span> together with linear maps <span class="math inline">\(\{f_{\alpha}^{\alpha&#39;}:F_\alpha\to F_{\alpha&#39;}\}\)</span> such that for any <span class="math inline">\(\alpha\le \alpha&#39;\le\alpha&#39;&#39;\)</span>, we have <span class="math inline">\(f^{\alpha&#39;&#39;}_\alpha=f^{\alpha&#39;&#39;}_{\alpha&#39;}\circ f^{\alpha&#39;}_\alpha\)</span> and <span class="math inline">\(f^\alpha_\alpha=id_{F_\alpha}\)</span>. A persistence module is called <strong>tame</strong> if each vector space is of finite dimension. In the following all persistence modules are assumed to be tame.</p><p>Let <span class="math inline">\(\Delta\)</span> be the diagonal <span class="math inline">\(\{(x,x)|x\in\bar{\mathbb{R}}\}\)</span> and <span class="math inline">\(\Delta_{+}\)</span> be the space on and upon the diagonal <span class="math inline">\(\{(x,y)|y\ge x\}\)</span>. If <span class="math inline">\(A\)</span> is discrete with no accumulation point, we can define the <strong>persistence diagram</strong> of <span class="math inline">\(\mathcal{F}_A\)</span> to be a multiset <span class="math inline">\(\mathcal{DF}_A\)</span> in <span class="math inline">\(\Delta_{+}\)</span>. The points in <span class="math inline">\(\mathcal{DF}_A\)</span> are in the form <span class="math inline">\((\alpha,\alpha&#39;)\)</span> where <span class="math inline">\(\alpha\le \alpha&#39;\in A\)</span>. For each point on the diagonal <span class="math inline">\(\Delta\)</span> the multiplicity is <span class="math inline">\(\infty\)</span>. For each point off the diagonal, the multiplicity is <span class="math display">\[\mu(\alpha_i,\alpha_j)=\left\{\begin{array}{l}rank(f^{\alpha_{j-1}}_{\alpha_i})-rank(f^{\alpha_j}_{\alpha_i}),\alpha_i=\inf A\\rank(f^{\alpha_{j-1}}_{\alpha_i})-rank(f^{\alpha_j}_{\alpha_i})+rank(f^{\alpha_j}_{\alpha_{i-1}})-rank(f^{\alpha_{j-1}}_{\alpha_{i-1}}),else\end{array}\right.\]</span> This coincides with the <a href="http://yueqicao.top/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/">previous definition</a>. To define the persistence diagram of an arbitrary index set needs some preparation.</p><p>Let <span class="math inline">\(B\subseteq A\)</span> be a subset with no accumulation point. Let <span class="math inline">\(\Gamma_B\)</span> be the grid <span class="math inline">\(\{(\beta_i,\beta_j)|\beta\in B\}\)</span>. To each grid <span class="math inline">\(\Gamma_B\)</span> is associated with a <em>B-pixelization map</em> <span class="math inline">\(pix_B:\Delta_{+}\to \Gamma_B\cup\Delta\)</span>. If a cell does not hit the diagonal, <span class="math inline">\(pix_B\)</span> sends each point in the cell to the upper right grid point. If a cell hits the diagonal, then each point is send to the closest point on the diagonal.</p><figure><img src="/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/pixelization.png" alt="pixelizationmap"><figcaption aria-hidden="true">pixelizationmap</figcaption></figure><p>Since we can always pick out a subsequence in the persistence module, it is natural to define the persistence diagram of <span class="math inline">\(\mathcal{F}_A\)</span> through an approximation of <span class="math inline">\(\mathcal{DF}_B\)</span> with <span class="math inline">\(B\subseteq A\)</span>.</p><p>Let us say a persistence module <span class="math inline">\(\mathcal{F}_B\)</span> is <span class="math inline">\(\epsilon\)</span>-periodic is the index set <span class="math inline">\(B=\alpha_0+\epsilon \mathbb{Z}\)</span>. Then we have the following lemma</p><blockquote><p>Let <span class="math inline">\(B,C\subseteq A\)</span>. The pixelization map <span class="math inline">\(pix_B\)</span> defines a multi-bijection between <span class="math inline">\(\mathcal{DF}_{B\cup C}\)</span> and <span class="math inline">\(\mathcal{DF}_B\)</span>. Furthermore, for any <span class="math inline">\(\epsilon\)</span>-periodic persistence modules <span class="math inline">\(\mathcal{F}_B\)</span> and <span class="math inline">\(\mathcal{F}_C\)</span>, we have <span class="math inline">\(d_B^{\infty}(\mathcal{DF}_B,\mathcal{DF}_A)\le \epsilon\)</span>.</p></blockquote><p>Therefore, Set <span class="math inline">\(\epsilon=2^{-n}\)</span>, the persistence diagrams <span class="math inline">\(\mathcal{DF}_{B_n}\)</span> will form a Cauchy sequence under bottleneck distance. The diagram of <span class="math inline">\(\mathcal{F}_A\)</span> is defined to be the limit of this sequence.</p><hr><p>As mentioned in the beginning, the interleaving between level sets of spaces become interleaving between persistence modules. Two persistence modules <span class="math inline">\(\mathcal{F}_{\mathbb{R}}\)</span> and <span class="math inline">\(\mathcal{G}_{\mathbb{R}}\)</span> are <strong>strongly <span class="math inline">\(\epsilon\)</span>-interleaved</strong> if there exists two families of homomorphisms <span class="math inline">\(\{\phi_\alpha:F_\alpha\to G_{\alpha+\epsilon}\}\)</span> and <span class="math inline">\(\{\psi_{\alpha}:G_\alpha:\to F_{\alpha+\epsilon}\}\)</span> such that the diagram in the beginning commutes. Then we have the following theorem about stability</p><blockquote><p>Let <span class="math inline">\(\mathcal{F}_{\mathbb{R}}\)</span> and <span class="math inline">\(\mathcal{G}_{\mathbb{R}}\)</span> be tame persistence modules. If <span class="math inline">\(\mathcal{F}_{\mathbb{R}}\)</span> and <span class="math inline">\(\mathcal{G}_{\mathbb{R}}\)</span> are strongly <span class="math inline">\(\epsilon\)</span>-interleaved, then <span class="math inline">\(d_B^{\infty}(\mathcal{DF}_{\mathbb R},\mathcal{DG}_{\mathbb R})\le \epsilon\)</span>.</p></blockquote><p>The proof of this theorem is lengthy. However, for persistence diagrams with finite points off the diagonal, the idea is nothing but an algebraic analogy to the geometric stability proof -- one constructs a family of persistence modules <span class="math inline">\(\mathcal{H}^s\)</span> which interpolates between <span class="math inline">\(\mathcal{F}_{\mathbb{R}}\)</span> and <span class="math inline">\(\mathcal{G}_{\mathbb{R}}\)</span>. Then, for close enough <span class="math inline">\(\mathcal{H}^s\)</span> and <span class="math inline">\(\mathcal{H}^{s&#39;}\)</span> the distance between persistence diagrams can be bounded by the <a href="http://yueqicao.top/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/">Box Lemma</a>. The bound for <span class="math inline">\(\mathcal{DF}_{\mathbb R}\)</span> and <span class="math inline">\(\mathcal{DG}_{\mathbb R}\)</span> is obtained by triangle inequality.</p><p>For general persistence diagram (with possibly infinite points off the diagonal), more techniques are involved. Two persistence diagrams <span class="math inline">\(\mathcal{F}_A\)</span> and <span class="math inline">\(\mathcal{G}_B\)</span> are <strong>weakly <span class="math inline">\(\epsilon\)</span>-interleaved</strong> if there exists some <span class="math inline">\(\alpha_0\)</span> such that <span class="math inline">\(\alpha_0+2\epsilon\mathbb Z\subseteq A\)</span> and <span class="math inline">\(\alpha_0+\epsilon+2\epsilon\mathbb Z\subseteq B\)</span> and there are homomorphisms such that the following diagram commutes</p><figure><img src="/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/weaklyinter.png" alt="weakly interleaving"><figcaption aria-hidden="true">weakly interleaving</figcaption></figure><p>One needs to prove a weakly stability theorem to assist the strong one.</p><blockquote><p>Let <span class="math inline">\(\mathcal{F}_{A}\)</span> and <span class="math inline">\(\mathcal{G}_{B}\)</span> be tame persistence modules. If <span class="math inline">\(\mathcal{F}_A\)</span> and <span class="math inline">\(\mathcal{G}_B\)</span> are weakly <span class="math inline">\(\epsilon\)</span>-interleaved, then <span class="math inline">\(d_B^{\infty}(\mathcal{DF}_A,\mathcal{DG}_B)\le 3\epsilon\)</span>, and the bound is tight.</p></blockquote><p>To obtain the <span class="math inline">\(3\epsilon\)</span> bound we need to investigate the pixelization map carefully. But we can use the pixelization lemma to prove a loose bound easily. Let <span class="math inline">\(\mathcal{H}\)</span> be the persistence module <span class="math display">\[\cdots\to G_{\alpha_0+(2n-1)\epsilon}\to F_{\alpha_0+2n\epsilon}\to G_{\alpha_0+(2n+1)\epsilon}\to\cdots\]</span> Then <span class="math inline">\(\mathcal{F}_{\alpha_0+2\epsilon\mathbb Z}\)</span> and <span class="math inline">\(\mathcal{G}_{\alpha_0+\epsilon+2\epsilon\mathbb Z}\)</span> are subcollections of <span class="math inline">\(\mathcal{H}\)</span>. By the pixelization lemma, we have <span class="math inline">\(d_B^{\infty}(\mathcal{DF}_{\alpha_0+2\epsilon\mathbb Z},\mathcal{DG}_{\alpha_0+\epsilon+2\epsilon\mathbb Z})\le 2\epsilon\)</span>. Similarly, we have <span class="math inline">\(d_B^{\infty}(\mathcal{DF}_{\alpha_0+2\epsilon\mathbb Z},\mathcal{DF}_A)\le 2\epsilon\)</span> and <span class="math inline">\(d_B^{\infty}(\mathcal{DG}_{\alpha_0+\epsilon+2\epsilon\mathbb Z},\mathcal{DG}_B)\le 2\epsilon\)</span>. By triangle inequality, we have the bound <span class="math inline">\(6\epsilon\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Curvature of Warped Products</title>
      <link href="/2019/09/03/Curvature-of-Warped-Products/"/>
      <url>/2019/09/03/Curvature-of-Warped-Products/</url>
      
        <content type="html"><![CDATA[<h2 id="curvature-of-coupled-planes">Curvature of Coupled Planes</h2><p>Consider the product of two planes <span class="math inline">\(\mathbb R^2\times\mathbb R^2\)</span>, with the following Riemannian metric <span class="math display">\[ds^2=dx^2+dy^2+e^{2f(x,y)}(du^2+dv^2),\]</span> where <span class="math inline">\(f(x,y)\)</span> is a smooth function. We compute the sectional/Ricci/scalar curvature of the 'coupled' planes using orthonormal frames. Basics about moving frames are referred to <a href="https://link.springer.com/book/10.1007/978-3-319-55084-8" target="_blank" rel="noopener">Loring W. Tu</a>.</p><a id="more"></a><p>Consider the following orthonormal bases <span class="math display">\[e_1=\partial_x,e_2=\partial_y,e_3=e^{-f}\partial_u,e_4=e^{-f}\partial_v.\]</span> The dual 1-forms are <span class="math display">\[\theta^1=dx,\theta^2=dy,\theta^3=e^{f}du,\theta^4=e^fdv.\]</span> There exists a unique skew-symmetric matrix of 1-forms <span class="math inline">\([\omega^i_j]\)</span> such that <span class="math display">\[d\theta^i+\omega^i_j\wedge\theta^j=0.\]</span> The <span class="math inline">\(\omega^i_j\)</span>'s are called the connection 1-forms, and the above equation is called <em>the first structure equation</em>. Suppose <span class="math inline">\(\omega^i_j=a^i_jdx+b^i_jdy+c^i_jdu+d^i_jdv\)</span>. Substituting into the first structure equation, we solve all the connection 1-forms <span class="math display">\[\begin{aligned}&amp;\omega^1_2=\omega^3_4=0,\\&amp;\omega^1_3=-e^ff_xdu,\omega^1_4=-e^ff_xdv,\\&amp;\omega^2_3=-e^ff_ydu,\omega^2_4=-e^ff_ydv.\end{aligned}\]</span> The curvature 2-forms are defined by <em>the second structure equation</em> <span class="math display">\[\Omega^i_j=d\omega^i_j+\omega^i_k\wedge\omega^k_j.\]</span> Therefore, we write out all the curvature forms <span class="math display">\[\begin{aligned}&amp;\Omega^1_2=0,\\&amp;\Omega^1_3=-e^f(f_{xx}+f_x^2)dx\wedge du-e^f(f_{xy}+f_xf_y)dy\wedge du,\\&amp;\Omega^1_4=-e^f(f_{xx}+f_x^2)dx\wedge dv-e^f(f_{xy}+f_xf_y)dy\wedge dv,\\&amp;\Omega^2_3=-e^f(f_{xy}+f_xf_y)dx\wedge du-e^f(f_{yy}+f_y^2)dy\wedge du,\\&amp;\Omega^2_4=-e^f(f_{xy}+f_xf_y)dx\wedge dv-e^f(f_{yy}+f_y^2)dy\wedge dv,\\&amp;\Omega^3_4=-e^{2f}(f_x^2+f_y^2)du\wedge dv.\end{aligned}\]</span> By the definition of sectional curvature, for any plane spanned by <span class="math inline">\(e_i,e_j\)</span>, <span class="math display">\[K_{ij}=\langle R(e_i,e_j)e_j,e_i\rangle=\Omega_j^i(e_i,e_j).\]</span> Hence, we can compute all the sectional curvature using curvature forms, <span class="math display">\[\begin{aligned}&amp;K_{12}=0,\\&amp;K_{13}=K_{14}=-(f_{xx}+f_x^2),\\&amp;K_{23}=K_{24}=-(f_{yy}+f_y^2),\\&amp;K_{34}=-(f_x^2+f_y^2).\end{aligned}\]</span> Ricci curvature is the sum of sectional curvatures, thus, <span class="math display">\[\begin{aligned}&amp;Ric_{11}=K_{12}+K_{13}+K_{14}=-2(f_{xx}+f_x^2),\\&amp;Ric_{22}=K_{12}+K_{23}+K_{24}=-2(f_{yy}+f_y^2),\\&amp;Ric_{33}=K_{13}+K_{23}+K_{34}=-(f_{xx}+f_{yy}+2f_x^2+2f_y^2),\\&amp;Ric_{44}=K_{14}+K_{24}+K_{34}=-(f_{xx}+f_{yy}+2f_x^2+2f_y^2),\\\end{aligned}\]</span> Scalar curvature is the sum of Ricci curvature, thus, <span class="math display">\[S=Ric_{11}+Ric_{22}+Ric_{33}+Ric_{44}.\]</span></p><h2 id="curvature-of-4-dimensional-warped-product-spaces">Curvature of 4 Dimensional Warped Product Spaces</h2><p>More generally, let <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span> be two Riemannian manifolds with metric <span class="math inline">\(g_M\)</span> and <span class="math inline">\(g_N\)</span>, respectively. Consider the product space <span class="math inline">\(M\times N\)</span> with the following metric <span class="math display">\[g=g_M+e^{2f}g_N,\]</span> where <span class="math inline">\(f\)</span> is a smooth function on <span class="math inline">\(M\)</span>. This is called the <strong>warped products</strong> of <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span>, and often denoted by <span class="math inline">\(M\times_{e^f}N\)</span> (see <a href="https://max.book118.com/html/2019/0228/6024223150002012.shtm" target="_blank" rel="noopener">John Lee, Example 2.24</a>). Let <span class="math inline">\(\theta^1,\theta^2\)</span> be the orthonormal coframe on <span class="math inline">\(M\)</span> and <span class="math inline">\(\omega^1_2\)</span> be the corresponding connection form. Similarly, let <span class="math inline">\(\theta^3,\theta^4,\omega^3_4\)</span> be the orthonormal coframe and connection form on <span class="math inline">\(N\)</span>. For the product space <span class="math inline">\(M\times N\)</span>, we have the orthonormal coframe <span class="math display">\[\bar{\theta^1}=\theta^1,\bar{\theta^2}=\theta^2,\bar{\theta^3}=e^f\theta^3,\bar{\theta^4}=e^f\theta^4.\]</span> Assume that the connection 1-form for <span class="math inline">\(M\times N\)</span> is <span class="math display">\[\bar{\omega^i_j}=a^i_j\bar{\theta_1}+b_j^i\bar{\theta_2}+c^i_j\bar{\theta_3}+d^i_j\bar{\theta_4}.\]</span> Substituting the connection 1-forms into the first structure equation, we have <span class="math display">\[\begin{aligned}&amp;\bar{\omega^1_2}=\omega^1_2,\\&amp;\bar{\omega^1_3}=-e^{-f}\frac{de^f}{\theta^1}\bar{\theta^3},\\&amp;\bar{\omega^1_4}=-e^{-f}\frac{de^f}{\theta^1}\bar{\theta^4},\\&amp;\bar{\omega^2_3}=-e^{-f}\frac{de^f}{\theta^2}\bar{\theta^3},\\&amp;\bar{\omega^2_4}=-e^{-f}\frac{de^f}{\theta^2}\bar{\theta^4},\\&amp;\bar{\omega^3_4}=\omega^3_4.\end{aligned}\]</span> Since <span class="math inline">\(de^f\)</span> is a 1-form on <span class="math inline">\(M\)</span>, it can be written as a linear combination of <span class="math inline">\(\theta^1\)</span> and <span class="math inline">\(\theta^2\)</span>. Let <span class="math inline">\(de^f/\theta^1\)</span> and <span class="math inline">\(de^f/\theta^2\)</span> be the coefficients. Similarly, let <span class="math inline">\(\omega^1_2=a\theta^1+b\theta^2\)</span>, by structure equation, <span class="math inline">\(d\theta^1=-\omega^1_2\wedge\theta^2=-a\theta^1\wedge\theta^2\)</span>, <span class="math inline">\(d\theta^2=-\omega^2_1\wedge\theta^1=-b\theta^1\wedge\theta^2\)</span>. We can write <span class="math display">\[\begin{aligned}&amp;\omega^1_2=-\frac{d\theta^1}{\theta^1\wedge\theta^2}\theta^1-\frac{d\theta^2}{\theta^1\wedge\theta^2}\theta^2,\\&amp;\omega^3_4=-\frac{d\theta^3}{\theta^3\wedge\theta^4}\theta^3-\frac{d\theta^4}{\theta^3\wedge\theta^4}\theta^4.\end{aligned}\]</span> With these notations, we can list all the curvature 2-forms <span class="math display">\[\begin{aligned}&amp;\bar{\Omega^1_2}=d\omega^1_2=\Omega^1_2,\\&amp;\bar{\Omega^1_3}=(\frac{de^f}{\theta^2}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^1)^2})\theta^1\wedge\theta^3+(\frac{de^f}{\theta^2}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{\theta^1\theta^2})\theta^2\wedge\theta^3,\\&amp;\bar{\Omega^1_4}=(\frac{de^f}{\theta^2}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^1)^2})\theta^1\wedge\theta^4+(\frac{de^f}{\theta^2}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{\theta^1\theta^2})\theta^2\wedge\theta^4,\\&amp;\bar{\Omega^2_3}=(-\frac{de^f}{\theta^1}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{\theta^2\theta^1})\theta^1\wedge\theta^3+(-\frac{de^f}{\theta^1}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^2)^2})\theta^2\wedge\theta^3,\\&amp;\bar{\Omega^2_4}=(-\frac{de^f}{\theta^1}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{\theta^2\theta^1})\theta^1\wedge\theta^4+(-\frac{de^f}{\theta^1}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^2)^2})\theta^2\wedge\theta^4,\\&amp;\bar{\Omega^3_4}=\Omega^3_4-((\frac{de^f}{\theta^1})^2+(\frac{de^f}{\theta^2})^2)\theta^3\wedge\theta^4.\end{aligned}\]</span> Hence, the sectional curvatures are <span class="math display">\[\begin{aligned}&amp;\bar{K}_{12}=K_{12},\\&amp;\bar{K}_{13}=\bar{K}_{14}=e^{-f}(\frac{de^f}{\theta^2}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^1)^2}),\\&amp;\bar{K}_{23}=\bar{K}_{24}=e^{-f}(-\frac{de^f}{\theta^1}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^2)^2}),\\&amp;\bar{K}_{34}=e^{-2f}(K_{34}-(\frac{de^f}{\theta^1})^2-(\frac{de^f}{\theta^2})^2).\end{aligned}\]</span></p><h2 id="more-examples">More Examples</h2><p>Let <span class="math inline">\(\mathbb S^2\)</span> be the unit sphere in <span class="math inline">\(\mathbb R^3\)</span>. Using stereographic projection, the <em>round metric</em> in local coordinate <span class="math inline">\((x,y)\)</span> is <span class="math display">\[g_{\mathbb S^2}=\frac{4}{(1+x^2+y^2)^2}(dx^2+dy^2).\]</span> Let <span class="math inline">\(\theta^1=2/(1+x^2+y^2)dx\)</span> and <span class="math inline">\(\theta^2=2/(1+x^2+y^2)dy\)</span> be the orthonormal coframe. We compute that the connection 1-form is <span class="math inline">\(\omega^1_2=-y\theta^1+x\theta^2\)</span>. Therefore, <span class="math display">\[\frac{d\theta^1}{\theta^1\wedge\theta^2}=y,\frac{d\theta^2}{\theta^1\wedge\theta^2}=-x.\]</span> Denote <span class="math inline">\((1+x^2+y^2)/2\)</span> by <span class="math inline">\(J\)</span>. Then, <span class="math display">\[\begin{aligned}&amp;\frac{de^f}{\theta^1}=\frac{e^ff_xdx+e^ff_ydy}{\theta^1}=e^ff_xJ,\\&amp;\frac{d^2e^f}{(\theta^1)^2}=\frac{d(e^ff_xJ)}{\theta^1}=e^f(f_x)^2J^2+e^ff_{xx}J^2+xe^ff_xJ,\\&amp;\frac{d^2e^f}{\theta^1\theta^2}=\frac{d(e^ff_xJ)}{\theta^2}=e^ff_xf_yJ^2+e^ff_{xy}J^2+ye^ff_xJ,\\\end{aligned}\]</span> Similarly, we have <span class="math display">\[\begin{aligned}&amp;\frac{de^f}{\theta^2}=\frac{e^ff_xdx+e^ff_ydy}{\theta^2}=e^ff_yJ,\\&amp;\frac{d^2e^f}{(\theta^2)^2}=\frac{d(e^ff_xJ)}{\theta^2}=e^f(f_y)^2J^2+e^ff_{yy}J^2+ye^ff_yJ,\\&amp;\frac{d^2e^f}{\theta^2\theta^1}=\frac{d(e^ff_xJ)}{\theta^1}=e^ff_xf_yJ^2+e^ff_{xy}J^2+xe^ff_yJ,\\\end{aligned}\]</span> Note that <span class="math inline">\(\frac{d^2e^f}{\theta^1\theta^2}\neq\frac{d^2e^f}{\theta^2\theta^1}\)</span>.</p><p>Let <span class="math inline">\(\mathbb H^2\)</span> be the upper plane with <em>hyperbolic metric</em><br><span class="math display">\[g_{\mathbb H^2}=\frac{1}{v^2}(du^2+dv^2).\]</span> Under this metric <span class="math inline">\(\mathbb H^2\)</span> will be a Riemannian manifold with constant curvature <span class="math inline">\(-1\)</span>. Consider the warped product <span class="math inline">\(\mathbb S^2\times_f \mathbb H^2\)</span>. The sectional curvatures are <span class="math display">\[\begin{aligned}&amp;\bar{K}_{12}=1,\\&amp;\bar{K}_{13}=\bar{K}_{14}=yf_yJ-(f_x)^2J^2-f_{xx}J^2-xf_xJ,\\&amp;\bar{K}_{23}=\bar{K}_{24}=xf_xJ-(f_y)^2J^2-f_{yy}J^2-yf_yJ,\\&amp;\bar{K}_{34}=e^{-2f}(-1-e^{2f}(f_x)^2J^2-e^{2f}(f_y)^2J^2).\end{aligned}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Warped Products </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Remarks on &#39;Submanifold density estimation&#39;</title>
      <link href="/2019/08/08/Remarks-on-Submanifold-density-estimation/"/>
      <url>/2019/08/08/Remarks-on-Submanifold-density-estimation/</url>
      
        <content type="html"><![CDATA[<p>There seems to be a gap in <a href="http://papers.nips.cc/paper/3826-submanifold-density-estimation" target="_blank" rel="noopener">Submanifold density estimation</a>.</p><p>In the deduction of variance, one needs to estimate</p><blockquote><p><span class="math inline">\(\frac{1}{h_m^n}\int_M f(q)\frac{1}{h_m^n}K^2(\frac{u_p(q)}{h_m})dV(q)\)</span></p></blockquote><p>Apply theorem 3.1 to function <span class="math inline">\(K^2\)</span> one obtains <span class="math display">\[\frac{1}{h^n}\int_M K^2(\frac{u_p(q)}{h})\xi(q)dV(q)\to\xi(p)\int_{\mathbb{R}^n}K^2(\|z\|)d^nz,h\to 0.\]</span> It seems <span class="math inline">\(h_m^{2n}\)</span> will cause trouble in estimation. Whatever, they claimed that the integration will converges to <span class="math inline">\(f(p)\int K^2(\|z\|)d^nz\)</span>. However, in the expression of variance</p><blockquote><p><span class="math inline">\(Var[\frac{1}{h_m^n}K(\frac{u_p(q)}{h_m})]=E[\frac{1}{h_m^{2n}}K^2(\frac{u_p(q)}{h_m})]-(E[\frac{1}{h_m^n}K(\frac{u_p(q)}{h_m})])^2\)</span></p></blockquote><p>the first term will converge to a constant times <span class="math inline">\(f(p)\)</span>, while the latter will converge to <span class="math inline">\(f(p)^2\)</span>. Thus the variance will not converge to 0.</p><p>The idea of proof is straightforward. Since <span class="math inline">\(K\)</span> will be supported in <span class="math inline">\([0,1]\)</span>, one notes that when the bandwidth <span class="math inline">\(h\)</span> is small, the integration will be zero outside the normal coordinate. Therefore, everything goes back to Euclidean space.</p><a id="more"></a>]]></content>
      
      
      <categories>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nonparametric Estimates </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stability Theorems in Topological Data Analysis (1)</title>
      <link href="/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/"/>
      <url>/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/</url>
      
        <content type="html"><![CDATA[<hr><p><strong>Personal Comments</strong></p><p>In this series I attempt to collect results concerning stabilities in <a href="https://en.wikipedia.org/wiki/Topological_data_analysis" target="_blank" rel="noopener">topological data analysis</a>. Researches in this direction has been increasing dramatically in recent years. Without doubt there will be more in the future. However, the best will not appear until one really understands all the progresses.</p><hr><p>The first stability theorem in TDA is proved by <a href="https://link.springer.com/article/10.1007%2Fs00454-006-1276-5" target="_blank" rel="noopener">David Cohen-Steiner, Herbert Edelsbrunner, John Harer</a>. It asserts that the bottleneck distance between two persistence diagrams is bounded by the <span class="math inline">\(\infty\)</span>-norm of tame functions. They soon generalized this result to <span class="math inline">\(L_p\)</span> cases for tame Lipschitz functions (see <a href="https://faculty.math.illinois.edu/~ymileyko/papers/L_p-stability.pdf" target="_blank" rel="noopener">here</a>). This theorem is fundamental in TDA. And the idea is also direct -- counting points in persistence diagrams. We sketch the procedure in the following.</p><a id="more"></a><p>Recall that a <em>persistence diagram</em> is a multiset in the extended plane <span class="math inline">\(\bar{\mathbb R}=\mathbb R\cup \{\infty\}\)</span>. A <em>tame</em> function <span class="math inline">\(f:X\to\mathbb R\)</span> defined on a topological space induces a finite filtration <span class="math display">\[\emptyset=f^{-1}((-\infty,a_0])\to\cdots\to f^{-1}((-\infty,a_i])\to\cdots\to f^{-1}((-\infty,a_n])=X.\]</span> Each <span class="math inline">\(a_i\)</span> is called a homological critical value. Thus, at <span class="math inline">\(a_i\)</span> the homotopy type of the sublevel sets of <span class="math inline">\(f\)</span> is changed. Tameness means that it changes finite types. Let <span class="math inline">\(b_i&lt;a_i&lt;b_{i+1}\)</span> be interleaved sequences. The maps <span class="math inline">\(H_k(f^{-1}(-\infty,b_i])\to H_k(f^{-1}(-\infty,b_{i+1}])\)</span> are <strong>not</strong> isomorphisms. For simplicity, let us drop the subscript <span class="math inline">\(k\)</span>, and denote the homology group by <span class="math inline">\(H(b_i)\)</span>. Let <span class="math inline">\(f_{x,y}\)</span> denote the map <span class="math inline">\(H(x)\to H(y)\)</span>. The image of <span class="math inline">\(f_{x,y}\)</span> is denoted by <span class="math inline">\(H(x,y)\)</span>. Set <span class="math inline">\(\beta(x,y)={\rm dim} H(x,y)\)</span>. We can define the <em>multiplicity</em> of a point <span class="math inline">\((a_i,a_j)\)</span> in persistence diagram <span class="math inline">\(D(f)\)</span> as <span class="math display">\[\mu(i,j)=\beta(b_{i-1},b_j)-\beta(b_i,b_j)+\beta(b_i,b_{j-1})-\beta(b_{i-1},b_{j-1}).\]</span> Then the <em>total multiplicity</em> (or <em>size</em>) of the persistence diagram over the diagonal is <span class="math display">\[\#(D(f)-\Delta)=\sum_{i&lt;j}\mu(i,j).\]</span> Next we <strong>count</strong> multiplicities in different areas on the diagram.</p><hr><p>Let <span class="math inline">\(Q_x^y=[-\infty,x]\times[y,\infty]\)</span> be the closed upper left quadrant. <strong>k-Triangle Lemma</strong> asserts that for <span class="math inline">\(x&lt;y\)</span> different from homological critical values, then <span class="math display">\[\#(D(f)\cap Q_x^y)=\beta(x,y).\]</span> The proof is shown in the following figure. Note that the small square with plus and minus notations explains the multiplicity at a point. When summing over, plus and minus cancels.</p><figure><img src="/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/ktriangle.png" alt="k-triangle lemma"><figcaption aria-hidden="true">k-triangle lemma</figcaption></figure><hr><p>Let <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> be two tame functions on the same topological space. Let <span class="math inline">\(\epsilon=\|f-g\|_{\infty}\)</span>. We want to compare the multiplicities of quadrants for <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>. In fact, the <strong>Quadrant Lemma</strong> asserts that <span class="math display">\[\#(D(f)\cap Q_{x-\epsilon}^{y+\epsilon})\le \#(D(g)\cap Q_x^y).\]</span> Symmetrically, we also have <span class="math display">\[\#(D(g)\cap Q_{x-\epsilon}^{y+\epsilon})\le \#(D(f)\cap Q_x^y).\]</span> Intuitively, when we shift the quadrant upper left by <span class="math inline">\(\epsilon\)</span>, the multiplicity decreases.</p><figure><img src="/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/quadrant.png" alt="quadrant lemma"><figcaption aria-hidden="true">quadrant lemma</figcaption></figure><hr><p>Let <span class="math inline">\(R=[a,b]\times[c,d]\)</span> be a closed rectangle where <span class="math inline">\(a&lt;b&lt;c&lt;d\)</span>. Let <span class="math inline">\(R_{\epsilon}=[a+\epsilon,c-\epsilon]\times[c+\epsilon,d-\epsilon]\)</span> be a smaller rectangle. The <strong>Box Lemma</strong> asserts that the points in the smaller box have less multiplicities. <span class="math display">\[\#(D(f)\cap R_{\epsilon})\le \#(D(g)\cap R).\]</span> Similarly, the positions of <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> can be exchanged in the inequality.</p><figure><img src="/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/box.png" alt="box lemma"><figcaption aria-hidden="true">box lemma</figcaption></figure><hr><p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be multisets in the extended plane. The Hausdorff distance between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is defined as <span class="math display">\[d_{\mathcal H}(A,B)=\max\{\sup_x\inf_y\|x-y\|_{\infty},\sup_y\inf_x\|y-x\|_{\infty}\},\]</span> where <span class="math inline">\(x\in A\)</span> and <span class="math inline">\(y\in B\)</span>. Using box lemma, we can easily deduce that <span class="math display">\[d_{\mathcal H}(D(f),D(g))\le \|f-g\|_{\infty},\]</span> since for each point in <span class="math inline">\(D(f)\)</span>, there is at least one point in <span class="math inline">\(D(g)\)</span>.</p><hr><p>The final step is to generalize the inequality to bottleneck distance. By definition, the bottleneck distance between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is <span class="math display">\[d_{\mathcal B}(A,B)=\inf_{\gamma}\sup_x\|x-\gamma(x)\|_{\infty},\]</span> where <span class="math inline">\(\gamma:A\to B\)</span> is <strong>bijection</strong>. Note that <span class="math inline">\(D(f)\)</span> contains the diagonal with infinite points. Therefore, the bottleneck distance is well defined. Since there are finitely many points off the diagonal in a persistence diagram, we can assume that <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are so close that in a square of length <span class="math inline">\(\|f-g\|_{\infty}\)</span> there is only one point in <span class="math inline">\(D(f)\)</span>. On the other hand, by box lemma we also have a point in <span class="math inline">\(D(g)\)</span>. This enables us to construct a bijection but the distance is <span class="math inline">\(\|f-g\|_{\infty}\)</span>. For general <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, we interpolate piecewise linear functions and use triangle inequality to pass the conclusion to <span class="math inline">\(\hat{f}\)</span> and <span class="math inline">\(\hat{g}\)</span>. In a word, we obtain the main theorem</p><blockquote><p>Let <span class="math inline">\(X\)</span> be a triangulable space with continuous tame functions <span class="math inline">\(f,h:X\to \mathbb R\)</span>. Then the persistence diagrams satisfy <span class="math inline">\(d_{\mathcal B}(D(f),D(g))\le\|f-g\|_{\infty}\)</span>.</p></blockquote><hr><p>In proving the quadrant lemma and box lemma, one notice that there is an interleaving phenomenon of the level sets of <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>. This interleaving is generalized to modules, which is purely algebraic. See <a href="https://geometry.stanford.edu/papers/ccggo-ppmd-09/ccggo-ppmd-09.pdf" target="_blank" rel="noopener">F. Chazal etc.</a> for references.</p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MSE of Locally Linear Regression</title>
      <link href="/2019/07/30/MSE-of-Locally-Linear-Regression/"/>
      <url>/2019/07/30/MSE-of-Locally-Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h2 id="backgrounds">Backgrounds</h2><p>Locally linear regression (also called locally weighted least squares) is an important method in nonparametric regression. Its understanding is intuitive. Suppose one has a general model <span class="math display">\[y=g(x)+\epsilon(x),\]</span> where <span class="math inline">\(g\)</span> is a any smooth function, and <span class="math inline">\(\epsilon(x)\)</span> is a variable with zero mean and finite variance. Let <span class="math inline">\(X_1,X_2,\cdots,X_n\)</span> be i.i.d. samples with bounded density <span class="math inline">\(f\)</span>, and <span class="math inline">\(Y_1,Y_2,\cdots, Y_n\)</span> be the corresponding responses (assume <span class="math inline">\(X_i\)</span>'s are scalars '). By Taylor expansion, we have <span class="math display">\[Y_i-g(x)\approx \beta(X_i-x)+\epsilon(x).\]</span> <a id="more"></a></p><p>To emphasize the local information around <span class="math inline">\(x\)</span>, we introduce the kernel function <span class="math inline">\(K_h(u)=K(u/h)/h\)</span>, where <span class="math inline">\(\int K(u){\rm d}u=1\)</span> and <span class="math inline">\(h&gt;0\)</span> is called the bandwidth. And consider the following optimization <span class="math display">\[\arg\min_{g,\beta}\sum_{i=1}^n(Y_i-g-\beta(X_i-x))^2K_h(X_i-x).\]</span> We introduce the following notations <span class="math display">\[\begin{gather}{\bf Y}=[Y_1,Y_2,\cdots,Y_n]^T,\\{\bf X}=\left[\begin{array}{cc}1&amp;X_1-x\\1&amp;X_2-x\\ \cdots&amp;\cdots\\ 1&amp;X_n-x\end{array}\right],\\{\bf W}={\rm diag}\{K_h(X_1-x),\cdots,K_h(X_n-x)\},\\e_1=[1,0]^T,\\e_2=[0,1]^T.\end{gather}\]</span> Then the optimization has a closed solution <span class="math display">\[\begin{gather}\hat{g}(x)=e_1^T({\bf X^TWX})^{-1}{\bf X^TWY},\\\hat{\beta}=e_2^T({\bf X^TWX})^{-1}{\bf X^TWY}.\end{gather}\]</span> For the present we analyze the conditional bias and variance of <span class="math inline">\(\hat{g}(x)\)</span>. Note that by assumptions we have <span class="math inline">\(\mathbb E[Y|X]=g(X)\)</span>,<span class="math inline">\(\mathbb E[(Y-g(X))^2|X]=\sigma^2(X)\)</span>. Let <span class="math inline">\(g({\bf X})=[g(X_1),g(X_2),\cdots,g(X_n)]^T\)</span>. We have the following decomposition <span class="math display">\[\begin{aligned}\hat{g}(x)-g(x)=&amp;e_1^T({\bf X^TWX})^{-1}{\bf X^TW}({\bf Y}-g({\bf X}))\\&amp;+e_1^T({\bf X^TWX})^{-1}{\bf X^TW}g({\bf X})-g(x).\end{aligned}\]</span> Taking expectations on both sizes, we have <span class="math display">\[\begin{aligned}\mathbb E[(\hat{g}(x)-g(x))^2|{\bf X}]=&amp;\mathbb E[(e_1^T({\bf X^TWX})^{-1}{\bf X^TW}({\bf Y}-g({\bf X})))^2|{\bf X}]\\&amp;+(e_1^T({\bf X^TWX})^{-1}{\bf X^TW}g({\bf X})-g(x))^2.\end{aligned}\]</span></p><p>The first term is variance and the second term is square of bias.</p><h2 id="variance">Variance</h2><p>Let <span class="math inline">\({\bf \Sigma}={\rm diag}\{\sigma^2(X_1),\cdots,\sigma^2(X_n)\}\)</span>. Variance can be simplified as <span class="math display">\[e_1^T({\bf X^TWX})^{-1}{\bf X^TW\Sigma WX}({\bf X^TWX})^{-1}e_1.\]</span> Denote <span class="math inline">\(\sum_{i=1}^n K_h(X_i-x)(X_i-x)^j\)</span> by <span class="math inline">\(S_j\)</span> for <span class="math inline">\(j=0,1,2\)</span>. Then we have <span class="math display">\[{\bf X^TWX}=\left[\begin{array}{cc}S_0&amp;S_1\\S_1&amp;S_2\end{array}\right].\]</span> Note that <span class="math display">\[\mathbb E[S_j/nh^j|{\bf X}]=\int K(u)u^jf(x+hu){\rm d}u=\mu_jf(x)+o(1),\]</span> where <span class="math inline">\(\mu_j=\int K(u)u^j{\rm d}u&lt;\infty\)</span>. Furthermore, <span class="math display">\[\begin{aligned}{\rm Var}[S_j/nh^j|{\bf X}]&amp;\le\frac{1}{n}\mathbb E[K_h(X_i-x)^2((X_i-x)/h)^{2j}|{\bf X}]\\&amp;\le \frac{1}{nh}\int K(u)^2u^{2j}f(x+hu){\rm d}u.\end{aligned}\]</span> Therefore, <span class="math inline">\(\rm Var\to 0\)</span> as <span class="math inline">\(nh\to \infty\)</span>. By Chebyshev's inequality, we can write <span class="math display">\[S_j/nh^j=\mu_jf(x)+o_{\mathbb P}(1),\]</span> where <span class="math inline">\(o_{\mathbb P}(1)\)</span> denotes an infinitesimal in probability. Let <span class="math inline">\({\bf H}={\rm diag}\{1,h\}\)</span>. Then <span class="math display">\[\frac{1}{n}{\bf H^{-1}X^TWXH^{-1}}=f(x)\left[\begin{array}{cc}1&amp;0\\0&amp;\mu_2\end{array}\right]+o_{\mathbb P}(1),\]</span> where we assume that <span class="math inline">\(\mu_j=0\)</span> for odd <span class="math inline">\(j\)</span>.</p><p>Let <span class="math inline">\(\nu_j=\int K(u)^2u^j{\rm d}u\)</span>. By a similar argument we have <span class="math display">\[\frac{1}{n}{\bf H^{-1}X^TW\Sigma WXH^{-1}}=f(x)\sigma^2(x)\left[\begin{array}{cc}\nu_0&amp;0\\0&amp;\nu_2\end{array}\right]+o_{\mathbb P}(1).\]</span> Now we can obtain that <span class="math display">\[\begin{aligned}&amp;e_1^T({\bf X^TWX})^{-1}{\bf X^TW\Sigma WX}({\bf X^TWX})^{-1}e_1\\=&amp;\frac{1}{nh}e_1^T{\bf H^{-1}}({\bf H^{-1}X^TWXH^{-1}}/n)^{-1}(\frac{h}{n}{\bf H^{-1}X^TW\Sigma WXH^{-1}})\\&amp;({\bf H^{-1}X^TWXH^{-1}}/n)^{-1}{\bf H^{-1}}e_1\\=&amp;\frac{1}{nh}\left(\left(e_1^T\left[\begin{array}{cc}1&amp;0\\0&amp;\mu_2\end{array}\right]^{-1}\left[\begin{array}{cc}\nu_1&amp;0\\0&amp;\nu_2\end{array}\right]\left[\begin{array}{cc}1&amp;0\\0&amp;\mu_2\end{array}\right]^{-1}e_1\right)\frac{\sigma^2(x)}{f(x)}+o_{\mathbb P}(1)\right)\\=&amp;\frac{1}{nh}(\frac{\nu_0\sigma^2(x)}{f(x)}+o_{\mathbb P}(1)).\end{aligned}\]</span></p><h2 id="bias">Bias</h2><p>Consider the Taylor expansion for <span class="math inline">\(g\)</span>, <span class="math display">\[g(X_i)=g(x)+g&#39;(x)(X_i-x)+\frac{1}{2}g&#39;&#39;(x)(X_i-x)^2+\frac{1}{6}g&#39;&#39;&#39;(\bar{X}_i)(X_i-x)^3.\]</span> Write <span class="math inline">\(r(X_i)=g(X_i)-g(x)-g&#39;(x)(X_i-x)\)</span>. Then we have <span class="math display">\[g({\bf X})=g(x){\bf X}e_1+g&#39;(x){\bf X}e_2+r({\bf X}).\]</span> Therefore, the bias term is <span class="math display">\[\begin{aligned}&amp;e_1^T({\bf X^TWX})^{-1}{\bf X^TW}g({\bf X})-g(x)\\=&amp;g(x)e_1^T({\bf X^TWX})^{-1}{\bf X^TWX}e_1+g&#39;(x)e_1^T({\bf X^TWX})^{-1}{\bf X^TWX}e_2\\&amp;+e_1^T({\bf X^TWX})^{-1}{\bf X^TW}r({\bf X})-g(x)\\=&amp;e_1^T({\bf X^TWX})^{-1}{\bf X^TW}r({\bf X}).\end{aligned}\]</span> For the second order term we have the following estimation <span class="math display">\[\frac{g&#39;&#39;(x)}{2nh^2}{\bf H^{-1}X^TW}[(X_1-x)^2,\cdots,(X_n-x)^2]^T=\left(\begin{array}{c}\frac{1}{2}\mu_2f(x)g&#39;&#39;(x)\\0\end{array}\right)+o_{\mathbb P}(1).\]</span> For the third order term, we assume that <span class="math inline">\(g&#39;&#39;&#39;(\bar{X}_i)\)</span> are bounded. The matrix norm is bounded by <span class="math display">\[\begin{aligned}&amp;\|\frac{1}{nh^2}{\bf H^{-1}X^TW}[(X_1-x)^3g(\bar{X}_1),\cdots,(X_n-x)^3g(\bar{X}_n)]^T\|\\\le &amp; C\max\{\frac{1}{nh^2}\sum_{i=1}^n K_h(X_i-x)|X_i-x|^3,\frac{1}{nh^2}S_4\}.\end{aligned}\]</span> By a similar argument as above, the bound is <span class="math inline">\(o_{\mathbb P}(1)\)</span> when <span class="math inline">\(n\to\infty\)</span>. Therefore, the bias is <span class="math display">\[\begin{aligned}&amp;e_1^T({\bf X^TWX})^{-1}{\bf X^TW}r({\bf X})\\=&amp;h^2e_1^T{\bf H^{-1}}(\frac{1}{n}{\bf H^{-1}X^TWXH^{-1}})^{-1}\frac{1}{nh^2}{\bf H^{-1}X^TW}r({\bf X})\\=&amp;\frac{h^2}{2}\left(g&#39;&#39;(x)e_1^T\left[\begin{array}{cc}1&amp;0\\0&amp;\mu_2\end{array}\right]^{-1}\left[\begin{array}{cc}\mu_2\\\mu_3\end{array}\right]+o_{\mathbb P}(1)\right)\\=&amp;\frac{h^2}{2}(g&#39;&#39;(x)\mu_2+o_{\mathbb P}(1))\end{aligned}\]</span> Note that if <span class="math inline">\(g\)</span> is a linear function then we will <strong>NOT</strong> have bias.</p><h2 id="derivative-estimation">Derivative Estimation</h2><p>Finally we consider the conditional bias and variance for <span class="math inline">\(\hat{\beta}\)</span>. The main difference is that <span class="math inline">\({\bf H}^{-1}e_2=e_2/h\)</span>. Therefore we will have order <span class="math inline">\(O(1/{nh^3})\)</span> in variance, and <span class="math inline">\(O(h)\)</span> in bias.</p><h2 id="references">References</h2><p>The whole procedure can be generalized to multivariate case, where <span class="math inline">\(X\)</span> is a vector and <span class="math inline">\(K(\cdot)\)</span> is a multivariate kernel. The details are displayed in <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1176325632" target="_blank" rel="noopener">D. Ruppert and M.P. Wand</a>'s paper.</p><p>Also in this paper, they considered locally polynomial regression for univariate case. Conditional bias and variance are deduced for derivatives in any order. They also explained why one can not drop <em>condition</em> in asymptotic analysis for locally linear regression.</p><p>A nice introduction to LLR (and other topics in nonparametric estimates) can be found in the <a href="https://ocw.mit.edu/courses/economics/14-385-nonlinear-econometric-analysis-fall-2007/lecture-notes/" target="_blank" rel="noopener">lecture notes</a>. This article mainly follows <a href="https://ocw.mit.edu/courses/economics/14-385-nonlinear-econometric-analysis-fall-2007/lecture-notes/local_lin_reg.pdf" target="_blank" rel="noopener">local linear regression</a>.</p>]]></content>
      
      
      <categories>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linear Regression </tag>
            
            <tag> Least Squares </tag>
            
            <tag> Nonparametric Estimates </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Concentration in Gauss Space</title>
      <link href="/2019/07/25/Concentration-in-Gauss-Space/"/>
      <url>/2019/07/25/Concentration-in-Gauss-Space/</url>
      
        <content type="html"><![CDATA[<p>This article aims to solve an exercise in <a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html#" target="_blank" rel="noopener">High-Dimensional Probability</a> (Page 112, Exercise 5.2.3).</p><h2 id="gaussian-isoperimetric-inequality">Gaussian Isoperimetric Inequality</h2><p>Let <span class="math inline">\(\mathbb R^n\)</span> equip with the <a href="https://en.wikipedia.org/wiki/Gaussian_measure" target="_blank" rel="noopener">Gauss measure</a> <span class="math inline">\(\gamma_n\)</span>: <span class="math display">\[\gamma_n(E)=\frac{1}{(2\pi)^{n/2}}\int_E\exp\{-\frac{\|x\|^2}{2}\}{\rm d}x\]</span> where <span class="math inline">\(E\subseteq \mathbb R^n\)</span> is a Borel set. Then <span class="math inline">\((\mathbb R^n,\gamma_n)\)</span> is called Gauss space. Gaussian isoperimetric inequality states that, among all the measurable sets with given volume, half spaces have the <strong>smallest perimeter</strong>. Moreover, define the Minkowski sum <span class="math display">\[A+B=\{a+b\in\mathbb R^n,a\in A,b\in B\}.\]</span> We have the following statement.</p><a id="more"></a><p><strong>Theorem.</strong> Let <span class="math inline">\(\epsilon&gt;0\)</span>. Among all sets <span class="math inline">\(A\subseteq \mathbb R^n\)</span> with fixed Gauss measure <span class="math inline">\(\gamma_n(A)\)</span>, the half spaces minimizes the Gauss measure <span class="math inline">\(\gamma_n(A+B^n(\epsilon))\)</span>, where <span class="math inline">\(B^n(\epsilon)\)</span> is a solid ball centered at origin with radius <span class="math inline">\(\epsilon\)</span>.</p><p>See <a href="https://link.springer.com/content/pdf/10.1007/BF01425510.pdf" target="_blank" rel="noopener">here</a> for a general definition of Gauss space and proof of the above theorem. See <a href="https://www.jstor.org/stable/29782707?seq=1#page_scan_tab_contents" target="_blank" rel="noopener">this paper</a> for a friendly introduction of Gaussian isoperimetric inequality.</p><h2 id="concentration">Concentration</h2><p>We want to prove the following concentration inequality in Gauss spaces.</p><p><strong>Theorem.</strong> Let <span class="math inline">\(X\sim\mathcal N(0,I_n)\)</span> be a Gauss variable. <span class="math inline">\(f:\mathbb R^n\to \mathbb R\)</span> is a Lipschitz function with respect to the standard Euclidean metric. Then <span class="math inline">\(f(X)-\mathbb Ef(X)\)</span> is a sub-gaussian variable. That is, <span class="math display">\[\mathbb P(|f(X)-\mathbb Ef(X)|&gt;t)\le \exp(-ct^2),\forall t&gt;0,\]</span> where <span class="math inline">\(c&gt;0\)</span> is a constant.</p><p>First we prove a blow-up lemma using isoperimetric inequality.</p><p><strong>Lemma.</strong> Let <span class="math inline">\(A\)</span> be a subset in Gauss space. If <span class="math inline">\(\gamma_n(A)&gt;1/2\)</span>, then for any <span class="math inline">\(t&gt;0\)</span>, <span class="math inline">\(\gamma_n(A+B^n(t))\ge1-\frac{1}{2}\exp\{-t^2/2\}\)</span>.</p><p><strong>Proof of Lemma.</strong> Let <span class="math inline">\(H=\{x\in\mathbb R^n:x_1\le 0\}\)</span> be the half space. By assumption, <span class="math inline">\(\gamma_n(A)&gt;\gamma_n(H)\)</span>. The Gaussian isoperimetric inequality implies <span class="math display">\[\gamma_n(A+B^n(t))&gt;\gamma_n(H+B^n(t)).\]</span> Note that <span class="math inline">\(H+B^n(t)=\{x\in\mathbb R_n:x_1\le t\}\)</span> is also a half space. Direct computation shows that <span class="math display">\[\begin{aligned}\gamma_n(H+B^n(t))&amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^t\exp\{-\frac{x_1^2}{2}\}{\rm d}x_1\\&amp;=1-\frac{1}{\sqrt{2\pi}}\int_t^{\infty}\exp\{-\frac{x_1^2}{2}\}{\rm d}x_1\\&amp;=1-\frac{1}{\sqrt{2\pi}}\int_0^{\infty}\exp\{-\frac{(t+y)^2}{2}\}{\rm d}y\\&amp;\ge 1-\frac{1}{\sqrt{2\pi}}\int_0^{\infty}\exp\{-\frac{t^2}{2}\}\exp\{-\frac{y^2}{2}\}{\rm d}y\\&amp;=1-\frac{1}{2}\exp\{-\frac{t^2}{2}\}.\end{aligned}\]</span> Therefore, we have <span class="math inline">\(\gamma_n(A+B^n(t))\ge1-\frac{1}{2}\exp\{-t^2/2\}\)</span>. <span class="math inline">\(\blacksquare\)</span></p><p>This blow-up lemma enables us to prove the main theorem.</p><p><strong>Proof of Main Theorem.</strong> Without loss of generality, we assume <span class="math inline">\(|f(x)-f(y)|\le \|x-y\|\)</span>. Let <span class="math inline">\(m\)</span> denote the median of <span class="math inline">\(f(X)\)</span>. That is, <span class="math display">\[\mathbb P(f(X)\le m)\ge 1/2,\mathbb P(f(X)\ge m)\ge 1/2.\]</span> Consider the level set <span class="math inline">\(A=f^{-1}((-\infty,m])\)</span>. Note that <span class="math inline">\(\mathbb P(f(X)\le m)=\gamma_n(A)\ge 1/2\)</span>. By blow-up lemma, <span class="math inline">\(\gamma_n(A+B^n(t))\ge 1-1/2\exp\{-t^2/2\}\)</span>. On the other hand, if $xA+B^n(t) $, there exists <span class="math inline">\(y\in A\)</span> such that <span class="math inline">\(\|x-y\|\le t\)</span>. Therefore, <span class="math display">\[f(x)\le \|x-y\|+f(y)=f(y)+t.\]</span> This implies <span class="math inline">\(\mathbb P(f(X)\le m+t)\ge \gamma_n(A+B^n(t))\)</span>, i.e. <span class="math inline">\(\mathbb P(f(X)-m&gt;t)\le 1/2\exp\{-t^2/2\}\)</span>.</p><p>Replace <span class="math inline">\(f\)</span> with <span class="math inline">\(-f\)</span>. We can obtain a similar inequality as <span class="math inline">\(\mathbb P(f(X)-m&lt;-t)\le 1/2\exp\{-t^2/2\}\)</span>. Hence, <span class="math display">\[\mathbb P(|f(X)-m|&gt;t)\le \exp\{-t^2/2\},\]</span> which implies <span class="math inline">\(f(X)-m\)</span> is a sub-gaussian variable. Note that <span class="math inline">\(\mathbb E(f(X)-m)=\mathbb E(f(X))-m\)</span>. We know that <span class="math inline">\(f(X)-\mathbb Ef(X)\)</span> is a sub-gaussian variable. <span class="math inline">\(\blacksquare\)</span></p><h2 id="applications">Applications</h2><p>Let us take <span class="math inline">\(f:\mathbb R^n\to\mathbb R\)</span> to be the norm <span class="math inline">\(\|\cdot\|\)</span>. Let <span class="math inline">\(X\sim\mathcal N(0,I_n)\)</span> be a Gauss variable. Direct computation shows that <span class="math display">\[\begin{aligned}\mathbb Ef(X)&amp;=\mathbb E\|X\|=\frac{1}{(2\pi)^{n/2}}\int \|x\|\exp\{-\frac{\|x\|^2}{2}\}{\rm d}x\\&amp;=\frac{1}{(2\pi)^{n/2}}\int_0^{\infty}\rho\exp\{-\rho^2/2\}{\rm d}\rho\int_{S^{n-1}(\rho)}{\rm d}\sigma\\&amp;=\frac{1}{(2\pi)^{n/2}}\times\frac{n\pi^{n/2}}{\Gamma(\frac{n}{2}+1)}\times 2^{(n-1)/2}\Gamma(\frac{n+1}{2})\\&amp;=\frac{n}{\sqrt{2}}\times\frac{\Gamma(\frac{n+1}{2})}{\Gamma(\frac{n+2}{2})}.\end{aligned}\]</span> By <a href="%5Bhttps://en.wikipedia.org/wiki/Stirling%27s_approximation%5D(https://en.wikipedia.org/wiki/Stirling&#39;s_approximation)">Stirling's formula</a>, the expectation is about <span class="math inline">\(\sqrt{n}\)</span> when <span class="math inline">\(n\)</span> is large. This shows that in high dimensions, the points concentrate in a sphere of radius <span class="math inline">\(\sqrt{n}\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> Probability </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Concentration Inequality </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bayesian Linear Regression</title>
      <link href="/2019/07/21/Bayesian-Linear-Regression/"/>
      <url>/2019/07/21/Bayesian-Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h2 id="the-linear-regression-model">The Linear Regression Model</h2><p>A <em>normal linear regression model</em> assumes an output variable <span class="math inline">\(Y\)</span>, a vector of covariates <span class="math inline">\({\bf x}=(x_1,\cdots ,x_p)\)</span>, and <span class="math inline">\(Y\)</span> is linear deterministic function <span class="math inline">\(\bf \beta^Tx\)</span> with additive Gaussian noise. <a id="more"></a>That is <span class="math display">\[Y={\bf \beta^Tx}+\epsilon, \epsilon\sim \mathcal N(0,\sigma^2).\]</span> Given <span class="math inline">\(n\)</span> i.i.d. samples <span class="math inline">\(({\bf x_1},y_1),\cdots, ({\bf x_n},y_n)\)</span>, the joint probability density of observed data <span class="math inline">\(y_1,\cdots, y_n\)</span> conditional upon <span class="math inline">\(\bf x_1,\cdots,x_n\)</span> and the value of <span class="math inline">\(\bf \beta\)</span> and <span class="math inline">\(\sigma^2\)</span> (or, the <strong>likelihood function</strong>) is <span class="math display">\[\begin{aligned}&amp;p(y_1,\cdots,y_n|{\bf x_1,\cdots,x_n},\beta,\sigma^2)=\prod_{i=1}^np(y_i|{\bf x_i},\beta,\sigma^2)\\&amp;=(2\pi\sigma^2)^{-n/2}\exp\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\beta^T{\bf x_i})^2\}.\end{aligned}\]</span> Maximizing the likelihood function is equivalent to minimizing the sum of squared residuals <span class="math inline">\(SSR(\beta)=\sum_{i=1}^n(y_i-\beta^T{\bf x_i})^2\)</span>. Let <span class="math inline">\(\bf X=[\bf x_1,\cdots,x_n]^T\)</span> and <span class="math inline">\({\bf y}=(y_1,\cdots, y_n)^T\)</span>. We have <span class="math display">\[SSR(\beta)=(\bf y-X\beta)^T(y-X\beta)=\|y-X\beta\|_2^2.\]</span> The minimizer, also called the ordinary least square (OLS) estimate, is given in the closed form (suppose <span class="math inline">\(\bf X^TX\)</span> is invertible) <span class="math display">\[\hat{\beta}_{ols}=\bf (X^TX)^{-1}X^Ty.\]</span> The closed form can be obtained using <a href="https://vdisk.weibo.com/s/uGmkTKh4CQ2uJ" target="_blank" rel="noopener">linear algebra</a>, which is often explained as the geometric meaning of least square.</p><h2 id="bayesian-linear-regression">Bayesian Linear Regression</h2><h3 id="a-semiconjugate-prior-for-beta">A Semiconjugate Prior for <span class="math inline">\(\beta\)</span></h3><p>Suppose <span class="math inline">\(\beta\sim\mathcal N(\beta_0,\Sigma_0)\)</span>. The posterior distribution for <span class="math inline">\(\beta\)</span> is <span class="math display">\[\begin{aligned}&amp;p(\beta|{\bf y,X,\sigma^2})\propto p({\bf y}|{\bf X,\beta,\sigma^2})p(\beta)\\\propto&amp; \exp\{\bf -\frac{1}{2}(-2\beta^TX^Ty/\sigma^2+\beta^TX^TX\beta/\sigma^2-2\beta^T\Sigma_0^{-1}\beta_0+\beta^T\Sigma_0^{-1}\beta)\}\\=&amp;\exp\{\bf \beta^T(\Sigma_0^{-1}\beta_0+X^Ty/\sigma^2)-\frac{1}{2}\beta^T(\Sigma_0^{-1}+X^TX/\sigma^2)\beta\}.\end{aligned}\]</span> Therefore, the posterior distribution for <span class="math inline">\(\beta\)</span> is Gaussian with <span class="math display">\[\begin{aligned}&amp;\mathbb E[{\bf \beta|y,X,\sigma^2}]=(\Sigma_0^{-1}+{\bf X^TX/\sigma^2})^{-1}(\Sigma_0^{-1}\beta_0+{\bf X^Ty/\sigma^2})\\&amp;\textrm{Var}[{\bf \beta|y,X,\sigma^2}]=(\Sigma_0^{-1}+{\bf X^TX/\sigma^2})^{-1}.\end{aligned}\]</span> Suppose <span class="math inline">\(\beta_0=0\)</span> and <span class="math inline">\(\Sigma_0=\alpha^{-1}{\bf I}\)</span>. The posterior density is simplified as <span class="math display">\[p(\beta|{\bf y,X,\sigma^2})\propto \exp\{-\frac{1}{2}(\beta^T({\bf \alpha I+X^TX/\sigma^2})\beta-2\beta^T{\bf X^Ty}/\sigma^2)\}.\]</span> Maximizing the posterior probability is equivalent to minimizing the error <span class="math display">\[\frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\beta^T{\bf x_i})^2+\alpha\beta^T\beta,\]</span> which leads to the regularized least square problem.</p><h3 id="a-semiconjugate-prior-for-sigma2">A Semiconjugate Prior for <span class="math inline">\(\sigma^2\)</span></h3><p>Let <span class="math inline">\(\gamma=\frac{1}{\sigma^2}\)</span> be the precision. Suppose <span class="math inline">\(\gamma\sim Gamma(\nu_0/2,\nu_o\sigma^2_0/2)\)</span>. The posterior probability for <span class="math inline">\(\gamma\)</span> is <span class="math display">\[\begin{aligned}&amp;p(\gamma|{\bf y,X,\beta})\propto p(\gamma)p({\bf y}|{\bf X,\beta,\gamma})\\\propto&amp;(\gamma^{\nu_0/2-1}\exp\{-\gamma\nu_0\sigma^0/2\})\times(\gamma^{n/2}\exp\{-\gamma SSR(\beta)/2\})\\=&amp;\gamma^{(\nu_0+n)/2-1}\exp\{-\gamma(\nu_0\sigma^2_0+SSR(\beta))/2\}.\end{aligned}\]</span> Hence, the posterior distribution for <span class="math inline">\(\gamma\)</span> is <span class="math inline">\(Gamma((\nu_0+n)/2,(\nu_0\sigma_0^2+SSR(\beta))/2)\)</span>.</p><h3 id="gibbs-sampler">Gibbs Sampler</h3><p>Given current values <span class="math inline">\(\{\beta^{(s)},(\sigma^2)^{(s)}\}\)</span>. We can sample a new value by</p><ol type="1"><li>Updating <span class="math inline">\(\beta\)</span>:<ol type="1"><li>Compute <span class="math inline">\(\mathbb E[\beta|{\bf y,X,(\sigma^2)^{(s)}}]\)</span> and <span class="math inline">\(\textrm{Var}[\beta|{\bf y,X,(\sigma^2)^{(s)}}]\)</span>;</li><li>Sample <span class="math inline">\(\beta^{(s+1)}\)</span> according to <span class="math inline">\(\mathcal N(\mathbb E,\textrm{Var})\)</span>.</li></ol></li><li>Updating <span class="math inline">\(\sigma^2\)</span>:<ol type="1"><li>Compute <span class="math inline">\(SSR(\beta^{(s+1)})\)</span>;</li><li>Sample <span class="math inline">\((\sigma^2)^{(s+1)}\)</span> according to inverse-gamma.</li></ol></li></ol><h3 id="zellners-g-prior">Zellner's g-Prior</h3><p>In the setting of prior distribution of <span class="math inline">\(\beta\)</span>, let <span class="math inline">\(\beta_0=0\)</span> and <span class="math inline">\(\Sigma_0=k({\bf X^TX})^{-1}\)</span>, where <span class="math inline">\(k=g\sigma^2\)</span>. We obtain what is called <a href="https://en.wikipedia.org/wiki/G-prior" target="_blank" rel="noopener">g-prior</a>. The mean and variance are simplified as <span class="math display">\[\begin{aligned}&amp;\mathbb E[\beta|{\bf y,X,\sigma^2}]=\frac{g}{g+1}\sigma^2{\bf \Phi}\\&amp;\textrm{Var}[\beta|{\bf y,X,\sigma^2}]=\frac{g}{g+1}{\bf \Phi X^Ty}.\end{aligned}\]</span> where <span class="math inline">\(\bf\Phi=(X^TX)^{-1}\)</span>. Suppose the prior distribution for <span class="math inline">\(\sigma^2\)</span> is inverse-gamma. We will show that <span class="math inline">\(p(\sigma^2|{\bf y,X})\)</span> is inverse-gamma. Compared with the above discussion, this simplifies our sampling procedure since the posterior does <strong>NOT</strong> depend on <span class="math inline">\(\beta\)</span>.</p><p>Note that <span class="math display">\[p({\bf y}|{\bf X,\sigma^2})=\int p({\bf y}|{\bf X,\sigma^2,\beta})p(\beta|{\bf X,\sigma^2}){\rm d}\beta.\]</span> Substituting the densities, we obtain <span class="math display">\[\begin{aligned}p({\bf y}|{\bf X,\sigma^2,\beta})p(\beta|{\bf X,\sigma^2})&amp;=C \exp\{-\frac{1}{2\sigma^2}({\bf y-X\beta})^T({\bf y-X\beta})-\frac{1}{2g\sigma^2}\beta^T{\bf \Phi^{-1}}\beta\}.\end{aligned}\]</span> where <span class="math display">\[C=(2\pi\sigma^2)^{-n/2}\times|2\pi g\sigma^2{\Phi}|^{-1/2}.\]</span> The terms in the exponent can be rewritten as <span class="math display">\[-\frac{1}{2\sigma^2}{\bf y^Ty}-\frac{1}{2}(\beta-m)^TV^{-1}(\beta-m)+\frac{1}{2}m^TV^{-1}m.\]</span> where <span class="math display">\[\begin{aligned}&amp;V=\frac{g}{g+1}\sigma^2{\bf \Phi}\\&amp;m=\frac{g}{g+1}{\bf \Phi X^Ty}.\end{aligned}\]</span> The integration is <span class="math display">\[p({\bf y}|{\bf X,\sigma^2})=(2\pi)^{-n/2}(1+g)^{-p/2}(\sigma^2)^{-n/2}\exp\{-\frac{1}{2\sigma^2}SSR_g\},\]</span> where <span class="math display">\[SSR_g=\bf y^T(I-{\it \frac{g}{g+1}}X^T\Phi X)y.\]</span> Let <span class="math inline">\(\gamma=1/\sigma^2\sim Gamma(\nu_0/2,\nu_0\sigma^2_0/2)\)</span>. Then we have <span class="math display">\[\begin{aligned}p(\gamma|{\bf X,y})&amp;\propto p(\gamma)p({\bf y}|{\bf X,\gamma})\\&amp;\propto \gamma^{(\nu_0+n)/2-1}\exp\{-\gamma(\nu_0\sigma^2_0+SSR_g)/2\}.\end{aligned}\]</span> Therefore, the posterior distribution for <span class="math inline">\(\sigma^2\)</span> is inverse-gamma with parameter <span class="math inline">\(((\nu_0+n)/2,(\nu_0\sigma^2_0+SSR_g)/2)\)</span>.</p><h3 id="monte-carlo-sampler">Monte Carlo Sampler</h3><p>Using g-prior, we see that the posterior distribution of <span class="math inline">\(\sigma^2\)</span> does not contain <span class="math inline">\(\beta\)</span>. Thus we can use a simple sampling procedure</p><ol type="1"><li>Sample <span class="math inline">\(\sigma^2\)</span> according to inverse-gamma;</li><li>Sample <span class="math inline">\(\beta\sim\mathcal N(\frac{g}{g+1}\hat{\beta}_{ols},\frac{g}{g+1}\sigma^2{\bf \Phi})\)</span>.</li></ol><h2 id="reference">Reference</h2><ol type="1"><li>Hoff, Peter D. <em>A first course in Bayesian statistical methods</em>. Vol. 580. New York: Springer, 2009.</li><li>Bishop, Christopher M. <em>Pattern recognition and machine learning</em>. springer, 2006.</li></ol>]]></content>
      
      
      <categories>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linear Regression </tag>
            
            <tag> Least Squares </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes on KNN Density Estimates (1)</title>
      <link href="/2019/07/18/Notes-on-KNN-Density-Estimates(1)/"/>
      <url>/2019/07/18/Notes-on-KNN-Density-Estimates(1)/</url>
      
        <content type="html"><![CDATA[<p>This series is a note of <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=530638" target="_blank" rel="noopener">Y. P. Mack and M. Rosenblatt</a>'s work in 1979.</p><h2 id="introduction">Introduction</h2><p>Suppose we have a <strong>bounded, twice differentiable density</strong> function <span class="math inline">\(f\)</span> on <span class="math inline">\(\mathbb R^p\)</span>. Let <span class="math inline">\(X_1,X_2,\cdots,X_n\)</span> be <span class="math inline">\(n\)</span> i.i.d. samples of <span class="math inline">\(f\)</span>. Let <span class="math inline">\(x\in\mathbb R^p\)</span> be a fixed point so that <span class="math inline">\(f(x)&gt;0\)</span>. We want to estimate <span class="math inline">\(f(x)\)</span> based on <span class="math inline">\(n\)</span> samples. The KNN estimate is given by <span class="math display">\[f_n(x)=\frac{1}{n(Z_n)^p}\sum_{j=1}^n\omega(\frac{x-X_j}{Z_n}),\]</span> where <span class="math inline">\(Z_n\)</span> is the distance between <span class="math inline">\(x\)</span> and its <span class="math inline">\(k\)</span>th nearest neighbor, and <span class="math inline">\(\omega\)</span> is a <strong>bounded integrable</strong> weight function with <span class="math inline">\(\int\omega(u){\rm d}u=1\)</span>.</p><a id="more"></a><p>Let <span class="math inline">\(\omega(u)=\frac{1}{|B_0(1)|}\chi_{B_0(1)}(u)\)</span>, where <span class="math inline">\(B_0(1)=\{x\in\mathbb R^p:\|x\|\le 1\}\)</span> is the solid ball centered at origin with radius 1, and <span class="math inline">\(|B_0(1)|=\frac{\pi^{p/2}}{\Gamma(\frac{p+2}{2})}\)</span> is the volume of unit ball. Under these settings we see that the KNN estimate is <span class="math display">\[f_n(x)=\frac{1}{n(Z_n)^p}\cdot\frac{k}{|B_0(1)|}=\frac{k}{n}\cdot\frac{1}{|B_0(Z_n)|}.\]</span> If we see <span class="math inline">\(\frac{k}{n}\)</span> as the probability that points lie in a ball around <span class="math inline">\(x\)</span>, then it is a naive analogy of <span class="math inline">\(\text{probability}=\int\text{density}\approx\text{density}\times \text{area}\)</span>. In the following we always assume that <span class="math inline">\(k=k(n)\)</span> is a function of sample size <span class="math inline">\(n\)</span> and <span class="math inline">\(k(n)\to \infty,k(n)/n\to0\)</span> as <span class="math inline">\(n\to\infty\)</span>.</p><p>We first investigate several properties of <span class="math inline">\(Z_n\)</span>. For simplicity, we call <span class="math inline">\(Z_n\)</span> the KNN distance variable.</p><h2 id="knn-distance-variable-z_n">KNN distance variable <span class="math inline">\(Z_n\)</span></h2><h3 id="cdf-and-pdf-of-z_n">CDF and PDF of <span class="math inline">\(Z_n\)</span></h3><p>For fixed <span class="math inline">\(x\in\mathbb R^p\)</span>, note that <span class="math inline">\(Z_n\)</span> is a function of <span class="math inline">\(n\)</span> i.i.d. samples. Let <span class="math inline">\(H(r)\)</span> be the cumulative distribution function (<strong>CDF</strong>) of <span class="math inline">\(Z_n\)</span>. Let <span class="math display">\[G(r)=\mathbb P(B_x(r))=\int_{B_x(r)}f(u){\rm d}u.\]</span> For any <span class="math inline">\(\epsilon&gt;0\)</span>, consider the probability <span class="math inline">\(H(r+\epsilon)-H(r)=\mathbb P(r\le Z_n\le r+\epsilon)\)</span>. Suppose <span class="math inline">\(\epsilon\)</span> is small enough so that there is <strong>exactly one</strong> point lying in the shell of thickness <span class="math inline">\(\epsilon\)</span> (It happens when multiple points lie in the shell, but they will not contribute to our deduction since higher order of <span class="math inline">\(\epsilon\)</span> vanishes when taking limit). There are <span class="math inline">\(n\)</span> choices of <span class="math inline">\(k\)</span>th nearest point, <span class="math inline">\(\binom{n-1}{k-1}\)</span> choices of <span class="math inline">\(k-1\)</span> points lying within the <span class="math inline">\(r\)</span> ball and others are outside the <span class="math inline">\(r+\epsilon\)</span> ball. Therefore, we have <span class="math display">\[H(r+\epsilon)-H(r)=n(G(r+\epsilon)-G(r))\binom{n-1}{k-1}G(r)^{k-1}(1-G(r+\epsilon))^{n-k}.\]</span> Let <span class="math inline">\(h(r)\)</span> be the probability density function (<strong>PDF</strong>) of <span class="math inline">\(Z_n\)</span>. By definition, we have <span class="math display">\[h(r)=\lim\limits_{\epsilon\to0}\frac{H(r+\epsilon)-H(r)}{\epsilon}=n\binom{n-1}{k-1}G(r)^{k-1}(1-G(r))^{n-k}G&#39;(r).\]</span> Furthermore, <span class="math inline">\(G&#39;(r)\)</span> can be expressed as an integration of <span class="math inline">\(f\)</span>. Note that <span class="math display">\[\begin{aligned}G&#39;(r)&amp;=\lim\limits_{\delta\to 0}\frac{1}{\delta}[\int_{B_x(r+\delta)}f(u){\rm d}u-\int_{B_x(r)}f(u){\rm d}u]\\&amp;=\lim_{\delta\to0}\frac{1}{\delta}\int_r^{r+\delta}\rho^{p-1}{\rm d}\rho\int_{S_x(1)}f(\rho,t){\rm d}\sigma(t)\\&amp;=\int_{S_x(r)}f(t){\rm d}\sigma(t),\end{aligned}\]</span> where <span class="math inline">\(S_x(r)=\{y\in\mathbb R^p:\|y-x\|=r\}\)</span> is the sphere centered at <span class="math inline">\(x\)</span> with radius <span class="math inline">\(r\)</span>, and <span class="math inline">\({\rm d}\sigma\)</span> is the volume element on the sphere.</p><p>On the other hand, for each <span class="math inline">\(X_i\)</span>, <span class="math inline">\(\|X_i-x\|\)</span> is a random variable with CDF <span class="math inline">\(G(r)\)</span> and PDF <span class="math inline">\(G&#39;(r)\)</span>. Note that <span class="math inline">\(Z_n\)</span> is exactly the <strong><span class="math inline">\(k\)</span>th order statistic</strong> from i.i.d. samples <span class="math inline">\(\|X_i-x\|\)</span>. The <span class="math inline">\(k\)</span>th order statistic gives the same <span class="math inline">\(h(r)\)</span> as we deduced above.</p><h3 id="moments-of-z_n">Moments of <span class="math inline">\(Z_n\)</span></h3><p>In general, for a measurable function <span class="math inline">\(\phi\)</span>, we want to compute the expectation <span class="math inline">\(\mathbb{E}\phi(Z_n)\)</span>. By definition, <span class="math display">\[\begin{aligned}\mathbb E\phi(Z_n)&amp;=\int_0^{\infty}\phi(r)h(r){\rm d}r\\&amp;=n\binom{n-1}{k-1}\int_0^{\infty}\phi(r)G(r)^{k-1}(1-G(r))^{n-k}{\rm d}G(r).\end{aligned}\]</span> In our situation, we consider a specific function of the type <span class="math display">\[\phi(r)=\frac{r^{\lambda}}{G(r)^\gamma(1-G(r))^\beta},\]</span> where <span class="math inline">\(\lambda,\gamma,\beta\)</span> are nonnegative integers. Note that <span class="math inline">\(G(r)\in[0,1]\)</span>. To ensure that the integration exists, we assume that <span class="math inline">\(1-G(r)=O(r^{-\xi})\)</span>, where <span class="math inline">\(\xi&gt;0\)</span>, as <span class="math inline">\(r\to \infty\)</span>.</p><p>Since <span class="math inline">\(G(r)\)</span> is a monotonically increasing function, we can solve <span class="math inline">\(r=G^{-1}(t)\)</span> where <span class="math inline">\(t\in[0,1]\)</span>. On the one hand, <span class="math display">\[\begin{aligned}G(r)&amp;=\int_{B_x(r)}f(u){\rm d}u\\&amp;=f(x)|B_x(r)|+\int_{B_x(r)}f(u)-f(x){\rm d}u,\\\end{aligned}\]</span> where <span class="math display">\[\int_{B_x(r)}|f(u)-f(x|{\rm d}u\le K(r)\int_{B_x(r)}\|u-x\|{\rm d  }u=\frac{K(r)}{n+1}r^{p+1}|S_x(1)|.\]</span> Therefore, <span class="math inline">\(t=G(r)=cf(x)r^p+o(r^p)\)</span> as <span class="math inline">\(r\to 0\)</span>, where <span class="math inline">\(c=|B_x(1)|\)</span>. From the assumption <span class="math inline">\(1-G(r)=O(r^{-\xi})\)</span>, we see that <span class="math inline">\(t=1-O(r^{-\xi})\)</span> as <span class="math inline">\(r\to\infty\)</span>. We can write <span class="math inline">\(r\)</span> as <span class="math display">\[r=G^{-1}(t)=(\frac{t}{cf(x)})^{1/p}+\eta(t),\]</span> where <span class="math inline">\(\eta(t)=o(t^{1/p})\)</span> as <span class="math inline">\(t\to 0\)</span> and <span class="math inline">\(\eta(t)=O(\frac{1}{(1-t)^\xi})\)</span> as <span class="math inline">\(t\to 1\)</span>.</p>Using the change of variable <span class="math inline">\(t=G(r)\)</span>, we see that $$<span class="math display">\[\begin{aligned}\mathbb E\phi(Z_n)&amp;=n\binom{n-1}{k-1}\int_0^1(\frac{t}{cf(x)}+\eta(t))^\lambda t^{k-1-\gamma}(1-t)^{n-k-\beta}{\rm d}t\\\end{aligned}\]</span><p>$$</p><hr><p><strong>Personal Comments</strong></p><p>I was intended to study <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=530638" target="_blank" rel="noopener">Y. P. Mack and M. Rosenblatt</a>'s paper thoroughly to understand the asymptotic behaviors of kNN estimator. Both results and techniques in proofs are important for me. However, I was very upset to find many subtle details I could not get through. The biggest problem appears in equation (12)</p><blockquote><p><span class="math display">\[\mathbb E \phi(R_n)=n\binom{n-1}{k-1}\int_0^1((\frac{t}{cf(x)})^\lambda+o(t^\lambda))t^{k-1-\gamma}(1-t)^{n-k-\beta}{\rm d}t\]</span></p></blockquote><p>The notation <span class="math inline">\(o(t)\)</span> is misleading. It is correct when <span class="math inline">\(t\)</span> is small. However, when <span class="math inline">\(t\)</span> is close to 1, this term tends to infinity. Therefore, the estimation for this expectation is not easy. One cannot simply drop the <span class="math inline">\(o(t)\)</span> integration. But according to the paper (though they did not display the computation), <span class="math inline">\(o(t)\)</span> was simply dropped.</p><p>It was also disappointing that I found there were few papers concerning the asymptotic behaviors about kNN. One could find that the asymptotic moments for kNN distance variable are well-known. But the references were not mentioned.</p><p>There is another paper discussing the asymptotic moments of kNN distance. But it restricts to compact convex area. See <a href="https://royalsocietypublishing.org/doi/abs/10.1098/rspa.2002.1011" target="_blank" rel="noopener">Asymptotic moments of nearneighbour distance distributions</a>.</p><p>I will keep on tracking about this topic.</p>]]></content>
      
      
      <categories>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Asymptotic Analysis </tag>
            
            <tag> Nonparametric Estimates </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Proof of &#39;Everything is connected&#39;</title>
      <link href="/2019/07/14/a%20proof%20of%20everything%20is%20connected/"/>
      <url>/2019/07/14/a%20proof%20of%20everything%20is%20connected/</url>
      
        <content type="html"><![CDATA[<h2 id="stories">Stories</h2><p>The idea about building my own blog was originated from a long time ago when I first read <a href="https://terrytao.wordpress.com/" target="_blank" rel="noopener">Terry Tao's homepage</a>. In 2019, when I became a graduate student, I realized it is necessary to write down the mathematics to keep track of my research. Honestly speaking, repetitions and failures really make one suffer. It is those tiny progresses that encourage me to hold on.</p><a id="more"></a><p>The title of this blog comes from an <a href="https://mp.weixin.qq.com/s?__biz=MjM5NDEzNjI0Mg==&amp;mid=2698897605&amp;idx=1&amp;sn=a19cb1dd989bbb6740345ceb476b01a7&amp;chksm=83a52118b4d2a80e6d4254aa2dac5442302180583a257c251e051a33f18b3a4a7a993d68cd36&amp;mpshare=1&amp;scene=1&amp;srcid=0716ytVjaAJNShmEwGSlmWpF&amp;key=dd5051400a9fb58f808d2da53b802b8acf4d01cc85436ece84b72433ba3fd12b83709bd533967b30d98fcbb2cafbb52010be54affd44f1114534efd269163755ef08943a54bd6644af14d7e46d5cd77b&amp;ascene=1&amp;uin=MTU3ODQ1MjIwNg%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=hrTohvqyTgd41p4viiU31IowGnkdWinCUMu%2FwVIpE0dlQBF2lbrj1dBRey65qCVn" target="_blank" rel="noopener">essay</a> which ingeniously describes a philosophy, that every thing, every event, every people are connected with each other. Like <a href="https://en.wikipedia.org/wiki/Leonardo_da_Vinci" target="_blank" rel="noopener">da Vinci</a> said, <em>Learn how to see. Realize that everything connects to everything else</em>. When I read these, a great idea hit me that I could formulate a mathematical proposition and prove it. Though the proof seems naive, I was happy to find such an explanation.</p><h2 id="modeling">Modeling</h2><p>Let us recall some basics from <a href="https://en.wikipedia.org/wiki/Graphical_model" target="_blank" rel="noopener">probabilistic graphical model</a>. We use random variables to represent true events in real life. The relationship between events is modeled by <strong>conditional independence</strong>. Suppose <span class="math inline">\(X,Y\)</span> and <span class="math inline">\(Z\)</span> are three random variables. If the joint probability density function of <span class="math inline">\(X,Y\)</span> and <span class="math inline">\(Z\)</span> satisfy the following equation</p><p><span class="math display">\[f(x,y|z)=f(x|z)f(y|z),\]</span></p><p><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be conditional independent given <span class="math inline">\(Z\)</span>. It generalizes the concept of <strong>independence</strong> where we known nothing prior (<span class="math inline">\(Z=\emptyset\)</span>). In our case, it is reasonable that two events have no relationship if they are independent when we know all the other things.</p><p>Now we can construct a graph based on the above argument. The vertices are random variables. Two distinct vertices are connected by an edge if they are not conditional independent. One event is 'connected' to another if there is a path joining them. Therefore, when we say 'Everything is connected', we actually mean that the graph is connected.</p><p>However, we should be cautious about 'everything' since we just considered finite number of events. Given arbitrary <span class="math inline">\(n\)</span> events, we can say 'These <span class="math inline">\(n\)</span> things are connected' if the corresponding graph is connected. But it is too stupid to say such things, and we are not going to infer whether a graph is connected, either. Instead, let us look at the probability that a graph with <span class="math inline">\(n\)</span> nodes is connected.</p><h2 id="proof">Proof</h2><p>Let <span class="math inline">\(\mathbb{G}_n\)</span> be the space of undirected simple graphs with <span class="math inline">\(n\)</span> nodes. <span class="math inline">\(\mathbb{G}_n\)</span> is a finite set with <span class="math inline">\(2^\frac{n(n-1)}{2}\)</span> elements. Let <span class="math inline">\(\mathbb{G}_n^c\)</span> be the subset of all connected graphs, <span class="math inline">\(\mathbb G_n^d\)</span> be the subset of all disconnected graphs. Define the connectivity ratio</p><p><span class="math display">\[R_n=\frac{ \#(\mathbb{G}_n^d)}{ \#(\mathbb{G}_n)}:=\frac{K_n^d}{K_n^c+K_n^d}.\]</span></p><p>where we have <span class="math inline">\(K_n^c+K_n^d=2^\frac{n(n-1)}{2}\)</span>.</p><p>Let <span class="math inline">\(v_0,v_1,\cdots,v_n\)</span> be vertices of a graph <span class="math inline">\(G\in\mathbb{G}_{n+1}^d\)</span>. Suppose <span class="math inline">\(C_0\)</span> is the component containing <span class="math inline">\(v_0\)</span> with <span class="math inline">\(i\)</span> points. Let <span class="math inline">\(C_1\)</span> be the complement of <span class="math inline">\(C_0\)</span> with <span class="math inline">\(n+1-i\)</span> points. The we have the following recursion</p><p><span class="math display">\[K_{n+1}^d=\sum_{i=1}^n\binom{n}{i-1}K_i^c2^\frac{(n+1-i)(n-i)}{2}.\]</span></p><p>Dividing both sides by <span class="math inline">\(2^\frac{(n+1)n}{2}\)</span>, we have</p><p><span class="math display">\[R_{n+1}=\sum_{i=1}^n\binom{n}{i-1}(1-R_i)2^\frac{-i(n+1-i)}{2}.\]</span></p><p>Note that <span class="math inline">\(1-R_i&lt;1\)</span> when <span class="math inline">\(i&gt;1\)</span>. Multiply both sides by <span class="math inline">\(2^n\)</span>,</p><p><span class="math display">\[2^nR_{n+1}&lt;1+n+\sum_{i=2}^{n-1}\binom{n}{i-1}2^{-(n-i)(i-1)}.\]</span></p><p>When <span class="math inline">\(1&lt;i&lt;n\)</span>, <span class="math inline">\(2^{-(n-i)(i-1)}&lt;2^{-(n-2)}\)</span>. We obtain that <span class="math inline">\(2^nR_{n+1}&lt;n+5\)</span>. That is, the connectivity ratio <span class="math inline">\(R_n&lt;\frac{n+4}{2^{n-1}}\)</span>. When <span class="math inline">\(n\)</span> tends to infinity, <span class="math inline">\(R_n\)</span> tends to <span class="math inline">\(0\)</span>.</p><p>Given arbitrarily <span class="math inline">\(n\)</span> events, we are right to guess that they are connected with probability greater than <span class="math inline">\(1-\frac{n+4}{2^{n-1}}\)</span>. The probability grows with respect to the number of events. When <span class="math inline">\(n\)</span> goes to infinity, we are 100% certain that 'Everything is connected'!</p><h2 id="section"></h2>]]></content>
      
      
      <categories>
          
          <category> About-My-Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About My Blog </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
