<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>List of Convergence Rates in TDA</title>
      <link href="/2021/10/16/List-of-Convergence-Rates-in-TDA/"/>
      <url>/2021/10/16/List-of-Convergence-Rates-in-TDA/</url>
      
        <content type="html"><![CDATA[<h2 id="Frechet-means-for-distributions-of-persistence-diagrams"><a href="#Frechet-means-for-distributions-of-persistence-diagrams" class="headerlink" title="Frechet means for distributions of persistence diagrams"></a>Frechet means for distributions of persistence diagrams</h2><p>Consider the measure $\rho = 1/m \sum<em>{i=1}^m \delta</em>{Z_i}$ on the space of persistence diagrams. The population Frechet mean is defined by </p><script type="math/tex; mode=display">Y = \arg\min_Z \int_{\mathcal{D}} d(X,Z)^2 {\rm d}\rho(X)</script><p>Let $X<em>1,\cdots, X_n$ be iid samples from $\rho$. Let $\rho_n = 1/n\sum</em>{i=1}^n \delta_{X_i}$. The empirical Frechet mean is defined by</p><script type="math/tex; mode=display">Y_n = \arg\min_Z \int_{\mathcal{D}} d(X,Z)^2 {\rm d}\rho_n(X)</script><blockquote><p>There exists a $Y_n$ such that with probability greater than $1-\delta$</p><script type="math/tex; mode=display">d(Y,Y_n)^2\le \frac{m^2F(Y)}{n}\log(\frac{m}{\delta})</script><p>for $n\ge 8m\log(m/\delta)$ and the right hand side is less than $r^2$ where $r$ characterizes the separation between the local minima of $F$.</p></blockquote><h2 id="Convergence-rates-for-persistence-diagrams-estimation-in-topological-data-analysis"><a href="#Convergence-rates-for-persistence-diagrams-estimation-in-topological-data-analysis" class="headerlink" title="Convergence rates for persistence diagrams estimation in topological data analysis"></a>Convergence rates for persistence diagrams estimation in topological data analysis</h2><p>Let $(M,\rho)$ be a metric space and $\mu$ is a measure on it with support $M<em>\mu$. Further, assume that $\mu$ satisfies the $(a,b)$-standard assumption: for any $x$ in $M</em>\mu$ and $r&gt;0$, $\mu(B(x,r))\ge \min(ar^b,1)$ (usually $b=d$ if in $\mathbb{R}^d$). Let $\mathbb{X}={X_1,\cdots,X_n}$ be iid samples from $\mu$. Then we have </p><blockquote><p>$\mathbb{E}[d<em>\infty(\text{Dgm}(M</em>\mu),\text{Dgm}(\mathbb{X}))]\le C(\frac{\log n}{n})^{1/b}$<br>where $C$ is a constant depending on $a$ and $b$. </p></blockquote><p>Essentially the result comes from the stability theorem of persistence diagrams and convergence for random sets with respect to Hausdorff distance. In fact, we have </p><blockquote><p>for any $\epsilon&gt;0$, $\mathbb{P}[d<em>H(M</em>\mu,\mathbb{X})]\le \frac{2^b}{a\epsilon^b}\exp(-na\epsilon^b)\wedge 1$</p></blockquote><h2 id="Subsampling-methods-for-persistent-homology"><a href="#Subsampling-methods-for-persistent-homology" class="headerlink" title="Subsampling methods for persistent homology"></a>Subsampling methods for persistent homology</h2><p>Let $(M,\rho,\mu)$ be the metric measure space satisfying the $(a,b)$-standard assumption. Let $S_1,\cdots,S_n$ be $n$ iid samples of size $m$ from $\mu$. Define the empirical average landscape by </p><script type="math/tex; mode=display">(\hat\lambda) = \frac{1}{n}\sum_{i=1}^n\lambda_{S_i}</script><p>The empirical average landscape is supposed to converge to the population average landscape which is </p><script type="math/tex; mode=display">\mathbb{E}_{(\mu^{\otimes m})_*}[\lambda]</script><p>The bias part is controlled by the following</p><blockquote><p>Let $r<em>m = 2(\frac{\log m}{am})^{1/b}$. If $\mu$ satisfies the $(a,b,r_0)$ assumption, then<br>$|\lambda</em>{M<em>\mu}-\mathbb{E}[\lambda]|</em>\infty\le r<em>0+r_m 1</em>{(r_0,\infty)}(r_m)+C(a,b)\frac{r_m}{(\log m)^2}$</p></blockquote><p>The variance part is controlled by </p><blockquote><p>$\mathbb{E}|\hat{\lambda}-\mathbb{E}[\lambda]|_\infty\le O(1/\sqrt{n})$</p></blockquote><h2 id="Estimation-and-quantization-of-expected-persistence-diagrams"><a href="#Estimation-and-quantization-of-expected-persistence-diagrams" class="headerlink" title="Estimation and quantization of expected persistence diagrams"></a>Estimation and quantization of expected persistence diagrams</h2><p>Let $\Omega={(x,y)|y&gt;x}$ be the open half-plane, and $\partial\Omega={(x,x)|x\in\mathbb{R}}$ be the boundary of $\Omega$. Let $\mathcal{M}^p$ be the space of Radon measures supported on $\Omega$ that have finite total $p$-persistence, i.e. $\int |x-\partial\Omega|^p {\rm d}\mu(x)&lt;\infty$. Define the distance between two measures $\mu$ and $\nu$ by </p><script type="math/tex; mode=display">\textrm{OT}_p = \inf_{\pi}\left(\int_{\bar{\Omega}\times\bar{\Omega}}\|x-y\|^p {\rm d}\pi(x,y)\right)</script><p>where $\pi$ ranges all measures supported on $\bar{\Omega}\times\bar{\Omega}$ whose first marginal coincides with $\mu$ and second marginal coincides with $\nu$. Let $P$ be a probability distribution supported on $(\mathcal{M}^p,\textrm{OT}_p)$. Let $\mathbf{E}(P)$ be the measure defined by, for $A\subset \Omega$ compact, </p><script type="math/tex; mode=display">\mathbb{E}(P)[A] = \mathbb{E}_P[\mu(A)]</script><p>$\mathbf{E}(P)$ is called the expected persistence diagram. If $P$ is a distribution supported on the space of persistence diagrams, it is proved under mild assumptions, $\mathbb{E}(P)$ admits a density with respect to the Lebesgue measure on $\Omega$.</p><p>Given iid samples $\mu_1,\cdots,\mu_n\sim P$, the empirical expected persistence diagram is defined by $\bar{\mu}_n=\frac{1}{n}\sum \mu_i$. We have $\bar{\mu}_n\rightarrow\mathbf{E}(P)$ under $\textrm{OT}_p$ almost surely. Specifically,</p><script type="math/tex; mode=display">\mathbb{E}[\textrm{OT}_p^p(\bar{\mu}_n, \mathbf{E}(P))] = O(\frac{1}{n^{1/2}}+\frac{a_p(n)}{n^{p-q}})</script><p>where $1\le p&lt;\infty$, $0\le q&lt; p$, and $P$ is a distribution with ‘some’ restrictions. </p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Error Estimate under Hausdorff Distance</title>
      <link href="/2021/07/22/Error-Estimate-under-Hausdorff-Distance/"/>
      <url>/2021/07/22/Error-Estimate-under-Hausdorff-Distance/</url>
      
        <content type="html"><![CDATA[<p>This is a useful technique I learned from <a href="https://jmlr.csail.mit.edu/papers/volume16/chazal15a/chazal15a.pdf" target="_blank" rel="noopener">Convergence rates for persistence diagram estimation in topological data analysis</a> when one wants to prove convergence results for some estimator under Hausdorff distance. Typical settings are </p><ul><li>A metric space $(X,\rho)$;</li><li>A probability measure $\mu$ with support $X_\mu$;</li><li>i.i.d samples $\hat{X}={X_1,\cdots,X_n}$ from $\mu$;</li></ul><p>the final essential condition is the $(a,b,r_0)$-standard assumption:</p><ul><li>when $r&gt;r_0$, the ball with radius $r$ has volume $\mu(B(r))&gt; ar^b\wedge 1$.</li></ul><h2 id="Bound-on-Covering-Number"><a href="#Bound-on-Covering-Number" class="headerlink" title="Bound on Covering Number"></a>Bound on Covering Number</h2><p>The covering number $Cv(A,r)$ for a metric space $A$ is the minimum number of balls of radius $r$ that cover $A$.</p><p>The packing number $Pk(A,r)$ for a metric space $A$ is the maximum number of balls of radius $r$ such that they are mutually disjoint.</p><p>We have the following relation</p><blockquote><p>$Pk(A,r)\le Cv(A,r)\le Pk(A,r/2)$</p></blockquote><p><strong>proof.</strong> Set $m=Pk(A,r)$. There exists $m$ points from $A$ such that the mutual distance is greater than $2r$. If $Cv(A,r)&lt;m$, then by pigeonhole principle there are at least two points lying in the same ball of radius $2r$ which is a contradiction.</p><p>Similarly, let $m’=Pk(A,r/2)$, and let $x<em>1,\cdots, x</em>{m’}$ be points realizing the packing number. By definition, for any other point $x$ there exists at least one $x_i$ such that $\rho(x,x_i)\le r$. Therefore, $\cup B(x_i,r)=A$.$\blacksquare$</p><p>The $(a,b,r_0)$-standard assumption helps us to bound the packing number and covering number.</p><blockquote><p>when $r&gt;r_0$, the packing number and covering number are such that<br>$Pk(A,r)\le \frac{1}{ar^b}\vee 1$ and $Cv(A,r)\le \frac{2^b}{ar^b}\vee 1$.</p></blockquote><p><strong>proof.</strong> It suffices to prove for $r&lt;a^{-1/b}$. Let $m=Pk(A,r)$. Choose a packing $B_1,\cdots,B_m$. Since they are disjoint, by assumption we have</p><script type="math/tex; mode=display">\sum_{i=1}^m\mu(B_i)\le 1\implies m\le \frac{1}{ar^b}\vee 1</script><p>and for covering numbers we have</p><script type="math/tex; mode=display">Cv(A,r)\le Pk(A,r/2)\le \frac{2^b}{ar^b}\vee 1\quad\blacksquare</script><h2 id="Bound-on-Hausdorff-distance"><a href="#Bound-on-Hausdorff-distance" class="headerlink" title="Bound on Hausdorff distance"></a>Bound on Hausdorff distance</h2><p>When the sample size $n$ tends to infinity, our intuition tells us that the Hausdorff distance between samples and $X<em>\mu$ should converge to zeros. The idea of our proof is to reduce the distance between $X</em>\mu$ and $\hat{X}$ to the distance between covering set and $\hat{X}$, where in the latter case we are dealing with a fixed finite set. More precisely, let $\mathcal{C}={c<em>1,\cdots,c_p}$ be the set of points realizing the covering number $Cv(X</em>\mu,r/2)$. Then</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{P}(d_H(X,X_\mu)>r)\le & \mathbb{P}(d_H(X,\mathcal{C})+d_H(\mathcal{C},\hat{X})>r)\\\le & \mathbb{P}(d_H(\mathcal{C},\hat{X})>r/2)\\\le & \mathbb{P}(\exists i\in\{1,\cdots,p\} \text{ such that } \hat{X}\cap B(c_i)=\emptyset)\\\le & \sum_{i=1}^p \mathbb{P}(\hat{X}\cap B(c_i)=\emptyset)\\\le & \frac{4^b}{ar^b}[1-\frac{ar^b}{2^b}]^n\\\le & \frac{4^b}{ar^b}\exp(-n\frac{a}{2^b}r^b)\end{aligned}</script><p>The first inequality comes from the triangle inequality. The second line comes from the fact that any point in $X_\mu$ is in the ball of radius $r/2$ centered at covering set. The third line comes from the definition of Hausdorff distance. Note that for any sample point the distance to $\mathcal{C}$ is less than $r/2$. However, for a covering point the distance to $\hat{X}$ can be larger than $r/2$. The fourth line is a relaxation of the third line. The fifth line substitutes the bound on covering number and standard assumption.</p><p>Then we have </p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[d_H(\hat{X},X_\mu)]=&{}\int_{r>0}\mathbb{P}(d_H(\hat{X},X_\mu))\ {\rm d}r\\\le&{} r_0 + \int_{r>r_0}\mathbb{P}(d_H(\hat{X},X_\mu))\ {\rm d}r\end{aligned}</script><p>Let $r_n=2(\frac{\log n}{an})^{1/b}$ (the choice of $r_n$ depends on the term inside the exponential). Then </p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[d_H(\hat{X},X_\mu)]= r_0+r_n1_{(r_0,\infty)}+\int_{r>r_n}\mathbb{P}(d_H(\hat{X},X_\mu))\ {\rm d}r\end{aligned}</script><p>It suffices to bound the integral. Use the transformation of variable $t=nar^b/2^b$. The integral becomes</p><script type="math/tex; mode=display">\begin{aligned}   \int_{r>r_n}\frac{4^b}{ar^b}\exp(-n\frac{a}{2^b}r^b)\,{\rm d}r &\le \int_{r>\log n} \frac{2^bn}{t}\exp(-t)\, {\rm d}(\frac{2t^{1/b}}{(na)^{1/b}})\\   & = \frac{2^{b+1}n^{1-1/b}}{a^{1/b}b}\int_{r>\log n}t^{1/b-2}\exp(-t)\,{\rm d}t\\    & =  \frac{2^{b+1}n^{1-1/b}}{a^{1/b}b}(\log n)^{1/b-2}\exp(-t)|^{\log n}_\infty\\    & = \frac{2^{b+1}}{a^{1/b}b}(\frac{\log n}{an})^{1/b}\frac{1}{(\log n)^2}\end{aligned}</script><p>If $r_0=0$, then we see that </p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[d_H(\hat{X},X_\mu)]=O((\log n/n)^{1/b}) \end{aligned}</script><p>Moreover, we see that </p><script type="math/tex; mode=display">\begin{aligned}   \mathbb{P}(d_H(\hat{X},X_\mu)\le r_n) = 1-\mathbb{P}(d_H(\hat{X},X_\mu)>r_n)\ge 1-O((\log n)^{1/b-2}/n^{1/b}) \end{aligned}</script><p>thus with high probability, </p><script type="math/tex; mode=display">d_H(\hat{X},X_\mu)\le r_n</script><p>or </p><script type="math/tex; mode=display">\lim_{n\to \infty}\mathbb{P}(d_H(\hat{X},X_\mu)\le r_n)=1</script><h2 id="Bound-on-Bottleneck-Distance"><a href="#Bound-on-Bottleneck-Distance" class="headerlink" title="Bound on Bottleneck Distance"></a>Bound on Bottleneck Distance</h2><p>As an application we can prove the convergence rate for persistence diagrams. Suppose we are sampling points from a measure $\mu$ with support $X<em>\mu$. The support $X</em>\mu$ has a true persistence diagram $D(X<em>\mu)$. From the sampling points we have an empirical persistence diagram $D(\hat{X})$. We ask if $D(\hat{X})$ is a good estimator of $D(X</em>\mu)$. By the <a href="http://yueqicao.top/2019/10/02/Stability-Theorems-in-Topological-Data-Analysis-3/" target="_blank" rel="noopener">stability theorem</a>, </p><script type="math/tex; mode=display">d_B(D(X_\mu),D(\hat{X}))\le d_H(X_\mu,\hat{X})</script><p>therefore we find that with high probability,</p><script type="math/tex; mode=display">d_B(D(X_\mu),D(\hat{X}))\le r_n</script><p>Thus we can approximate the true persistence diagram by sampling. As the sample size tends to infinity, the empirical persistence diagram will approximate the true diagram with rate at least $O(r_n)$. However, since the computational cost for persistence homology is not economic, in practice we cannot have persistence diagram from large samples. In this case, we need bootstrap method to first approximate the persistence diagram for the large sample. See <a href="http://proceedings.mlr.press/v37/chazal15.pdf" target="_blank" rel="noopener">Subsampling methods fro persistent homology</a>.</p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tex2svg</title>
      <link href="/2021/05/31/tex2svg/"/>
      <url>/2021/05/31/tex2svg/</url>
      
        <content type="html"><![CDATA[<p>When I want to save a bunch of tex equations/symbols as svg files I find there is no efficient software to fulfill this goal. But I find the interesting <a href="https://xiaoxiang.io/5aab4864f26b75000111c679/" target="_blank" rel="noopener">pipeline</a> which is implemented on a Linux system.   The following is what I did on my CentOS7 server.</p><h3 id="Install-Latex"><a href="#Install-Latex" class="headerlink" title="Install Latex"></a>Install Latex</h3><p>Login in as <strong>root</strong>. Then install texlive using the following two commands</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install texlive-latex</span><br><span class="line">yum install texmaker</span><br></pre></td></tr></table></figure><p>Check your pdflatex version using </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pdflatex --version</span><br></pre></td></tr></table></figure><h3 id="Install-pdfcrop"><a href="#Install-pdfcrop" class="headerlink" title="Install pdfcrop"></a>Install pdfcrop</h3><p><a href="https://github.com/ho-tex/pdfcrop" target="_blank" rel="noopener">pdfcrop</a> is written by perl. First install perl on CentOS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install perl</span><br></pre></td></tr></table></figure><p>Check the perl version using</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perl -v</span><br></pre></td></tr></table></figure><p>Then download or clone the pdfcrop repository</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ho-tex/pdfcrop.git</span><br></pre></td></tr></table></figure><p>cd to the directory where you can see the perl file. Run with a test pdf using </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perl pdfcrop.pl test.pdf</span><br></pre></td></tr></table></figure><p>The output is named as </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test-crop.pdf</span><br></pre></td></tr></table></figure><h3 id="Install-poppler"><a href="#Install-poppler" class="headerlink" title="Install poppler"></a>Install poppler</h3><p>The original pipeline uses <a href="https://github.com/dawbarton/pdf2svg" target="_blank" rel="noopener">pdf2svg</a> which requires <a href="https://poppler.freedesktop.org/" target="_blank" rel="noopener">poppler</a> and <a href="https://www.cairographics.org/" target="_blank" rel="noopener">cairo</a> of high version. Installing dependencies on CentOS7 is a terrible struggle. The simplest way is to use pdftocairo which is contained in poppler-utils. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install poppler-utils</span><br></pre></td></tr></table></figure><p>Then run with a test pdf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pdftocairo test.pdf output.svg -svg</span><br></pre></td></tr></table></figure><h3 id="Pack-the-pipeline"><a href="#Pack-the-pipeline" class="headerlink" title="Pack the pipeline"></a>Pack the pipeline</h3><p>Write a bash script to pack all steps together.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">vim test.tex</span><br><span class="line">pdflatex test.tex</span><br><span class="line">perl pdfcrop.pl test.pdf</span><br><span class="line">pdftocairo test-crop.pdf output.svg -svg</span><br><span class="line">rm test.* test-crop.pdf</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> CS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Random Perturbation to Low Rank Matrices</title>
      <link href="/2021/04/09/Random-Perturbation-to-Low-Rank-Matrices/"/>
      <url>/2021/04/09/Random-Perturbation-to-Low-Rank-Matrices/</url>
      
        <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Let $A$ stand for the true symmetric matrix and $E$ represent the perturbation. Let $\lambda_i$’s be the eigenvalues of $A$, sorted in descending order and denote $\delta=\lambda_1-\lambda_2$ to be the eigengap. Let $u_i$ be the eigenvector of $A$ and $v_i$ be the eigenvector of $A+E$. The classical <a href="http://yueqicao.top/2021/01/12/Davis-Kahan-s-Theorem/" target="_blank" rel="noopener">Davis-Kahan theorem</a> states that</p><script type="math/tex; mode=display">\sin(u_1,v_1)\le \frac{2\|E\|_{op}}{\delta}</script><p>up to constant the DK bound is sharp, as we see in the following example</p><script type="math/tex; mode=display">A=\left[\begin{array}{cc}1+\epsilon & 0\\ 0 &1-\epsilon    \end{array}\right],\, E=\left[\begin{array}{cc}    -\epsilon & \epsilon \\ \epsilon & \epsilon\end{array}\right]</script><p>However, researchers find that in certain cases the bound can be improved. In <a href="https://arxiv.org/abs/1311.2657v5" target="_blank" rel="noopener">Vu’s paper</a>, the author considered the following setting: $A$ is an $n\times n$ matrix but of low rank $r$, $E$ is a random Bernoulli matrix whose entries are iid random variables taking values $\pm 1$ with probability $1/2$. From random matrix theory we know that $|E|_{op}=(2+o(1))\sqrt{n}$. Then the classical DK bound is about $4\sqrt{n}/\delta$. But the experiment shows that this bound is far from optimal.</p><p><img src="/2021/04/09/Random-Perturbation-to-Low-Rank-Matrices/DKbound.png" alt="DK"></p><p>In the experiment we construct a $400\times 400$ matrix $A$ such that $A(1,1)=10+\delta, A(2,2)=10$ and zero otherwise. We set $\delta=100(1+rand)$ where $rand$ is uniform on $[0,1]$ and we run 20 times. The blue bar represents the true $\sin$ and red bar represents the classical DK bound. The result motivates us to find a sharper bound in random settings.</p><h2 id="Random-Noises"><a href="#Random-Noises" class="headerlink" title="Random Noises"></a>Random Noises</h2><p>We state a simplified assumption on the random matrix $E$ from <a href="https://arxiv.org/abs/1311.2657v5" target="_blank" rel="noopener">Vu</a> (just omit the constants). The $n\times n$ matrix $E$ is $\gamma$-concentrated if for all unit vectors $u,v$ and every $t$ we have</p><script type="math/tex; mode=display">\log\mathbb{P}(|u^TEv|>t)=O(-t^\gamma)</script><p>i.e. the rate tending to $-\infty$ is in order at least $\gamma$. </p><p><strong>Examples:</strong></p><ul><li>Bernoulli matrix is $2$-concentrated;</li><li>Gaussian matrix is $2$-concentrated;</li><li>Sub-exponential matrix is $1$-concentrated.</li></ul><p>In particular, for bounded random matrices we have </p><blockquote><p>Let $E=(\xi<em>{ij})$ be a matrix with independent random variables each with mean zero. Furthermore, $|\xi</em>{ij}|\le K$ a.s. for all $i,j$. Then $E$ is $2$-concentrated and </p><script type="math/tex; mode=display">\mathbb{P}(|u^TEv|>t)\le 2\exp(\frac{-t^2}{2K^2})</script></blockquote><h2 id="Improved-DK-bound"><a href="#Improved-DK-bound" class="headerlink" title="Improved DK bound"></a>Improved DK bound</h2><p>Note that </p><script type="math/tex; mode=display">\sin^2\angle (u_1,v_1) = 1-\cos^2\angle (u_1,v_1) = \sum_{i=2}^n |u_k\cdot v_1|^2</script><p>Thus it suffices to bound $|u<em>k\cdot v_1|$. In fact, Let $Q=[u_2,\cdots,u_r]$ and  $P=[u</em>{r+1},\cdots,u_n]$. we want to bound </p><script type="math/tex; mode=display">\|Q^Tv_1\|^2 \text{ and } \|P^Tv_1\|^2</script><p>For $Q^Tv_1$ we have</p><script type="math/tex; mode=display">Q^T(A+E)v_1-Q^TAv_1=Q^TEv_1</script><p>which is equivalent to</p><script type="math/tex; mode=display">(\mu_1I-\Lambda_r)Q^Tv_1 = Q^TEv_1</script><p>where $\Lambda_k$ is the diagonal matrix consisting of $\lambda_2,\cdots,\lambda_r$. It follows</p><script type="math/tex; mode=display">|\mu_1-\lambda_2|\|Q^Tv_1\|\le \|Q^TEv_1\|</script><p>Thus we need a lower bound for gap $|\mu_1-\lambda_2|$ and an upper bound for $|Q^TEv_1|$. For $\mu_1$ we have</p><script type="math/tex; mode=display">\mu_1 = \|A+E\|_{op}\ge u_1^T(A+E)u_1 = \lambda_1 + u_1^TEu_1</script><p>Since $E$ is $\gamma$-concentrated, we see that $\mu_1\ge \lambda_1-t$ with probability at least $1-\exp(O(-t^\gamma))$. If the eigengap $\delta=\lambda_1-\lambda_2$ is sufficient large (for example, $\delta= 2t$) then $\mu_1-\lambda_2&gt;\delta/2$ with probability at least $1-\exp(O(-\delta^\gamma))$.</p><p> From the above discussion we know $\mu_1\ge \lambda_1/2$ with probability at least $1-\exp(O(-\lambda_1^\gamma))$. Then</p><script type="math/tex; mode=display">(PP^Tv_1)^T(A+E)v_1-(PP^Tv_1)^TAv_1=(PP^Tv_1)^TEv_1</script><p>Note that $P^TA=0$ by our construction and so $\mu_1(PP^Tv_1)^Tv_1=(PP^Tv_1)^TEv_1$. We know that </p><script type="math/tex; mode=display">\|P^Tv_1\|^2 = (PP^Tv_1)^Tv_1 \le \frac{1}{\mu_1}\|P^Tv_1\|\|E\|_{op}</script><p>Therefore, $|P^Tv<em>1|\le 2|E|</em>{op}/\lambda_1$ with probability at least $1-\exp(O(-\lambda_1^\gamma))$. </p><p>Now if we write $v_1= (I-PP^T)v_1+PP^Tv_1$ then</p><script type="math/tex; mode=display">\|Q^TEv_1\|\le \|(I-PP^T)^TE(I-PP^T)\|_{op}+\|E\|_{op}\|P^Tv_1\|</script><p>By a standard $\epsilon$-net argument the first term is proved to be no greater than $tr^{1/\gamma}$ with probability at least $1-\exp(O(-rt^\gamma)+O(r))$. Above all we have</p><script type="math/tex; mode=display">\|Q^Tv_1\|\le \frac{2}{\delta}(tr^{1/\gamma}+2\frac{\|E\|_{op}^2}{\lambda_1})</script><p>with probability at least $1-\exp(O(-rt^\gamma)+O(r))-\exp(O(-\lambda_1^\gamma))-\exp(O(-\delta^\gamma))$.</p><p>Finally we see that the new bound is related to three items: the largest eigenvalue $\lambda_1$, the eigengap $\delta$ and the rank $r$:</p><script type="math/tex; mode=display">\sin\angle (u_1,v_1)\le 4(\frac{tr^{1/\gamma}}{\delta}+\frac{\|E\|_{op}}{\lambda_1}+\frac{\|E\|^2_{op}}{\lambda_1\delta})</script><p>with probability at least $1-\exp(O(-rt^\gamma)+O(r))-\exp(O(-\lambda_1^\gamma))-\exp(O(-\delta^\gamma))$. </p><p><strong>Remark 1.</strong> This result is only nontrivial if the amplitude of $\lambda<em>1$ is much larger than $\delta$ and $|E|</em>{op}$ (otherwise when $\lambda_1\sim\delta$ the bound is worse than classical DK bound). In this case the bound is $O(r^{\gamma}/\delta)$ which is much better than $O(n^{1/\gamma}/\delta)$. </p><p><strong>Remark 2.</strong> From the above simulation we see that the bound is not sharp. Therefore there are chances to obtain better results for random perturbations.</p>]]></content>
      
      
      <categories>
          
          <category> Matrix-Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Perturbation Theory </tag>
            
            <tag> Matrix Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Davis-Kahan Theorem</title>
      <link href="/2021/01/12/Davis-Kahan-s-Theorem/"/>
      <url>/2021/01/12/Davis-Kahan-s-Theorem/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><p>Let $A$ be an $n\times n$ Hermitian matrix, and suppose we have the following spectral decomposition for $A$</p><script type="math/tex; mode=display">A=\sum_{i=1}^n\lambda_iu_iu_i^*</script><p>where $\lambda_i$’s are eigenvalues of $A$ (<em>we do not need to sort eigenvalues here</em>), and $u_i$’s are corresponding eigenvectors and $u^*_i$ denotes conjugate transpose. Now, suppose $H$ is also an $n\times n$ Hermitian matrix that represents some perturbation added to $A$. The spectral decomposition for $A+H$ is</p><script type="math/tex; mode=display">A+H=\sum_{i=1}^n\mu_iv_iv_i^*</script><p>We want to know </p><ul><li>(1) the error bound for eigenvalues $\lambda_i$ and $\mu_i$;</li><li>(2) the error bound for eigenvectors $u_i$ and $v_i$. </li></ul><p>It is worth thinking whether above two problems are <strong>well-defined</strong>. In fact, a critical thing is how to ‘match’ the original eigen-value/vector and the corresponding perturbed eigen-value/vector. For the first problem, we can sort the eigenvalues and match them according to the order. For the second problem, however, the situation is more intricate: think about two different eigenvalues that become equal after perturbation. Then any vector in the two-dimensional eigenspace will be an eigenvector corresponding to the new eigenvalue. Therefore, it is natural to place some assumptions on eigengaps to separate different eigenspaces.</p><p>Define</p><script type="math/tex; mode=display">P=\sum_{i=1}^ku_iu_i^*=UU^*</script><p>to be the orthogonal projection operator to the $k$-dimensional eigenspace spanned by $u_1,\cdots,u_k$. Let</p><script type="math/tex; mode=display">P_\perp=I-P=\sum_{i=k+1}^nu_iu_i^*=U_\perp U_\perp^*</script><p>Similarly, define</p><script type="math/tex; mode=display">Q=\sum_{i=1}^kv_iv_i^*=VV^*</script><p>and $Q_\perp=I-Q$. The second problem can be generalized to </p><ul><li>(2*) the error bound for projections $P$ and $Q$.</li></ul><p>We focus on the generalized problem.</p><h2 id="Principal-Angles"><a href="#Principal-Angles" class="headerlink" title="Principal Angles"></a>Principal Angles</h2><p>Recall that for nonzero vectors $x$ and $y$ we define their angles to be</p><script type="math/tex; mode=display">\angle x,y = \text{arccos} \frac{x^*y}{\|x\|\|y\|}</script><p>The range for angles between two vectors is $[0,\pi]$. Recall that in affine geometry we have defined angles between lines and planes: we define the angle between $\mathbb{R}x$ and $\mathbb{R}y$ to be</p><script type="math/tex; mode=display">\angle \mathbb{R}x,\mathbb{R}y = \text{arccos} \frac{|x^*y|}{\|x\|\|y\|} = \inf_{a,b\in\mathbb{R}}\text{arccos} \frac{(ax)^*(by)}{\|ax\|\|by\|}</script><p>Suppose $y$ and $z$ span a two-plane $\mathbb{R}{y,z}$. The angle between $\mathbb{R}x$ and $\mathbb{R}{y,z}$ is defined as</p><script type="math/tex; mode=display">\angle \mathbb{R}x,\mathbb{R}\{y,z\} = \angle \mathbb{R}x,\mathbb{R}(yy^*+zz^*)x = \inf_{a,b,c\in\mathbb{R}}\text{arccos}\frac{(ax)^*(by+cz)}{\|ax\|\|by+cz\|}</script><p>Now we want to define angles between two vector spaces. Suppose $E=\mathbb{C}{e_1,\cdots,e_r}$ and $F=\mathbb{C}{f_1,\cdots,f_s}$ are two vector spaces, we define</p><script type="math/tex; mode=display">\angle E,F = \inf_{e\in E,f\in F}\text{arccos}\frac{e^*f}{\|e\|\|f\|}</script><p>With a bit abuse of notation, we let $E$ denote the matrix $[e_1,\cdots,e_r]$ and similarly let $F$ denote $[f_1,\cdots,f_s]$. If we write</p><script type="math/tex; mode=display">e=E\alpha,\quad f=F\beta</script><p>where $\alpha$ and $\beta$ are unit vectors. Then we see that </p><script type="math/tex; mode=display">\angle E,F = \inf_{\alpha,\beta}\text{arccos}(\alpha^*E^*F\beta)=\text{arccos}(\lambda_{\max}(E^*F))</script><p>where $\lambda_{\max}$ denotes the largest singular value. Since we have seen that the angle between two vector spaces is related to the singular value, it will be helpful to consider all the singular values instead of the largest one. We call each</p><script type="math/tex; mode=display">\theta_i=\text{arccos}\lambda_i(E^*F)</script><p>the principal angle between $E$ and $F$. Note that each principal angle is between $[0,\pi/2]$. Define the following matrix</p><script type="math/tex; mode=display">\Theta_{E,F}=\text{arccos}(E^*F)</script><p>where $\text{arccos}$ acts on singular values in the singular decomposition. Though in general cases $\Theta<em>{E,F}\neq \Theta</em>{F,E}$, in the following discussion we only focus on $\sin(\Theta)$ whose nonzero singular values are equal for $\sin(\Theta<em>{E,F})$ and $\sin(\Theta</em>{F,E})$.   Now we have prepared to look at the Davis-Kahan theorem. </p><h2 id="Davis-Kahan’s-sin-Theta-Theorem"><a href="#Davis-Kahan’s-sin-Theta-Theorem" class="headerlink" title="Davis-Kahan’s $\sin(\Theta)$ Theorem"></a>Davis-Kahan’s $\sin(\Theta)$ Theorem</h2><p>Back to our set-up, we want to compare two projection matrices $P$ and $Q$. Using the Frobenius norm for matrices, we have</p><script type="math/tex; mode=display">\|P-Q\|_{F}^2=\text{tr}((P-Q)^2)=2k-2\text{tr}(PQ)</script><p>Note that</p><script type="math/tex; mode=display">\text{tr}(PQ)=\|U^*V\|_F^2=\|\cos(\Theta_{U,V})\|^2_F</script><p>Therefore, we have</p><script type="math/tex; mode=display">\|P-Q\|_F=\sqrt{2}\|\sin(\Theta_{U,V})\|_F</script><p>To bound the error between two projection matrices, it suffices to bound $|\sin(\Theta_{U,V})|_F$.</p><p>We write the eigen-decomposition for $A$ with respect to $U$ and $U_\perp$ as</p><script type="math/tex; mode=display">A = [U,U_\perp]\left[\begin{array}{cc}    \Lambda & 0\\    0 & \Lambda_\perp\end{array}\right]\left[\begin{array}{c}    U^*\\ U^*_\perp\end{array}\right]</script><p>where $\Lambda$ is the diagonal matrix consisting of eigenvalues $\lambda<em>1,\cdots,\lambda_k$ whereas $\Lambda</em>\perp$ consists of eigenvalues $\lambda<em>{k+1},\cdots,\lambda_n$. Similarly, write the eigen-decomposition for $A+H$ with respect to $V$ and $V</em>\perp$ as</p><script type="math/tex; mode=display">A+H = [V,V_\perp]\left[\begin{array}{cc}    M & 0\\    0 & M_\perp\end{array}\right]\left[\begin{array}{c}    V^*\\ V^*_\perp\end{array}\right]</script><p>Finally, we write $H$ in the following form</p><script type="math/tex; mode=display">H = [U,U_\perp]\left[\begin{array}{cc}    H_0 & B\\    B^* & H_1\end{array}\right]\left[\begin{array}{c}    U^*\\ U^*_\perp\end{array}\right]</script><p>so that under the basis $[U,U_\perp]$, we have</p><script type="math/tex; mode=display">A+H = [U,U_\perp]\left[\begin{array}{cc}    \Lambda+H_0 & B\\    B^* & \Lambda_\perp+H_1\end{array}\right]\left[\begin{array}{c}    U^*\\ U^*_\perp\end{array}\right]</script><p>Since our focus is the eigenspace spanned by $u_1,\cdots,u_k$, define the following residual</p><script type="math/tex; mode=display">\begin{aligned}R =& (A+H)U-AU=UH_0+U_{\perp} B^*\\=& VM V^*U+V_\perp M_\perp V_\perp^*U-U\Lambda\end{aligned}</script><p>If we multiply $V_\perp^*$ on the left, we have</p><script type="math/tex; mode=display">V_\perp^*R=M_\perp V_\perp^*U-V_\perp^*U\Lambda</script><p>Note that $M_\perp$ and $\Lambda$ are nonnegative diagonal matrices. For Frobenius norm we have the following lemma</p><blockquote><p>Suppose $D$ is a nonnegative diagonal matrix whose largest and smallest elements are $d<em>{\max}$ and $d</em>{\min}$, respectively. Then we have $d<em>{\min}|X|_F\le |DX|_F\le d</em>{\max}|X|_F$.</p></blockquote><p>This lemma inspires us to place some assumption on gaps between $M<em>\perp$ and $\Lambda$ to obtain a bound for $R$. It coincides with our intuition: In most cases we often assume that there is some eigengap between $\Lambda$ and $\Lambda</em>\perp$. After a small perturbation $H$, we may expect $M<em>\perp$ is close to $\Lambda</em>\perp$. Now if there is still a gap between $\Lambda$ and $M_\perp$, we believe that the original projection is close to the perturbed projection. So, we may assume the following</p><blockquote><p>Suppose there exists some $\delta&gt;0$ such that $|\lambda_i-M_j|&gt;\delta$ for all $i=1,\cdots,k$ and $j=k+1,\cdots,n$.</p></blockquote><p>Under this assumption we see that </p><script type="math/tex; mode=display">\|R\|_F\ge\|V_\perp^*R\|_F\ge \delta\|V_\perp^*U\|_F</script><p>However, $|V^*<em>\perp U|_F$ is nothing but $|\sin(\Theta</em>{U,V})|_F$. To see that, we have</p><script type="math/tex; mode=display">\|V_\perp^*U\|_F^2=\text{tr}(P(I-Q))=k-\text{tr}(PQ)=\|\sin(\Theta_{U,V})\|_F^2</script><p>Thus we obtain the so-called Davis-Kahan’s $\sin(\Theta)$ theorem</p><script type="math/tex; mode=display">\|P-Q\|_F\le \frac{\sqrt{2}\|R\|_F}{\delta}\le \frac{\sqrt{2}\|H\|_F}{\delta}</script><p>If we are interested in the spectral norm, apply the inequality $|R|<em>F\le \sqrt{k}|R|</em>{op}$, and note that $|R|<em>{op}\le|H|</em>{op}$. So we also have</p><script type="math/tex; mode=display">\|P-Q\|_{op}\le \|P-Q\|_F\le \frac{\sqrt{2k}\|H\|_{op}}{\delta}</script><p>It is also possible to make assumptions on gaps between $\Lambda_\perp$ and $M$. Consider another residual</p><script type="math/tex; mode=display">\begin{aligned}R_\perp =& (A+H)U_\perp-AU_\perp=UB+U_\perp H_1\\=& VM V^*U_\perp+V_\perp M_\perp V_\perp^*U_\perp-U_\perp \Lambda_\perp\end{aligned}</script><p>Multiply $V^*$ on the left</p><script type="math/tex; mode=display">V^*R_\perp = M V^*U_\perp-V^*U_\perp\Lambda_\perp</script><p>But what is $|V^*U_\perp|_F$? The computation shows that </p><script type="math/tex; mode=display">\|V^*U_\perp\|_F^2=\text{tr}(Q(I-P))=k-\text{tr}(PQ)=\|\sin(\Theta_{U,V})\|_F^2</script><p>Thus we come to the same conclusion.</p><h2 id="Generalizations"><a href="#Generalizations" class="headerlink" title="Generalizations"></a>Generalizations</h2><p>A defect in Davis-Kahan’s theorem is that we need to compare either $\Lambda$ with $M<em>\perp$ or $\Lambda</em>\perp$ with $M$. However, in practice (especially in statistics where $A$ usually stands for some population matrix and $H$ stands for random noises), it is more reasonable to assume a priori comparison between $\Lambda$ and $\Lambda_\perp$. Can we change the assumption but still keep the same error bound? The answer is YES. It is noted by <a href="http://www.statslab.cam.ac.uk/~rjs57/Biometrika-2015-Yu-315-23.pdf" target="_blank" rel="noopener">this paper</a> that the theorem also holds if we only assume an eigengap on $A$. The proof is straightforward: without perturbation we have $AU=U\Lambda$. After perturbation we want to know the residual $L=AV-V\Lambda$. On the one hand,</p><script type="math/tex; mode=display">L=AV-V\Lambda+HV-HV=V(M-\Lambda)-HV</script><p>Using <a href="http://i.stanford.edu/pub/cstr/reports/cs/tr/70/150/CS-TR-70-150.pdf" target="_blank" rel="noopener">Wielandt-Hoffman theorem</a> we have</p><script type="math/tex; mode=display">\|L\|_F\le \|V(M-\Lambda)\|_F+\|HV\|_F\le 2\|H\|_F</script><p>On the other hand, using the eigen-decomposition for $A$, we have</p><script type="math/tex; mode=display">L=U\Lambda U^*V+U_\perp\Lambda_\perp U_\perp^*V-V\Lambda</script><p>Note that</p><script type="math/tex; mode=display">V\Lambda=(P+P_\perp)V\Lambda=UU^*V\Lambda+U_\perp U_\perp^*V\Lambda</script><p>Thus,</p><script type="math/tex; mode=display">L=U(\Lambda U^*V-U^*V\Lambda)+U_\perp(\Lambda_\perp U_\perp^*V-U_\perp^*V\Lambda)</script><p>Since $U$ and $U_\perp$ are orthogonal, we have</p><script type="math/tex; mode=display">\|L\|_F\ge \|\Lambda_\perp U_\perp^*V-U_\perp^*V\Lambda\|_F</script><p>Now if we assume</p><blockquote><p>$|\lambda_i-\lambda_j|\ge \delta&gt;0$ for all $i=1,\cdots,k$ and $j=k+1,\cdots,n$.</p></blockquote><p>from the above discussions it holds</p><script type="math/tex; mode=display">\delta \|\sin(\Theta_{U,V})\|_F\le \|L\|_F\le 2\|H\|_F</script><p>If we are interested in the spectral norm, control $|L|<em>F$ by $2\sqrt{k}|H|</em>{op}$. So it holds</p><script type="math/tex; mode=display">\|\sin(\Theta_{U,V})\|_F\le \frac{2\min\{\sqrt{k}\|H\|_{op},\|H\|_F\}}{\delta}</script><p>if $k=1$, obviously $|H|_{op}\le|H|_F$. Hence,</p><script type="math/tex; mode=display">\|\sin(\Theta_{U,V})\|_F\le \frac{2\|H\|_{op}}{\delta}</script><p>One may also consider the generalization of Davis-Kahan theorem to rectangular matrices. It is not hard if we notice that for any matrix $A$, </p><script type="math/tex; mode=display">A^*A \text{ and } AA^*</script><p>are Hermitian matrices, and their eigenvalues are nothing but the square of singular values. The only thing to take care of is that we need to relate </p><script type="math/tex; mode=display">\widehat{A}^*\widehat{A}-A^*A \text{ and } \widehat{A}\widehat{A}^*-AA^*</script><p>with $\widehat{A}-A=H$. This is done by the following</p><script type="math/tex; mode=display">\begin{aligned}    \|\widehat{A}^*\widehat{A}-A^*A\|_{op}=&\|(\widehat{A}-A)^*\widehat{A}+A^*(\widehat{A}-A)\|_{op}\\    \le & (\|\widehat{A}\|_{op}+\|A\|_{op})\|\widehat{A}-A\|_{op}\\    \le & (2\|A\|_{op}+\|A-\widehat{A}\|_{op})\|\widehat{A}-A\|_{op}\\    \|\widehat{A}^*\widehat{A}-A^*A\|_F=&\|(\widehat{A}-A)^*\widehat{A}+A^*(\widehat{A}-A)\|_{F}\\    \le & \|(\widehat{A}-A)^*\widehat{A}\|_F+\|A^*(\widehat{A}-A)\|_F\\    =& \|(\widehat{A}^*\otimes I)vec(\widehat{A}-A)\|+\|(I\otimes A^*)vec(\widehat{A}-A)\|\\    \le & (\|\widehat{A}\|_{op}+\|A\|_{op})\|vec(\widehat{A}-A)\|\\    \le & (2\|A\|_{op}+\|A-\widehat{A}\|_{op})\|\widehat{A}-A\|_{F}\\\end{aligned}</script><p>Alternatively, for any $A\in\mathbb{R}^{m\times n}$, we can define the $(m+n)\times (m+n)$ matrix</p><script type="math/tex; mode=display">\tilde{A}=\left[\begin{array}{cc}    0 & A\\    A^*&0\end{array}\right]\text{ and }\tilde{H}=\left[\begin{array}{cc}    0 & H\\    H^*&0\end{array}\right]</script><p>If $Av=\sigma u$ Then</p><script type="math/tex; mode=display">\tilde{A}\left[\begin{array}{c}    u\\    v\end{array}\right]=\sigma \left[\begin{array}{c}    u\\    v\end{array}\right]\text{ and }\tilde{A}\left[\begin{array}{c}    u\\    -v\end{array}\right]=-\sigma \left[\begin{array}{c}    u\\    -v\end{array}\right]</script><p>Therefore, the non-zero eigenvalues of $\tilde{A}$ are the positive and negative of singular values of $A$, and the eigenvectors are formed by the left and right singular vectors of $A$. Note that $\tilde{A}$ and $\tilde{H}$ are Hermitian matrices, which implies we can use Davis-Kahan theorem directly. The only thing we should take care of is that we need to relate $\sin(\tilde{\Theta})$ with $\sin(\Theta_u)$ and $\sin(\Theta_v)$. A typical model is stated as follows.</p><p>Let</p><script type="math/tex; mode=display">\tilde{u}=\left[\begin{array}{c}    u_1\\    u_2\end{array}\right]\text{ and }\tilde{v}=\left[\begin{array}{c}    v_1\\    v_2\end{array}\right]</script><p>Note that $|u_i|=|v_i|=1$ so $|\tilde{u}|=|\tilde{v}|=2$. Then</p><script type="math/tex; mode=display">\cos^2(\tilde{u},\tilde{v})=\frac{1}{4}|\tilde{u}\cdot\tilde{v}|^2\le\frac{1}{2}\cos^2(u_1,v_1)+\frac{1}{2}\cos^2(u_2,v_2)</script><p>Therefore,</p><script type="math/tex; mode=display">\sin^2(\tilde{u},\tilde{v})\ge \frac{1}{2}\sin^2(u_1,v_1)+\frac{1}{2}\sin^2(u_2,v_2)</script><p>The first generalization to singular spaces is carried out by <a href="https://www.semanticscholar.org/paper/Perturbation-bounds-in-connection-with-singular-Wedin/133c76f0d580e09de0ca28ff0fb599d5e57321f8" target="_blank" rel="noopener">Wedin</a>. Thus Davis-Kahan theorem is also called Davis-Kahan-Wedin theorem in literature. </p><h2 id="A-Final-Remark"><a href="#A-Final-Remark" class="headerlink" title="A Final Remark"></a>A Final Remark</h2><p>In Davis and Kahan’s <a href="https://epubs.siam.org/doi/10.1137/0707001" target="_blank" rel="noopener">original paper</a>, they actually proved four types of theorems. Except for the $\sin(\Theta)$ theorem, the remaining three are: (1) $\sin(2\Theta)$ theorem; (2) $\tan(\Theta)$ theorem; (3) $\tan(2\Theta)$ theorem. For $2\Theta$ theorems, they allow the principal angle to be close to either 0 or $\pi/2$. It seems that $2\Theta$ theorems are more general. However, it is rather difficult to find practical applications. </p>]]></content>
      
      
      <categories>
          
          <category> Matrix-Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Perturbation Theory </tag>
            
            <tag> Matrix Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Convergence Rate of Graph Laplacian</title>
      <link href="/2020/12/17/The-Convergence-Rate-of-Graph-Laplacian/"/>
      <url>/2020/12/17/The-Convergence-Rate-of-Graph-Laplacian/</url>
      
        <content type="html"><![CDATA[<p>Amit Singer’s paper <a href="https://www.math.pku.edu.cn/teachers/yaoy/Fall2011/Amit06_laplacian.pdf" target="_blank" rel="noopener"><em>From graph to manifold Laplacian: The convergence rate</em></a> presents a very clear proof of the convergence rate of graph Laplacian that everyone can follow. Techniques involved in this proof have become standard in following works in manifold learning. We review the proof in this post.</p><h2 id="Setups"><a href="#Setups" class="headerlink" title="Setups"></a>Setups</h2><p>Let $x_1,\cdots,x_N$ be i.i.d. samples from uniform distribution on compact submanifold $\mathcal{M}^d\subseteq \mathbb{R}^m$. Construct the weight matrix $W$ by using heat kernel</p><script type="math/tex; mode=display">W_{ij}=\exp\{-\|x_i-x_j\|^2/2\epsilon\}</script><p>Define the degree matrix by</p><script type="math/tex; mode=display">D_{ii}=\sum_j W_{ij}</script><p>The graph Laplacian is given by</p><script type="math/tex; mode=display">L=D^{-1}W-I</script><p>Let $f:\mathcal{M}\to\mathbb{R}$ be a smooth function and $\Delta_\mathcal{M}$ be the Beltrami-Laplacian operator  on $\mathcal{M}$. The convergence result states that for each $x_i$ we have</p><script type="math/tex; mode=display">\frac{1}{\epsilon}\sum_j L_{ij}f(x_j)=\frac{1}{2}\Delta_\mathcal{M}f(x_i)+O(\frac{1}{\sqrt{N\epsilon^{1+d/2}}},\epsilon)</script><p>$\textbf{Remark}:$ In fact the convergence result holds for any $x\in\mathcal{M}$. However, if we choose $x$ to be a fixed data point, then in the following arguments there will be $N-1$ random data and the summation should exclude $x_i$. We follow the original paper and denote $x_i$ by $x$.</p><h2 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h2><p>Define</p><script type="math/tex; mode=display">\begin{aligned}& F(x_j)=\exp\{-\|x-x_j\|^2/2\epsilon\}f(x_j)\\& G(x_j)=\exp\{-\|x-x_j\|^2/2\epsilon\}\end{aligned}</script><p>The graph Laplacian can be written as</p><script type="math/tex; mode=display">(Lf)(x)=\frac{\sum_j F(x_j)}{\sum_j G(x_j)}-f(x)</script><p>As we remarked above, we need to consider</p><script type="math/tex; mode=display">\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}</script><p>As $N$ tends infinity, we may expect</p><script type="math/tex; mode=display">\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}\to\frac{\mathbb{E}F}{\mathbb{E}G}</script><p>where</p><script type="math/tex; mode=display">\begin{aligned}& \mathbb{E}F=\int_{\mathcal{M}} \exp\{-\|x-y\|^2/2\epsilon\}f(y)\frac{\rm{d}y}{vol(\mathcal{M})}\\& \mathbb{E}G=\int_{\mathcal{M}} \exp\{-\|x-y\|^2/2\epsilon\}\frac{\rm{d}y}{vol(\mathcal{M})}\end{aligned}</script><p>The Taylor expansion for $\mathbb{E}F$ is given by</p><script type="math/tex; mode=display">\begin{aligned}&\frac{1}{(2\pi \epsilon)^{d/2}}\int_\mathcal{M}\exp\{-\|x-y\|^2/2\epsilon\}f(y){\rm d}y\\=&f(x)+\frac{\epsilon}{2}(E(x)f(x)+\Delta_\mathcal{M}f(x))+O(\epsilon^{2})\end{aligned}</script><p>Therefore,</p><script type="math/tex; mode=display">\begin{aligned}\frac{\mathbb{E}F}{\mathbb{E}G}&=\frac{f(x)+\frac{\epsilon}{2}(E(x)f(x)+\Delta_\mathcal{M}f(x))+O(\epsilon^{2})}{1+\frac{\epsilon}{2}E(x)+O(\epsilon^{2})}\\&=f(x)+\frac{\epsilon}{2}\Delta_\mathcal{M}f(x)+O(\epsilon^{2})\end{aligned}</script><p>The bias term is given by</p><script type="math/tex; mode=display">\frac{\mathbb{E}F}{\mathbb{E}G}-f(x)=\frac{\epsilon}{2}\Delta_\mathcal{M}f(x)+O(\epsilon^{2})</script><p>$\textbf{Remark}$: In fact we have decomposed $Lf$ as </p><script type="math/tex; mode=display">\begin{aligned}(Lf)(x)=&\frac{\sum_j F(x_j)}{\sum_j G(x_j)}-\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}\\&+\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}\\&+\frac{\mathbb{E}F}{\mathbb{E}G}-f(x)\end{aligned}</script><p>For the first term, a rough estimate is given by</p><script type="math/tex; mode=display">\begin{aligned}&\frac{\sum_j F(x_j)}{\sum_j G(x_j)}-\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}\\=&\frac{(\sum_j F(x_j))(\sum_{j\neq i} G(x_j))-(\sum_{j\neq i} F(x_j))(\sum_{j} G(x_j))}{\sum_j G(x_j)\sum_{j\neq i} G(x_j)}\\=&\frac{F(x)(\sum_{j\neq i} G(x_j))-G(x)(\sum_{j\neq i} F(x_j))}{\sum_j G(x_j)\sum_{j\neq i} G(x_j)}\\=&\frac{f(x)(\sum_{j\neq i} G(x_j))-(\sum_{j\neq i} F(x_j))}{\sum_j G(x_j)\sum_{j\neq i} G(x_j)}\\=&\frac{\sum_j F(x_j)}{\sum_j G(x_j)}\frac{f(x)}{\sum_j F(x_j)}-\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}\frac{1}{\sum_j G(x_j)}\\\le& C\frac{1/N}{1/N\sum_j F(x_j)}f(x)-C'\frac{1/N}{1/N\sum_j G(x_j)}\\=& O(\frac{1}{N\epsilon^{d/2}}) \end{aligned}</script><p>where bounds $C$ and $C’$ come from the fact that convergent sequences are bounded. However, as we will see later $O(1/N\epsilon^{d/2})$ is negligible compared to bias and variance. </p><h2 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h2><p>For the variance error</p><script type="math/tex; mode=display">\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}</script><p>we can only expect it to be small <em>in probability</em> when $N$ tends to infinity. </p><p>Recall that the <a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality" target="_blank" rel="noopener">Chebyshev inequality</a> states that for a random variable $X$ with finite mean $\mu$ and variance $\sigma^2$ then for any $t&gt;0$ we have</p><script type="math/tex; mode=display">\mathbb{P}(|X-\mu|\ge t)\le \frac{\sigma^2}{t^2}</script><p>$\textbf{Remark}$: In the original paper the author used Chernoff inequality. However, according to the context it seems to be <a href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory" target="_blank" rel="noopener">Bernstein inequality</a> ). But in the final statement the author only used variance and missed another term in the denominator. It seems Chebyshev inequality is enough for our use.</p><p>Consider the following probability</p><script type="math/tex; mode=display">\begin{aligned}p(N,\alpha)&=\mathbb{P}\left(\left|\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}\right|\ge \alpha\right)\\&=p_+(N,\alpha)+p_-(N,\alpha)\\&=\mathbb{P}\left(\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}\ge \alpha\right)\\&+\mathbb{P}\left(\frac{\sum_{j\neq i} F(x_j)}{\sum_{j\neq i} G(x_j)}-\frac{\mathbb{E}F}{\mathbb{E}G}\le -\alpha\right)\end{aligned}</script><p>Since $G(x_j)$ are positive, it is equivalent to </p><script type="math/tex; mode=display">\begin{aligned}p(N,\alpha)&=p_+(N,\alpha)+p_-(N,\alpha)\\&=\mathbb{P}\left(\sum_{j\neq i}F(x_j)\mathbb{E}G-G(x_j)\mathbb{E}F-\sum_{j\neq i}\alpha G(x_j)\mathbb{E}G\ge 0\right)\\&+\mathbb{P}\left(\sum_{j\neq i}F(x_j)\mathbb{E}G-G(x_j)\mathbb{E}F+\sum_{j\neq i}\alpha G(x_j)\mathbb{E}G\le 0\right)\end{aligned}</script><p>If we define</p><script type="math/tex; mode=display">Y_j=F(x_j)\mathbb{E}G-G(x_j)\mathbb{E}F+\alpha\mathbb{E}G(\mathbb{E}G-G(x_j))</script><p>Then $Y_j$ are i.i.d. samples with zero mean. The probability $p(N,\alpha)$ can be expressed by</p><script type="math/tex; mode=display">p(N,\alpha)=\mathbb{P}\left(\left|\sum_{j\neq i} Y_j\right|\ge (N-1)\alpha(\mathbb{E}G)^2\right)</script><p>To apply Chebyshev inequality, we only need to compute $\mathbb{E}Y_j^2$. Using the expansion from $\mathbb{E}F$ we have</p><script type="math/tex; mode=display">\begin{aligned}& \mathbb{E}F=\frac{(2\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(f(x)+\frac{\epsilon}{2}(E(x)f(x)+\Delta_\mathcal{M}f(x))+O(\epsilon^2)\right)\\& \mathbb{E}G=\frac{(2\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(1+\frac{\epsilon}{2} E(x)+O(\epsilon^2)\right)\\& \mathbb{E}F^2=\frac{(\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(f^2(x)+\frac{\epsilon}{4}(E(x)f^2(x)+\Delta_\mathcal{M}f^2(x))+O(\epsilon^2)\right)\\& \mathbb{E}G^2=\frac{(\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(1+\frac{\epsilon}{4} E(x)+O(\epsilon^2)\right)\\& \mathbb{E}FG=\frac{(\pi\epsilon)^{d/2}}{vol(\mathcal{M})}\left(f(x)+\frac{\epsilon}{4}(E(x)f(x)+\Delta_\mathcal{M}f(x))+O(\epsilon^2)\right)\end{aligned}</script><p>Note that in the derivation of bias term, the coefficient before $\Delta_\mathcal{M}f$ is $\epsilon/2$. Therefore, we should focus on $\alpha\ll \epsilon$. Neglecting $\alpha$ and $\alpha^2$ term, we have</p><script type="math/tex; mode=display">\mathbb{E}Y_j^2=\frac{2^d(\pi\epsilon)^{3d/2}}{vol(\mathcal{M})^3}\frac{\epsilon}{2}(\|\nabla_\mathcal{M} f\|^2+O(\epsilon))</script><p>Substituting into Chebyshev inequality we have</p><script type="math/tex; mode=display">\begin{aligned}&p(N,\alpha)\\\le&\frac{\mathbb{E}Y_j^2}{(N-1)\alpha^2(\mathbb{E}G)^4}\\=&\frac{\frac{2^d(\pi\epsilon)^{3d/2}}{vol(\mathcal{M})^3}\frac{\epsilon}{2}(\|\nabla_\mathcal{M} f\|^2+O(\epsilon))}{(N-1)\alpha^2\frac{(2\pi\epsilon)^{2d}}{vol(\mathcal{M})^4}\left(1+\frac{\epsilon}{2} E(x)+O(\epsilon^2)\right)^4}\\=&\frac{vol(\mathcal{M})\epsilon(\|\nabla_\mathcal{M} f\|^2+O(\epsilon))}{2^{d+1}(\pi\epsilon)^{d/2}(N-1)\alpha^2\left(1+\frac{\epsilon}{2} E(x)+O(\epsilon^2)\right)^4}\end{aligned}</script><p>If we take </p><script type="math/tex; mode=display">\alpha\approx\sqrt{\frac{vol(\mathcal{M})\epsilon(\|\nabla_\mathcal{M} f\|^2+O(\epsilon))}{2^{d+1}(\pi\epsilon)^{d/2}(N-1)}}</script><p>Thus the noise error is </p><script type="math/tex; mode=display">\frac{\alpha}{\epsilon}=O(\frac{1}{\sqrt{N\epsilon^{1+d/2}}})</script>]]></content>
      
      
      <categories>
          
          <category> Manifold-Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Asymptotic Analysis </tag>
            
            <tag> Spectral Graph Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Two Applications of the Weingarten Map</title>
      <link href="/2020/07/11/Two-Applications-of-the-Weingarten-Map/"/>
      <url>/2020/07/11/Two-Applications-of-the-Weingarten-Map/</url>
      
        <content type="html"><![CDATA[<p>If $\mathcal{M}$ is an Euclidean submanifold, many intrinsic quantities can be expressed using information from the ambient Euclidean space. Here are two examples where the Weingarten map plays an important role: first is the Riemannian hessian of a smooth map, and second is the Ricci curvature tensor. Assume that $f:\mathcal{M}\to\mathbb{R}$ is a smooth function and $\widehat{f}:\mathbb{R}^d\to\mathbb{R}$ is a smooth extension of $f$. In data science, it is much more convenient to compute the gradient or hessian of $\widehat{f}$ than those of $f$. <a href="https://link.springer.com/chapter/10.1007/978-3-642-40020-9_39" target="_blank" rel="noopener">Absil etc.</a> presented the relations and applied them in Riemannian optimization. On the other hand, Ricci curvature is also a hot topic in manifold learning. An old paper of <a href="https://www.jstor.org/stable/2034680" target="_blank" rel="noopener">Hicks</a> proves that there is a simple relation between Ricci map and Weingarten map if the manifold is a hypersurface. Many problems concerning Ricci curvature thus descend to the estimation of the Weingarten map.</p><h3 id="Riemannian-Hessian"><a href="#Riemannian-Hessian" class="headerlink" title="Riemannian Hessian"></a>Riemannian Hessian</h3><p>Let $\mathcal{M}$ be an $m$-dimensional submanifold in the $d$-dimensional Euclidean space $\mathbb{R}^d$. Let $\pi_x:\mathbb{R}^d\to T_x\mathcal{M}$ be the orthogonal projection to the tangent space $T_x\mathcal{M}$. It turns out that the gradient of $f$ is nothing but the projection of gradient of $\widehat{f}$. Denote $\partial\widehat{f}$ to be the usual gradient for $\widehat{f}$ and $\text{grad}(f)$ to be the Riemannian gradient for $f$. Then we have</p><blockquote><p>$\text{grad}(f)_x=\pi_x(\partial\widehat{f}_x)$ </p></blockquote><p>To check this equality, note that for any tangent vector $v\in T_x\mathcal{M}$ and smooth curve $\gamma$ in $\mathcal{M}$ with $\gamma(0)=x$ and $\gamma’(0)=v$ we have </p><script type="math/tex; mode=display">\langle v,\text{grad}(f)_x\rangle=v(f)=\frac{\rm d}{\rm dt}|_0f(\gamma(t))=\frac{\rm d}{\rm dt}|_0\widehat{f}(\gamma(t))=\langle v,\pi_x(\partial\widehat{f}_x)\rangle</script><p>Let $\nabla$ be the Riemannian connection on $\mathcal{M}$. For any vector fields $X$ and $Y$, by definition, </p><script type="math/tex; mode=display">\nabla_XY=\pi(\partial_XY)</script><p>Define the Riemannian hessian of $f$ at $x$ by</p><script type="math/tex; mode=display">\text{Hess}f_x(v)=\nabla_v\text{grad}(f)</script><p>It is easy to check that</p><script type="math/tex; mode=display">\langle\text{Hess}f_x(v),w\rangle=v(w(f))-\langle\nabla_vw,\text{grad}(f)\rangle=\langle\text{Hess}f_x(w),v\rangle</script><p>Thus, $\text{Hess}f_x:T_x\mathcal{M}\to T_x\mathcal{M}$ is a self-adjoint operator. However, the original definition is difficult to use in practice. Further computation shows that</p><script type="math/tex; mode=display">\text{Hess}f_x(v)=\pi_x\partial_v(\pi\partial\widehat{f})=\pi_x((\partial_v\pi)\partial\widehat{f}_x+\partial^2\widehat{f}_x(v))</script><p>where we have used the product rule. If we understand $\pi$ as matrices then $\partial_v\pi$ contains elements which are derivatives of those in $\pi$. From the result we see that Riemannian hessian of $f$ differs from the projection of usual hessian of $\widehat{f}$ by a term involving the covariant derivative of $\pi$. Let $\pi^\perp:\mathbb{R}^d\to T^\perp_x\mathcal{M}$ be the projection operator to the normal space at $x$. Using the identity $\text{id}=\pi+\pi^\perp$, we have </p><script type="math/tex; mode=display">\pi_x\partial_v\pi=-\pi_x\partial_v\pi^\perp</script><p>In addition, since $\pi\pi^\perp=0$, we have</p><script type="math/tex; mode=display">0=\pi_x\partial_v(\pi\pi^\perp)=\pi_x(\partial_v\pi^\perp)_x+\pi_x(\partial_v\pi)_x\pi^\perp_x</script><p>which implies $\pi_x(\partial_v\pi^\perp)_x\pi_x=0$ and $\pi_x(\partial_v\pi)\partial\widehat{f}_x=-\pi_x(\partial_v\pi^\perp)_x\pi_x^\perp\partial\widehat{f}_x$.</p><p>Recall that the Weingarten map at $x$ for $\xi\in T^\perp<em>x\mathcal{M}$ is defined by $A</em>{x,\xi}(v)=-\pi_x\partial_v\tilde{\xi}$ where $v\in T_x\mathcal{M}$ and $\tilde{\xi}$ is any extension of $\xi$. The identity implies that</p><script type="math/tex; mode=display">A_{x,\xi}(v)=-\pi_x\partial_v(\pi^\perp\tilde{\xi})=-\pi_x(\partial_v\pi^\perp)_x\xi-\pi_x\pi_x^\perp\partial_v\tilde{\xi}=-\pi_x(\partial_v\pi^\perp)_x\xi</script><p>Utilizing the above results we have</p><script type="math/tex; mode=display">\text{Hess}f_x(v)=\pi_x\partial^2\widehat{f}_x(v)+A_{x,\pi_x^\perp\partial\widehat{f}_x}(v)</script><h3 id="Ricci-Map"><a href="#Ricci-Map" class="headerlink" title="Ricci Map"></a>Ricci Map</h3><p>Suppose $\mathcal{M}$ is a <strong>hypersurface</strong> in Euclidean space. i.e. $\mathcal{M}$ is of codimension $1$. Let $\xi$ be a local unit normal vector field. The second fundamental form can be simplified as </p><script type="math/tex; mode=display">h(X,Y)=\partial_XY-\nabla_XY=\langle\partial_XY,\xi\rangle\xi=\langle Y,A_\xi(X)\rangle\xi</script><p>where we have used the identity $X\langle Y,\xi\rangle=\langle\partial_XY,\xi\rangle+\langle Y,\partial_X\xi\rangle$ and the definition of Weingarten map. Recall that for Euclidean submanifolds Gauss equation reads</p><script type="math/tex; mode=display">0=\tilde{R}(X,Y)Z=R(X,Y)Z-A_{h(Y,Z)}X+A_{h(X,Z)}Y</script><p>Substituting the expression of the second fundamental form into above, we have</p><script type="math/tex; mode=display">R(X,Y)Z=\langle A_\xi(Y),Z\rangle A_\xi(X)-\langle A_\xi(X),Z\rangle A_\xi(Y)</script><p>Let $Z<em>1,\cdots,Z_m$ be an orthonormal base of principal vectors of $A</em>\xi$. i.e. $A<em>\xi(Z_i)=k_iZ_i$ where $k_i$’s are principal curvatures. Let $H=\sum k_i$ be the mean curvature and define $Ric:T_x\mathcal{M}\to T_x\mathcal{M}$ by $Ric(X)=\sum</em>{i=1}^m R(X,Z_i)Z_i$, called <strong>Ricci map</strong>. Then we have</p><script type="math/tex; mode=display">\begin{aligned}Ric(X)&=\sum_{i=1}^mR(X,Z_i)Z_i\\&=\sum_{i=1}^m\langle A_\xi(Z_i),Z_i\rangle A_\xi(X)-\langle A_\xi(X),Z_i\rangle A_\xi(Z_i)\\&=\sum_{i=1}^m\langle k_iZ_i,Z_i\rangle A_\xi(X)-\langle A_\xi(X),Z_i\rangle k_iZ_i\\&=H A_\xi(X)-\sum_{i=1}^m\langle A_\xi(X),A_\xi(Z_i)\rangle Z_i\\&=H A_\xi(X)-\sum_{i=1}^m\langle A_\xi^2(X),Z_i\rangle Z_i\\&=H A_\xi(X)-A_\xi^2(X)\end{aligned}</script><p>Therefore we have proved the following identity relating the Ricci map with Weingarten map</p><script type="math/tex; mode=display">A_\xi^2-HA_\xi+Ric=0</script><p>An obvious corollary is that every principal vector is also a Ricci vector. i.e. $Ric$ has the same eigenvectors as $A_\xi$.</p><p><strong>Example.</strong> Let $\mathcal{M}$ be a surface in the three space. The characteristic equation for Weingarten map is $A^2-HA+KI=0$ where $K$ is Gauss curvature. Comparing both equations we have $Ric=KI$.</p>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Local PCA with Gaussian Noise</title>
      <link href="/2020/07/08/Local-PCA-with-Gaussian-Noise/"/>
      <url>/2020/07/08/Local-PCA-with-Gaussian-Noise/</url>
      
        <content type="html"><![CDATA[<p>In noiseless case we have a clean result for the convergence rate about tangent space estimation via local PCA ( see <a href="http://yueqicao.top/2020/05/21/Tangent-Space-Estimation-Using-Local-PCA/" target="_blank" rel="noopener">this post</a> ). However, for noisy data the analysis is much more involved, because noise will cause points in the local neighborhood to exit, and points from the outside to enter. The integration in normal patch no more holds due to loss of true location. People proposed many practical approaches to do local PCA with the existence of noise, but few provided with a comprehensive theory. There is one method which covers both sides, called Multiscale SVD. The details are complex and can be found in this <a href="http://dspace.mit.edu/handle/1721.1/72597" target="_blank" rel="noopener">technical report</a>, while the intuition is easy to explain and help us gain insights into other problems in manifold learning.  </p><p>Let $\mathcal{M}\subseteq\mathbb{R}^d$ be an $m$ dimensional submanifold. Assume $X$ is a random vector valued in $\mathcal{M}$ with smooth density with respect to the volume measure. Let $\eta$ be a Gaussian random vector independent of $X$, with zero mean and isotropic covariance, i.e. $\eta\sim\mathcal{N}(0,\sigma^2I_{d\times d})$. Consider the new random vector $Y=X+\eta$. Then the i.i.d samples from $Y$ are our noisy data. In noiseless case we expect to estimate the tangent space at a fixed point $x$ from nearby samples with identical distribution as $X$. For noisy data, however, a fixed point $x$ on the manifold does not make sense because of noise. Instead, we are only accessible to the perturbed point $\tilde{x}=x+\eta$, and the nearby samples are selected with respect to $\tilde{x}$. The bandwidth, i.e. the radius of neighborhood, should be determined according to three factors:</p><ul><li><em>bias</em>: the bandwidth cannot be too large, or the nonlinearity coming from curvature will lead to terrible bias;</li><li><em>variance</em>: the bandwidth cannot be too small, or insufficient points will cause large variance due to sampling;</li><li><em>noise</em>: the bandwidth cannot be too small, or the error will be completely random due to noise.</li></ul><p>In variance though the bandwidth is required not to be ‘small’, as the number of sampling increases the bandwidth does approach zero in noiseless case. This never happens when we take noise into consideration. When the bandwidth is much smaller than the magnitude of noise, the error is controlled by noise and independent to our change of bandwidth. Thus, we cannot expect a convergence by letting the bandwidth tend to zero. Accordingly, the multiscale SVD suggests us to look at the bandwidth varying in a suitable interval. This method especially works in intrinsic dimension estimation, where we need to investigate the eigenvalues of local covariance matrix. During this suitable interval, principal eigenvalues change differently from those coming from noise. Therefore, we are able to count these eigenvalues and estimate the dimension.</p><p><img src="/2020/07/08/Local-PCA-with-Gaussian-Noise/mpca.jpg" alt="mpca"></p><p>To understand the trade-off among bias, variance and noise, it is convenient for us to consider the following virtual case (which is also used in the analysis of multiscale SVD): suppose we first locate the nearby points and then add noise. That is, we know the true samples $X$ before corruption. Let $K$ be a differentiable function supported on $[0,1]$ and denote $K_h(u)=1/h^mK(|u|/h)$ for $u\in\mathbb{R}^d$. Without loss of generality, assume $x$ is placed at the origin and $T_x\mathcal{M}$ coincides with the coordinate space spanned by the first $m$ standard basis $e_1,\cdots,e_m$. Then the population covariance matrix is defined as </p><script type="math/tex; mode=display">\begin{aligned}\mathbb{H}&=\mathbb{E}[YY^tK_h(X)]\\&=\mathbb{E}[XX^tK_h(X)]+\mathbb{E}[X\eta^tK_h(X)]+\mathbb{E}[\eta X^tK_h(X)]+\mathbb{E}[\eta\eta^tK_h(X)]\end{aligned}</script><p>The first matrix is known to have the following decomposition</p><script type="math/tex; mode=display">\mathbb{E}[XX^tK_h(X)]=\begin{bmatrix}ch^2I+O(h^4)& 0\\0&0\end{bmatrix}+\begin{bmatrix}0&O(h^4)\\ O(h^4)&O(h^4)\end{bmatrix}</script><p>For the second and third matrix, using the independence of $X$ and $\eta$, they vanish in the sum. For the fourth term, using geodesic coordinates, we have</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[\eta\eta^tK_h(X)]=c\sigma^2I\end{aligned}</script><p>Therefore, the bias consists of two parts: the curvature of the manifold and noise. While we can eliminate the impact of curvature by choosing smaller and smaller neighborhoods, the impact of noise always exists. </p><p>Now let $Y_1,Y_2,\cdots,Y_n$ be i.i.d. samples. By substituting the expectation by empirical mean, we obtain the empirical covariance matrix</p><script type="math/tex; mode=display">\begin{aligned}H&=\frac{1}{n}\sum_{i=1}^nY_iY_i^tK_h(X_i)\\&=\frac{1}{n}\sum_{i=1}^nX_iX_i^tK_h(X_i)+\frac{1}{n}\sum_{i=1}^nX_i\eta_i^tK_h(X_i)+\frac{1}{n}\sum_{i=1}^n\eta_i X_i^tK_h(X_i)+\frac{1}{n}\sum_{i=1}^n\eta_i\eta_i^tK_h(X_i)\end{aligned}</script><p>Note that $\mathbb{E}[H]=\mathbb{H}$. For an arbitrary element in $H$, the variance is given by</p><script type="math/tex; mode=display">\mathbb{E}[(\frac{1}{n}\sum_{i=1}^nY_i^kY_i^lK_h(X_i)-\mathbb{E}[Y^kY^lK_h(X)])^2]=\frac{1}{n}\mathbb{Var}(Y^kY^lK_h(X))</script><p>It suffices to estimate the second moment terms,</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[(Y^k)^2(Y^l)^2K^2_h(X)]=&\mathbb{E}[((X^k)^2+2X^k\eta^k+(\eta^k)^2)((X^l)^2+2X^l\eta^l+(\eta^l)^2)K^2_h(X)]\\=&\mathbb{E}[\bigg((X^kX^l)^2+2(X^k)X^l\eta^l+(X^k\eta^l)^2+2X^k(X^l)^2\eta^k\\&+4X^kX^l\eta^k\eta^l+2X^k\eta^k(\eta^l)^2+(\eta^kX^l)^2+2X^l\eta^l(\eta^k)^2\\&+(\eta^k\eta^l)^2\bigg)K^2_h(X)]\\=&\mathbb{E}[(X^kX^l)^2K^2_h(X)]+\mathbb{E}[(X^k\eta^l)^2K^2_h(X)]\\&+4\mathbb{E}[X^kX^lK^2_h(X)]\mathbb{E}[\eta^k\eta^l]+\mathbb{E}[(\eta^kX^l)^2K^2_h(X)]\\&+\mathbb{E}[(\eta^k\eta^l)^2K^2_h(X)]\end{aligned}</script><p>The last equality holds by independence and symmetry. The order is then given by</p><script type="math/tex; mode=display">\mathbb{E}[(Y^kY^l)^2K^2_h(X)]=\left\{\begin{array}{l}\frac{1}{h^m}(h^4c_1+2h^2\sigma^2c_2+4\delta_{kl}h^2\sigma^2c_2+\sigma^4c_3),\quad k,l=1,2,\cdots,m\\\frac{1}{h^m}(h^8c_1+2h^4\sigma^2c_2+4\delta_{kl}h^4\sigma^2c_2+\sigma^4c_3),\quad k,l=m+1,\cdots,d\\\frac{1}{h^m}(h^6c_1+h^2\sigma^2c_2+h^4\sigma^2c_2+\sigma^4c_3),\quad \text{otherwise}\end{array}\right.</script><p>Thus with high probability, </p><script type="math/tex; mode=display">H=\begin{bmatrix}ch^2I+O(h^4)+O(\frac{h^2+\sigma^2}{\sqrt{nh^m}})& 0\\0&0\end{bmatrix}+\begin{bmatrix}0&O(h^4)\\ O(h^4)&O(h^4+\sigma^2)\end{bmatrix}+\frac{1}{\sqrt{nh^m}}\begin{bmatrix}0&O((h^2+\sigma^2)^{3/2})\\O((h^2+\sigma^2)^{3/2})&O(h^4+\sigma^2) \end{bmatrix}</script><p>If $\sigma \ll h^2$, then the error is mainly controlled by bandwidth and the noise is relatively negligible. If $\sigma\gg h^2$, the error largely depends on noise. Define $\gamma=\sigma/h^2$. Then</p><script type="math/tex; mode=display">\frac{1}{h^2}H=\begin{bmatrix}cI+O(h^2)+O(\frac{1+\sigma\gamma}{\sqrt{nh^m}})& 0\\0&0\end{bmatrix}+\begin{bmatrix}0&O(h^2)\\ O(h^2)&O(\sigma\gamma)\end{bmatrix}+\frac{1}{\sqrt{nh^m}}\begin{bmatrix}0&O(\sigma^2\gamma)\\O(\sigma^2\gamma)&O(\sigma\gamma) \end{bmatrix}</script><p>If $\sigma\ll \sqrt{nh^m}$, then the error is at least $O(\sigma\gamma)$ and one cannot get better result using local PCA. If $\sigma\gg \sqrt{nh^m}$, then the situation is terrible with large noise and inadequate sampling, with error in the order $O(\sigma^2\gamma/\sqrt{nh^m})$. </p>]]></content>
      
      
      <categories>
          
          <category> Manifold-Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
            <tag> Asymptotic Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tangent Space Estimation Using Local Polynomial Fitting</title>
      <link href="/2020/05/30/Tangent-Space-Estimation-Using-Local-Polynomial-Fitting/"/>
      <url>/2020/05/30/Tangent-Space-Estimation-Using-Local-Polynomial-Fitting/</url>
      
        <content type="html"><![CDATA[<p>Generalizations of local PCA to tangent space estimation are considered in various papers. E. Aamari and C. Levrard considered the non-asymptotic rates of local PCA in a <a href="https://arxiv.org/pdf/1512.02857.pdf" target="_blank" rel="noopener">paper</a> of manifold reconstruction. Suppose $n$ is the number of points and $m$ is the manifold dimension. Then with bandwidth chosen to be $O(\log(n)/n)^{1/m}$ the minimax rate is $O(1/n)^{1/m}$. In their <a href="https://arxiv.org/pdf/1705.00989.pdf" target="_blank" rel="noopener">subsequent work</a>, they considered generalization using local polynomial fitting. It happens that by involving more higher terms the convergence is faster. Suppose the fitting is up to $k$th term, then with the same bandwidth the minimax rate is $O(1/n)^{k/m}$. However, they did not show practical algorithms. In an <a href="http://www.cse.ust.hk/~scheng/pub/j5.pdf" target="_blank" rel="noopener">early paper</a> of S.-W. Cheng and M.-K. Chiu, they proposed an algorithm using polynomial fitting up to the second order, but their analysis did not show consistency with the above result. However, their method is practical and its principle is easy to understand. We present a simple description in the following.</p><p>Let $\mathcal{M}\subseteq \mathbb{R}^{m+1}$ be a hypersurface. Suppose $x\in\mathcal{M}$ is the origin and we want to estimate $T_x\mathcal{M}$. Firstly for the simplest case where $T_x\mathcal{M}$ coincides with the hyperplane spanned by the standard vectors $e_1,\cdots,e_m$ in $\mathbb{R}^{m+1}$, we have the local parametrization </p><script type="math/tex; mode=display">\mathbf{r}(u^1,\cdots,u^m)=(u^1,\cdots,u^m,f(u^1,\cdots,u^m))</script><p>By definition we deduce that</p><script type="math/tex; mode=display">f(0)=0,\, Df(0)=0</script><p>Thus locally $\mathcal{M}$ is the level set of the function </p><script type="math/tex; mode=display">F(\mathbf{u})=u^{m+1}-f(u^1,\cdots,u^m)=u^{m+1}-\frac{1}{2}[u^1,\cdots,u^m]D^2f(0)[u^1,\cdots,u^m]^t+O(\|\mathbf{u}\|^3)</script><p>We write the quadratic form in $F(\mathbf{u})$ as the product of two vectors</p><script type="math/tex; mode=display">\begin{aligned}&[\frac{1}{\sqrt{2}}(u^1)^2,u^1u^2,\cdots,u^1u^m,u^1u^{m+1},\frac{1}{\sqrt{2}}(u^2)^2,\cdots,u^2u^m,u^2u^{m+1},\cdots,\frac{1}{\sqrt{2}}(u^{m+1})^2]\cdot\\&[\frac{1}{\sqrt{2}}\partial_1^2f(0),\partial_1\partial_2f(0),\cdots,\partial_1\partial_mf(0),0,\frac{1}{\sqrt{2}}\partial_2^2f(0),\cdots,\partial_2\partial_mf(0),0,\cdots,0]\end{aligned}</script><p>Define $q:\mathbb{R}^{m+1}\to\mathbb{R}^{(m+1)m/2}$ by </p><script type="math/tex; mode=display">q(\mathbf{u})=[\frac{1}{\sqrt{2}}(u^1)^2,u^1u^2,\cdots,u^1u^{m+1},\frac{1}{\sqrt{2}}(u^2)^2,\cdots,u^2u^{m+1},\cdots,\frac{1}{\sqrt{2}}(u^{m+1})^2]^t</script><p>Denote the vectorized form of $D^2f(0)$ by $Q_{m+1}$. Then </p><script type="math/tex; mode=display">F(\mathbf{u})=\mathbf{u}^te_{m+1}-q(\mathbf{u})^tQ_{m+1}+O(\|\mathbf{u}\|^3)</script><p>In general, let $T<em>x\mathcal{M}=\text{span}{\hat{e}_1,\cdots,\hat{e}_m}$ and $[\hat{e}_1,\cdots,\hat{e}_m,\hat{e}</em>{m+1}]=P[e<em>1,\cdots,e_m,e</em>{m+1}]$. Let $\mathbf{u}=\sum<em>{i=1}^{m+1}u^ie_i=\sum</em>{i=1}^{m+1}\hat{u}^i\hat{e}_i$. As in above $F$ should be a function of $\hat{u}^1,\cdots,\hat{u}^{m+1}$, here we still express $F$ with respect to coordinate components $u^1,\cdots,u^{m+1}$.</p><script type="math/tex; mode=display">\begin{aligned}F(\mathbf{u})&=\mathbf{u}^t\hat{e}_{m+1}-q(\mathbf{u})^tq(P)Q_{m+1}+O(\|\mathbf{u}\|^3)\\&=\mathbf{u}^tPe_{m+1}-q(\mathbf{u})^tq(P)Q_{m+1}+O(\|\mathbf{u}\|^3)\\&=[\mathbf{u},q(\mathbf{u})]^t\left[\begin{array}{c}Pe_{m+1}\\ q(P)Q_{m+1}\end{array}\right]+O(\|\mathbf{u}\|^3)\end{aligned}</script><p>where $q(P)$ is an orthogonal matrix in $\mathbb{R}^{(m+1)m/2}$. </p><p>For Euclidean submanifolds $\mathcal{M} \subseteq\mathbb{R}^{d}$, it is similar since we have $l=d-m$ functions $F_{m+1},\cdots,F_d$ as above. That is, </p><script type="math/tex; mode=display">F_\alpha(\mathbf{u})=[\mathbf{u},q(\mathbf{u})]^t\left[\begin{array}{c}Pe_{\alpha}\\ q(P)Q_{\alpha}\end{array}\right]+O(\|\mathbf{u}\|^3)</script><p>for $\alpha=m+1,\cdots,d$. Now suppose we have $n$ samples $X_1,\cdots,X_n$ from some distribution on the manifold. It is natural to minimize the following error</p><script type="math/tex; mode=display">\frac{1}{n}\sum_{\alpha=m+1}^d\sum_{i=1}^n\left([X_i,q(X_i)]^t\left[\begin{array}{c}Pe_{\alpha}\\ q(P)Q_{\alpha}\end{array}\right]\right)^2</script><p>If we set</p><script type="math/tex; mode=display">\begin{aligned}&A=\left[\begin{array}{ccc}X_{11}&\cdots&X_{1d}\\ \vdots&\ddots&\vdots\\ X_{n1}&\cdots& X_{nd}\end{array}\right]\\&B=\left[\begin{array}{cccccc}\frac{1}{\sqrt{2}}X^2_{11}&X_{11}X_{12}&\cdots&X_{11}X_{1d}&\cdots&\frac{1}{\sqrt{2}}X_{1d}^2\\ \vdots&\vdots&\ddots&\vdots&\ddots&\vdots\\ \frac{1}{\sqrt{2}}X^2_{n1}&X_{n1}X_{n2}&\cdots&X_{n1}X_{nd}&\cdots&\frac{1}{\sqrt{2}}X_{nd}^2\end{array}\right]\\&\left[\begin{array}{c}Pe_{\alpha}\\ q(P)Q_{\alpha}\end{array}\right]=\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]\end{aligned}</script><p>it is equivalent to minimize</p><script type="math/tex; mode=display">\frac{1}{n}\sum_{\alpha=m+1}^d\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]^t[A,B]^t[A,B]\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]</script><p>Note the dimension of eigenspace of $[A B]^t[A B]$ corresponding to $0$ can be far larger than that of normal space. Hence it works for hypersurfaces but becomes intractable for general submanifolds. Furthermore, if one writes down the asymptotic order with respect to bandwidth it is easy to notice that in fact it does not exceed local PCA. To work out a method faster than local PCA needs further consideration. </p><p>Consider the case when the number of samples is less than $d(d+1)/2$. we add the penalty term </p><script type="math/tex; mode=display">\sum_{\alpha=m+1}^d(\frac{1}{n}\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]^t[A,B]^t[A,B]\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]+\gamma\|c_\alpha\|^2)</script><p>Then the objective function becomes</p><script type="math/tex; mode=display">\sum_{\alpha=m+1}^d\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]^tZ\left[\begin{array}{c}g_{\alpha}\\ c_{\alpha}\end{array}\right]</script><p>where </p><script type="math/tex; mode=display">Z=\left[\begin{array}{cc}\frac{1}{n}A^tA&\frac{1}{n}A^tB\\\frac{1}{n}A^tB&\frac{1}{n}B^tB+\gamma I\end{array}\right]=\left[\begin{array}{cc}I&H_\gamma\\0&I\end{array}\right]\left[\begin{array}{cc}W_\gamma&0\\0&U_\gamma\end{array}\right]\left[\begin{array}{cc}I&0\\H_\gamma&I\end{array}\right]</script><p>and </p><script type="math/tex; mode=display">U_\gamma=\frac{1}{n}B^tB+\gamma I,\, H_\gamma=\frac{1}{n}A^tBU_\gamma^{-1},\, W_\gamma=\frac{1}{n}A^tA-H_\gamma U_\gamma H_\gamma^t</script><p>Therefore, we have </p><script type="math/tex; mode=display">(10)=\sum_{\alpha=m+1}^dg_\alpha^tW_\gamma g_\alpha+(c_\alpha+H_\gamma^tg_\alpha)^tU_\gamma(c_\alpha+H^t_\gamma g_\alpha)</script><p>Note that $U<em>\gamma$ is positive definite for $\gamma&gt;0$. Hence the minimum is achieved if $c</em>\alpha+H^t<em>\gamma g</em>\alpha=0$. Let $B=LSR$ be the SVD decomposition of $B$. Then </p><script type="math/tex; mode=display">U_\gamma=R^t(\frac{1}{n}S^2+\gamma I)R</script><p>and </p><script type="math/tex; mode=display">\frac{1}{n}B(U_\gamma^{-1})^tB^t=\frac{1}{n}LS(\frac{1}{n}S^2+\gamma I)^{-1}SL^t</script><p>Substituting into the expression of $W_\gamma$, we see that</p><script type="math/tex; mode=display">W_\gamma=\frac{1}{n}A^t(I-\frac{1}{n}B(U_\gamma^{-1})^tB^t)A=\frac{1}{n}A^tL(I-\frac{1}{n}S(\frac{1}{n}S^2+\gamma I)^{-1}S)L^tA</script><p>Note that the middle term is a diagonal matrix. Write $W<em>\gamma=\gamma A^tL\Sigma</em>\gamma L^tA$. For an arbitrary element $\lambda\in S$ it becomes $1/(\lambda^2+n\gamma)$ in $\Sigma<em>\gamma$. Therefore, the solution is given by the eigenvectors of $W</em>\gamma$ corresponding to the smallest eigenvalue.</p><p>In the original paper, more techniques are involved to remove the penalty parameter and to make the computation efficient. For the convergence rate, the authors only focused on small sample size (between $(m+1)m/2$ and $(d+1)d/2$). Under some assumptions, the angular error is $O(h^2)$ with high probability, where $h$ is the bandwidth. The proof involves a lot of estimations of matrix elements which are used in Gershgorin circle theorem. In fact, the paper contributes a lot to the estimation of eigenvalues. For eigenvectors they used results from <a href="https://ipsen.math.ncsu.edu/ps/rr978.ps" target="_blank" rel="noopener">relative perturbation bounds for eigenspaces</a>, which are generalizations of Davis-Kahan theorem.   </p><p><img src="/2020/05/30/Tangent-Space-Estimation-Using-Local-Polynomial-Fitting/1.jpg" alt="1"><img src="/2020/05/30/Tangent-Space-Estimation-Using-Local-Polynomial-Fitting/2.jpg" alt="2"></p>]]></content>
      
      
      <categories>
          
          <category> Manifold-Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
            <tag> Asymptotic Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tangent Space Estimation Using Local PCA</title>
      <link href="/2020/05/21/Tangent-Space-Estimation-Using-Local-PCA/"/>
      <url>/2020/05/21/Tangent-Space-Estimation-Using-Local-PCA/</url>
      
        <content type="html"><![CDATA[<p>In <a href="https://arxiv.org/abs/1102.0075" target="_blank" rel="noopener">this paper</a>, A.Singer and H.-T. Wu derived an explicit expression for the bias and variance of local PCA in tangent space estimation. One thing to notice: when we have estimated a basis $\widehat{E}$ and want to compare with the true basis $E$, we can either</p><ul><li>consider the error between two projection operators $|\widehat{E}\widehat{E}^T-EE^T|$, </li></ul><p>or </p><ul><li>consider the error up to rotation $\min_{O}|O\widehat{E}-E|$.</li></ul><p>However, according to the original paper, the convergence rates for above two errors are different. Moreover, the authors did not claim the choice of bandwidth is optimal. So other choices of bandwidth may also by possible. </p><p>Let $\mathcal{M}$ be an $m$-dimensional compact submanifold in a Euclidean space $\mathbb{R}^d$. Fix $x\in\mathcal{M}$. Without loss of generality assume that $x$ is placed at the origin and the tangent space $T_x\mathcal{M}$ coincides with the coordinate space spanned by the first $m$ standard vectors ${e_1,\cdots,e_m}$ in $\mathbb{R}^d$. Suppose $X$ is a random vector valued in $\mathcal{M}$ with a smooth positive density function $f:\mathcal{M}\to \mathbb{R}$. Let $K:\mathbb{R}\to\mathbb{R}$ be a smooth function supported on $[0,1]$ and $h&gt;0$ be a positive number. To estimate the tangent space at $x$, we start by finding a direction $v$ such that the projection to $v$ is maximized</p><script type="math/tex; mode=display">\mathbb{E}[((X-x)\cdot v)^2K_h(X-x)]</script><p>where $K_h:\mathbb{R}^d\to\mathbb{R}$ is the function $K_h(u)=1/h^mK(|u|/h)$. Therefore, we consider the following Lagrange function</p><script type="math/tex; mode=display">f(v)=\mathbb{E}[((X-x)\cdot v)^2K_h(X-x)]-\lambda(\|v\|-1)</script><p>Setting $df/dv=0$ we have $\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]v=\lambda v$ and the objective function is $\lambda$. Thus we choose the eigenvector of $\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]$ with respect to the largest eigenvalue. </p><p>Next, we proceed by finding another direction $v’$ orthogonal to $v$ such that the same objective function is maximized. The result turns out that the optimal $v’$ is the eigenvector of $\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]$ of the second largest eigenvalue.  Iteratively, to estimate the tangent space $T_x\mathcal{M}$ we attempt to find the first $m$ eigenvectors of the matrix $\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]$. We set </p><script type="math/tex; mode=display">\mathbb{H}=\mathbb{E}[(X-x)(X-x)^tK_h(X-x)]</script><p>and call it the <strong>population covariance matrix</strong>. In practice, $\mathbb{H}$ is not available. Suppose $X_1,\cdots,X_n$ are i.i.d. samples of $X$.  Then we can replace $\mathbb{H}$ by the empirical mean,</p><script type="math/tex; mode=display">H=\frac{1}{n}\sum_{i=1}^n(X_i-x)(X_i-x)^tK_h(X_i-x)</script><p>which is called the <strong>empirical covariance matrix</strong>. Note that these are not in fact covariance matrices since the mean is forced to be $x$. But it will bring technical convenience for our analysis. We first give a detailed analysis of the covariance matrices.</p><p>For $h$ small enough, the contributed points will lie in a normal patch of $x$. Hence we can use the exponential map. Suppose $X=\exp_x(s_X\theta_X)$ where $|\theta_X|=1$. Then </p><script type="math/tex; mode=display">X-x=s_X\theta_X+\frac{1}{2}\Pi(\theta_X,\theta_X)s_X^2-\frac{1}{6}A_{\Pi(\theta_X,\theta_X)}(\theta_X)s^3+\frac{1}{6}(\nabla_{\theta_X}\Pi)(\theta_X,\theta_X)s^3+o(s^3)</script><p>Thus we have the following expansion for $\mathbb{H}$,</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{H}=&\underbrace{\mathbb{E}[(\theta_X\theta_X^ts_X^2-\frac{1}{6}(\theta_XA_\Pi^t+A_\Pi^t\theta_X)s_X^4+o(s^4_X))K_h(X-x)]}_{\mathbf{H}}\\+&\underbrace{\mathbb{E}[(\frac{1}{2}(\theta_X\Pi^t+\Pi\theta_X^t)s_X^3+\frac{1}{6}(\theta_X(\nabla\Pi)^t+(\nabla\Pi)\theta^t)s_X^4+o(s_X^4))K_h(X-x)]}_{\mathbf{B}_r}\\+&\underbrace{\mathbb{E}[(\frac{1}{4}\Pi\Pi^ts^4)K_h(X-x)]}_{\mathbf{B}_s}\end{aligned}</script><p>Note that $\mathbf{H}$, $\mathbf{B}_r$ and $\mathbf{B}_s$ are blocked matrices. i.e.</p><script type="math/tex; mode=display">\mathbf{H}=\left[\begin{array}{cc}*&0\\0&0\end{array}\right],\mathbf{B}_r=\left[\begin{array}{cc}0&*\\*&0\end{array}\right],\mathbf{B}_s=\left[\begin{array}{cc}0&0\\0&*\end{array}\right]</script><p>Call $\mathbf{H}$ the true covariance matrix since the eigenvectors of $\mathbf{H}$ will give the true basis of the tangent space. $\mathbf{B}_r$ and $\mathbf{B}_s$ will called the basis matrices and we see from the expansion that the bias comes from the curvature of the manifold $\mathcal{M}$. </p><p>Integration in the normal patch gives the leading terms of each matrices. Note that odd terms of $\theta$ will vanish. Therefore,</p><script type="math/tex; mode=display">\mathbf{H}=\left[\begin{array}{cc}ch^2I+O(h^4)&0\\0&0\end{array}\right],\mathbf{B}_r=\left[\begin{array}{cc}0&O(h^4)\\O(h^4)&0\end{array}\right],\mathbf{B}_s=\left[\begin{array}{cc}0&0\\0&O(h^4)\end{array}\right]</script><p>Thus the bias deviates from the true matrix will be of order $O(h^4)$.</p><p>Then we consider the variance coming from $\mathbb{H}$ and $H$. By definition, $\mathbb{E}[H]=\mathbb{H}$. Thus we may write</p><script type="math/tex; mode=display">H=\mathbb{H}+V</script><p>where $\mathbb{E}[V]=0$. For each element in $V$, the variance is</p><script type="math/tex; mode=display">\mathbb{Var}[V(i,j)]=\mathbb{E}[(H(i,j)-\mathbb{H}(i,j))^2]\le\frac{1}{n}\mathbb{E}[(X-x)_i^2(X-x)^2_jK^2_h(X-x)]</script><p>By computation the second moments are given by</p><script type="math/tex; mode=display">\mathbb{E}[(X-x)_i^2(X-x)_j^2K_h(X-x)]=\left\{\begin{array}{l}O(\frac{h^4}{h^m})\quad i,j=1,\cdots,m\\ O(\frac{h^8}{h^m})\quad i,j=m+1,\cdots,d\\ O(\frac{h^6}{h^m})\quad \text{otherwise}\end{array}\right.</script><p>Therefore, the variance matrix is given by</p><script type="math/tex; mode=display">\mathbb{Var}[V(i,j)]=\left\{\begin{array}{l}O(\frac{h^4}{nh^m})\quad i,j=1,\cdots,m\\ O(\frac{h^8}{nh^m})\quad i,j=m+1,\cdots,d\\ O(\frac{h^6}{nh^m})\quad \text{otherwise}\end{array}\right.</script><p>Using Chebyshev’s inequality, we see that for any $\epsilon&gt;0$,</p><script type="math/tex; mode=display">\mathbb{Pr}\{|H(i,j)-\mathbb{H}(i,j)|>\epsilon\}\le\frac{\mathbb{Var}(V(i,j))}{\epsilon^2}</script><p>If we set the right hand side to be $O(1)$, then with high probability $V(i,j)=O(\epsilon)$. Following this principle, with high probability, $V$ is in the following form</p><script type="math/tex; mode=display">V=\frac{h^2}{\sqrt{nh^m}}\left[\begin{array}{cc}O(1)& O(h)\\O(h)&O(h^2)\end{array}\right]</script><p>Overall, we have shown that with high probability, </p><script type="math/tex; mode=display">\begin{aligned}\frac{1}{h^2}H=&c\left[\begin{array}{cc}I&0\\0&0\end{array}\right]+h^2\left[\begin{array}{cc}O(1)&O(1)\\O(1)&O(1)\end{array}\right]+\frac{1}{\sqrt{nh^m}}\left[\begin{array}{cc}O(1)&O(h)\\O(h)&O(h^2)\end{array}\right]\end{aligned}</script><p>If we set $O(\text{bias})=O(\text{variance})$, the <em>optimal bandwidth</em> $h=O(n^{-1/(m+4)})$. In this case we have</p><script type="math/tex; mode=display">H=h^2c\left[\begin{array}{cc}I+h^2A&h^2C\\h^2C^t&h^2B\end{array}\right]=h^2c(S+h^2T)</script><p>where </p><script type="math/tex; mode=display">S=\left[\begin{array}{cc}I&0\\0&0\end{array}\right], T=\left[\begin{array}{cc}A&C\\C^t&B\end{array}\right]</script><p>Let $\mathbf{v}$ be an eigenvector of $S+h^2T$ with respect to the eigenvalue $\mathbf{\Lambda}$. Using the following expansion on both sides of $S\mathbf{v}=\mathbf{\Lambda}\mathbf{v}$,</p><script type="math/tex; mode=display">\begin{aligned}&\mathbf{v}=v_0+h^2v_1+h^4v_2+\cdots\\&\mathbf{\Lambda}=\lambda_0+h^2\lambda_1+h^4\lambda_2+\cdots\end{aligned}</script><p>and compare the coefficients on both sides. We have</p><script type="math/tex; mode=display">Sv_0=\lambda_0v_0</script><p>which means that $\lambda_0=1$ and $v_0$ is in the form $[\bar{v}_0,0]^t$. For the first order term we have</p><script type="math/tex; mode=display">Sv_1+Tv_0=\lambda_0v_1+\lambda_1v_0</script><p>Thus if we write $v_1=[\bar{v}_1,\tilde{v}_1]$ then $A\bar{v}_0=\lambda_1\bar{v}_0$ and $C^t\bar{v}_0=\tilde{v}_1$. For the second term, we have </p><script type="math/tex; mode=display">Sv_2+Tv_1=v_2+\lambda_1v_1+\lambda_2v_0</script><p>Then $A\bar{v}_1+C\tilde{v}_1=\lambda_1\bar{v}_1+\lambda_2\bar{v}_0$. Nothing will vanish after this step. Therefore, we have $\mathbf{v}=[\bar{v}_0,0]+O(h^2)$ and $\mathbf{\Lambda}=1+h^2\lambda_A+O(h^4)$. </p><p>However, if we make the leading term of $A$ different from $B$ and $C$, that is, $O(\text{variance})&gt;O(\text{bias})$ and the error mainly comes from sampling, the result will be different. As in the original paper we set $O(\text{variance})=O(h)$ which gives the sampling rate $h=O(n^{-1/(m+2)})$. In this case we have</p><script type="math/tex; mode=display">S=\left[\begin{array}{cc}I&0\\0&0\end{array}\right], T_1=\left[\begin{array}{cc}A&0\\0&0\end{array}\right], T_2=\left[\begin{array}{cc}0&C\\C^t&B\end{array}\right]</script><p>By <a href="https://www.math.usm.edu/lambers/mat610/sum10/lecture13.pdf" target="_blank" rel="noopener">Wielandt-Hoffman theorem</a>, the eigenvalues satisfy the following inequality</p><script type="math/tex; mode=display">\sum_{i=1}^d(\lambda_i(T_1+hT_2)-\lambda_i(T_1))^2\le\|hT_2\|_F^2</script><p>Hence if we set the eigenvalues in descending order $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_d$, then $\lambda_k(T_1+hT_2)=\lambda_k^A+O(h)$ for $k=1,\cdots,m$ and $\lambda_l(T_1+hT_2)=O(h)$ for $k=m+1,\cdots,d$. </p><p>For eigenvectors we use the same method as above. Suppose $\mathbf{v}$ is a unit eigenvector of $S+hT_1+hT_2$ corresponding to the eigenvalue $\mathbf{\Lambda}$. Using Taylor expansions for $\mathbf{v}$ and $\mathbf{\Lambda}$, </p><script type="math/tex; mode=display">\begin{aligned}&\mathbf{v}=v_0+hv_1+h^2v_2+h^3v_3+h^4v_4+O(h^5)\\&\mathbf{\Lambda}=\lambda_0+h\lambda_1+h^2\lambda_2+h^3\lambda_3+h^4\lambda_4+O(h^5)\end{aligned}</script><p>and by the relation </p><script type="math/tex; mode=display">(S+hT_1+h^2T_2)\mathbf{v}=\mathbf{\Lambda}\mathbf{v}</script><p>we compare the coefficients on both sides. For the constants we have</p><script type="math/tex; mode=display">\begin{aligned}&Sv_0=\lambda_0v_0\end{aligned}</script><p>which implies $\lambda_0=1$ and $v_0$ is in the form $[\bar{v}_0,0]^t$. For the first order term we have </p><script type="math/tex; mode=display">Sv_1+T_1v_0=v_1+\lambda_1v_0</script><p>which implies $v_1$ is also in the form $[\bar{v}_1,0]^t$, and $A\bar{v}_0=\lambda_1\bar{v}_0$. We see that $\bar{v}_0$ is an eigenvector of $A$. For the second order term we have</p><script type="math/tex; mode=display">T_2v_0+T_1v_1+Sv_2=v_2+\lambda_1v_1+\lambda_2v_0</script><p>Note that $T_2v_0=[0,C^t\bar{v}_0]^t$. We see that the left side is $[A\bar{v}_1+\bar{v}_2,C^t\bar{v}_0]^t$ and the right side is $[\lambda_1\bar{v}_1+\lambda_2\bar{v}_0,0]^t+[\bar{v}_2,\tilde{v}_2]^t$. This gives $\tilde{v}_2=C^t\bar{v}_0$ and </p><script type="math/tex; mode=display">A\bar{v}_1=\lambda_1\bar{v}_1+\lambda_2\bar{v}_0</script><p>Multiplying both sides by $\bar{v}_0^t$, we have</p><script type="math/tex; mode=display">\lambda_1\bar{v}_0^t\bar{v}_1=\lambda_1\bar{v}_0^t\bar{v}_1+\lambda_2\bar{v}_0^t\bar{v}_0</script><p>Thus, $\lambda_2=0$ and $A\bar{v}_1=\lambda_1\bar{v}_1$. If we assume the eigenvalues of $A$ are simple (as said in original paper it holds almost surely), then $\bar{v}_1=0$. For the third order term we have </p><script type="math/tex; mode=display">Sv_3+T_1v_2+T_2v_1=v_3+\lambda_1v_2+\lambda_2v_1+\lambda_3v_0</script><p>The left side is $[\bar{v}_3+A\bar{v}_2,0]^t$ and the right side is $[\bar{v}_3+\lambda_1\bar{v}_2+\lambda_3\bar{v}_0,\tilde{v}_3+\lambda_1\tilde{v}_2]$. Hence $\tilde{v}_3=-\lambda_1\tilde{v}_2$ and $A\bar{v}_2=\lambda_1\bar{v}_2+\lambda_3\bar{v}_0$. Using the same method as before we know that $\lambda_3=0$ and $\bar{v}_2=0$. For the fourth term we have</p><script type="math/tex; mode=display">Sv_4+T_1v_3+T_2v_2=v_4+\lambda_1v_3+\lambda_4v_0</script><p>The left side is $[\bar{v}_4+A\bar{v}_3+C\tilde{v}_2,B\tilde{v}_2]^t$ and the right side is $[\bar{v}_4+\lambda_1\bar{v}_3+\lambda_4\bar{v}_0,\lambda_1\tilde{v}_3+\tilde{v}_4]^t$. Hence $\tilde{v}_4=B\tilde{v}_2-\lambda_1\tilde{v}_3$ and </p><script type="math/tex; mode=display">A\bar{v}_3+CC^t\bar{v}_0=\lambda_1\bar{v}_3+\lambda_4\bar{v}_0</script><p>We cannot cancel anything in this step. Overall we have shown that </p><script type="math/tex; mode=display">\begin{aligned}&\mathbf{v}=v_0+h^2v_2+h^3v_3+O(h^4)=[\bar{v}_0+\bar{v}_3h^3+O(h^4),\tilde{v}_2h^2+O(h^3)]\\&\mathbf{\Lambda}=1+h\lambda_1+h^4\lambda_4+O(h^5)\end{aligned}</script><p>Suppose the first $m$ eigenvectors of $H$ are $u_1,\cdots,u_m$ and the eigenvectors of $A$ are $w_1,\cdots,w_m$. We have $u_k=[w_k+O(h^3),O(h^2)]^t$ for $k=1,\cdots,m$. Let $\mathbf{U}=[u_1,\cdots,u_m]$ be the $d\times m$ matrix and $\mathbf{O}=[w_1,\cdots,w_m]^t$ be the $m\times m$ orthogonal matrix. Furthermore, let $\mathbf{\Theta}=[e_1,\cdots,e_m]$ be the matrix consisting of $m$ standard basis of $\mathbb{R}^d$. We measure the deviation of the estimated $m$-plane spanned by $u_k$’s from $T_x\mathcal{M}$ by</p><script type="math/tex; mode=display">\min_{O\in O(m)} \|\mathbf{U}^t\mathbf{\Theta}-O\|_F\le\|\mathbf{U}^t\mathbf{\Theta}-\mathbf{O}\|_F=O(h^3)</script><p>If we choose the optimal bandwidth, the order will be $O(h^2)$. Thus we see that by choosing ‘non-optimal’ bandwidth the convergence even become faster. However, from another perspective, if we want to find a rotation $\hat{O}$ such that the basis $Q=\mathbf{\Theta}\hat{O}$ is closest to $\mathbf{U}$, i.e. the error is measured by </p><script type="math/tex; mode=display">\min_{\hat{O}\in O(m)}\|\mathbf{U}-\mathbf{\Theta}\hat{O}\|_F</script><p>then the error is bounded by </p><script type="math/tex; mode=display">\min_{\hat{O}\in O(m)}\|\mathbf{U}-\mathbf{\Theta}\hat{O}\|_F\le O(h^2)</script><p>Then different bandwidths yield the same order of convergence. Note that the difference comes from the estimation of eigenvectors.</p><p>A final remark is that the method used above by comparing coefficients for different orders is not very strict, since it automatically assumes that there are no fractional order terms. Generalization to fractional order convergence needs further investigation. </p><hr><p><strong>Update:</strong> In this <a href="https://arxiv.org/pdf/1512.02857.pdf" target="_blank" rel="noopener">paper</a> (appendix D theorem 38), a useful version of <a href="https://arxiv.org/pdf/1405.0680.pdf" target="_blank" rel="noopener">Davis-Kahan</a> theorem is stated as follows</p><blockquote><p>Let $O\in\mathbb{R}^{d\times d}$, $B\in\mathbb{R}^{m\times m}$ be positive semi-definite symmetric matrices such that </p><script type="math/tex; mode=display">O=\left[\begin{array}{cc}B&0\\ 0&0\end{array}\right]+E</script><p>Let $T_0$ be the vector space spanned by the first $m$ vectors of canonical basis and $T$ be the eigenspace spanned by the first $m$ eigenvectors of $O$. Then</p><script type="math/tex; mode=display">\angle (T_0,T)\le \frac{\sqrt{2}\|E\|_{op}}{\lambda_{\min}(B)}</script></blockquote><p>Now we return back to the expression of $1/h^2H$, consider the decomposition</p><script type="math/tex; mode=display">\frac{1}{h^2}H=\left[\begin{array}{cc}cI+O(h^2)+O(\sqrt{nh^m})&0\\0&0\end{array}\right]+\left[\begin{array}{cc}0&O(h^2+\frac{h}{\sqrt{nh^m}})\\O(h^2+\frac{h}{\sqrt{nh^m}})&O(h^2)\end{array}\right]</script><p>Note $\lambda_{\min}(cI+O(\cdot))\approx c$. Thus the convergence rate depends on the operator norm of the second matrix. If we set $1/\sqrt{nh^m}=h^\alpha$ where $0&lt;\alpha&lt;1$, then the rate is $O(h^{1+\alpha})$. If $\alpha\ge 1$, the rate is $O(h^2)$. </p><hr><p><strong>Update:</strong> <a href="https://djalil.chafai.net/blog/2011/12/03/the-hoffman-wielandt-inequality/" target="_blank" rel="noopener">This blog</a> gives a very nice introduction to Wielandt-Hoffman inequality. The proof is short and readable. I also write a new blog on Davis-Kahan’s $\sin(\Theta)$ theorem. See <a href="http://yueqicao.top/2021/01/12/Davis-Kahan-s-Theorem/" target="_blank" rel="noopener">this page</a> for more details.</p>]]></content>
      
      
      <categories>
          
          <category> Manifold-Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
            <tag> Asymptotic Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Extrinsic Quantities of Euclidean Submanifolds</title>
      <link href="/2020/05/16/Extrinsic-Quantities-of-Euclidean-Submanifolds/"/>
      <url>/2020/05/16/Extrinsic-Quantities-of-Euclidean-Submanifolds/</url>
      
        <content type="html"><![CDATA[<p>Consider an $m$-dimensional compact Riemannian submanifold $\mathcal{M}$ of a Euclidean space $\mathbb{R}^n$ with $m&lt;n$. There are two aspects to investigate the geometry and topology about the submanifold. On the one hand, one can look at the intrinsic quantities defined on $\mathcal{M}$, say Riemannian metric tensor, geodesics, Jacobian fields and sectional curvature. This has often been done in <a href="https://arxiv.org/pdf/1909.12057.pdf" target="_blank" rel="noopener">Lie Group Learning</a>, <a href="http://math.ucr.edu/home/baez/information/" target="_blank" rel="noopener">Information Geometry</a> and so on. On the other hand, extrinsic quantities related to the embedding of the submanifold into the Euclidean space, which often include normal vectors (or spaces), the second fundamental form and mean curvature etc. , are also useful and important. For data scientists who always confront with the situation where the underlying manifold is unknown and only random samples are available, to look extrinsically is the only choice. Here we list some extrinsic quantities and show their influences on the geometry and topology of the submanifold.</p><h2 id="Reach-and-Medial-Axis"><a href="#Reach-and-Medial-Axis" class="headerlink" title="Reach and Medial Axis"></a>Reach and Medial Axis</h2><p>Let $\mathcal{NM}$ be the normal bundle of $\mathcal{M}$. Consider the map given by</p><script type="math/tex; mode=display">\begin{aligned}E:\mathcal{NM}&\to \mathbb{R}^n\\(x,\xi)&\mapsto x+\xi\end{aligned}</script><p>A tubular neighborhood $\mathcal{M}^\tau$ is the diffeomorphic image under $E$ of some open subset $V\subseteq \mathcal{NM}$ of the form</p><script type="math/tex; mode=display">V=\{(x,\xi)\in\mathcal{NM}:\|\xi\|<\tau(x)\}</script><p>where $\tau:\mathcal{M}\to \mathbb{R}$ is some positive function. That is, $\mathcal{M}^\tau=E(V)$. It is a fact that every embedded submanifold of $\mathbb{R}^n$ has a tubular neighborhood (See <a href="https://www.amazon.com/Introduction-Smooth-Manifolds-Graduate-Mathematics/dp/1441999817" target="_blank" rel="noopener">Theorem 6.17 of Lee</a>. Moreover, if $\mathcal{M}$ is compact, $\tau$ can be chosen to be a constant. The projection $\pi^\tau:\mathcal{M}^\tau\to\mathcal{M}$ sends a point $y\in\mathcal{M}^\tau$ to the closest point $\pi^\tau(y)\in\mathcal{M}$. The <a href="https://arxiv.org/pdf/1705.00989.pdf" target="_blank" rel="noopener">reach</a> $\tau_{\mathcal{M}}$ of $\mathcal{M}$ is the supreme radius for which the tubular neighborhood can be defined. </p><p>In computational geometry, the <a href="http://people.cs.uchicago.edu/~niyogi/papersps/NiySmaWeiHom.pdf" target="_blank" rel="noopener">medial axis</a> is a commonly used notion which is defined as the closure of the set </p><script type="math/tex; mode=display">G=\{x\in\mathbb{R}^n|\exists p\neq q\in\mathcal{M}\text{ s.t. }d(x,\mathcal{M})=\|x-p\|=\|x-q\|\}</script><p>For any $p\in\mathcal{M}$, the local feature size $\sigma(p)$ is the distance of $p$ to $\bar{G}$. By definition we have</p><script type="math/tex; mode=display">\tau_{\mathcal{M}}=\inf_{p\in\mathcal{M}}\sigma(p)</script><h2 id="The-Second-Fundamental-Form"><a href="#The-Second-Fundamental-Form" class="headerlink" title="The Second Fundamental Form"></a>The Second Fundamental Form</h2><p>Fix a point $x\in\mathcal{M}$. Let $\Pi:T<em>x\mathcal{M}\times T_x\mathcal{M}\to T_x^\perp\mathcal{M}$ be the second fundamental form at $x$. For any unit normal $\xi\in T_x^\perp\mathcal{M}$, let $A</em>\xi$ be the shape operator such that </p><script type="math/tex; mode=display">\langle\Pi(u,v),\xi\rangle=\langle A_\xi(u),v\rangle</script><p>Let $\lambda<em>\xi$ be the largest eigenvalue of $A</em>\xi$. It turns out that the operator norm of the shape operator and thus the second fundamental form can be uniformly bounded by the reciprocal of reach. </p><blockquote><p>For all $x\in\mathcal{M}$ and all $\xi\in T<em>x^\perp\mathcal{M}$, we have $\lambda</em>\xi\le 1/\tau<em>{\mathcal{M}}$. Therefore, $|A</em>\xi|<em>{op}\le1/\tau</em>{\mathcal{M}}$. If we define $|\Pi|=\sup<em>{|u|=|v|=1}|\Pi(u,v)|$, then $|\Pi|\le 1/\tau</em>{\mathcal{M}}$.</p></blockquote><p>Suppose there exists $x$ and $\xi\in T<em>x^\perp\mathcal{M}$ such that $\lambda</em>\xi&gt;1/\tau<em>\mathcal{M}$. There exists some $t&lt;\tau</em>{\mathcal{M}}$ such that $\lambda<em>\xi&gt;1/t&gt;1/\tau</em>{\mathcal{M}}$. Note that </p><script type="math/tex; mode=display">\lambda_\xi=\sup_{\|u\|=1}\langle\Pi(u,u),\xi\rangle</script><p>Let $\gamma(s)$ be a geodesic parametrized by the arc length with $\gamma(0)=x$ and $\dot{\gamma}(0)=u$. Then $\Pi(u,u)=\ddot{\gamma}(0)$. Consider the point $q=x+t\xi$. By assumption, $x$ is the closest point to $q$ in $\mathcal{M}$. Thus </p><script type="math/tex; mode=display">f(s):=\|\gamma(s)-q\|^2\ge f(0)=t^2</script><p>Direct computation shows that</p><script type="math/tex; mode=display">\begin{aligned}&\dot{f}(s)=2\langle\dot{\gamma}(s),\gamma(s)-q\rangle\\&\ddot{f}(s)=2(\langle\ddot{\gamma}(s),\gamma(s)-q\rangle+1)\end{aligned}</script><p>Note that $\dot{f}(0)=0$ and $\ddot{f}(0)=2(-t\langle\Pi(u,u),\xi\rangle+1)&lt;0$. By continuity, there exists $s_0$ such that $f(s_0)&lt;f(0)$, which contradicts to our assumption.</p><p>Since $|\Pi(u,v)|=\sup<em>{|\xi|=1}\langle\Pi(u,v),\xi\rangle=\sup</em>{|\xi|=1}\langle A<em>\xi(u),v\rangle$ and for any $\xi$ the operator norm $|A</em>\xi|<em>{op}$ is uniformly bounded, we see that $|\Pi|\le1/\tau</em>{\mathcal{M}}$.</p><h2 id="Local-Parametrizations"><a href="#Local-Parametrizations" class="headerlink" title="Local Parametrizations"></a>Local Parametrizations</h2><h3 id="Exponential-Map"><a href="#Exponential-Map" class="headerlink" title="Exponential Map"></a>Exponential Map</h3><p>The second fundamental form is closely related to the injective radius, as proved by <a href="https://arxiv.org/pdf/math/0511570.pdf" target="_blank" rel="noopener">Bishop and Alexander</a>: If $|\Pi|\le C$, then $inj_\mathcal{M}\ge\pi/C$. Thus the exponential map can be defined on the ball of radius $\pi/C$ in $T_x\mathcal{M}$ for all $x$. Fix $x\in\mathcal{M}$, if we transform $\mathcal{M}$ isometrically in $\mathbb{R}^n$ so that $x$ is the origin and the first $m$ standard basis ${e_1,\cdots,e_m}$ spans the tangent space $T_x\mathcal{M}$. Consider the coordinate components of the exponential map</p><script type="math/tex; mode=display">\exp(u^1,\cdots,u^m)=(r^1(u^1,\cdots,u^m),\cdots,r^n(u^1,\cdots,u^m))</script><p>We can express the derivatives of component functions using the geometrical terms defined on $\mathcal{M}$. Firstly, since the tangent map of $\exp$ at origin is the identity, we have </p><script type="math/tex; mode=display">d\exp_0(e_i)=(\partial_ir^1(0),\cdots,\partial_ir^n(0))=e_i</script><p>We obtain that $\partial_ir^j(0)=\delta^j_i$ for $i,j=1,\cdots,m$ and $\partial_ir^\alpha(0)=0$ for $\alpha=m+1,\cdots,n$. </p><p>For the second derivatives, let $u=\sum_{i=1}^m u^ie_i$ and $\gamma(s)=\exp(su)$. we have</p><script type="math/tex; mode=display">\ddot{\gamma}(0)=(\sum_{i,j}\partial_{i}\partial_jr^1(0)u^iu^j,\cdots,\sum_{i,j}\partial_i\partial_jr^n(0)u^iu^j)=\Pi(u,u)</script><p>Thus, $\partial<em>i\partial_jr^k(0)=0$ for $i,j,k=1,\cdots,m$. Let $\Pi</em>\alpha(u,v)=\langle\Pi(u,v),e<em>\alpha\rangle=\langle A</em>{e<em>\alpha}(u),v\rangle$. We see that $[\partial_i\partial_jr^\alpha(0)=\langle A</em>{e<em>\alpha}e_i,e_j\rangle]</em>{i,j}$ is the matrix representation for shape operator $A<em>{e</em>\alpha}$ for $\alpha=m+1,\cdots,n$. </p><p>By Weingarten formula, we have</p><script type="math/tex; mode=display">\dddot{\gamma}=-A_{\Pi(\dot{\gamma},\dot{\gamma})}(\dot{\gamma})+\nabla_{\dot{\gamma}}^\perp\Pi(\dot{\gamma},\dot{\gamma})</script><p>If we define $\nabla\Pi(u,v,w)=\nabla_w^\perp\Pi(u,v)-\Pi(\nabla_wu,v)-\Pi(u,\nabla_wv)$. Then we have</p><script type="math/tex; mode=display">\dddot{\gamma}(0)=-A_{\Pi(u,u)}(u)+(\nabla\Pi)(u,u,u)</script><p>Comparing the components on both sides we obtain that</p><script type="math/tex; mode=display">\begin{aligned}&\partial_i\partial_j\partial_kr^l(0)=-\langle A_{\Pi(e_i,e_j)}(e_k),e_l\rangle\\&\partial_i\partial_j\partial_kr^\alpha(0)=\langle(\nabla_{e_i}\Pi)(e_j,e_k),e_l\rangle\end{aligned}</script><p>for $i,j,k,l=1,\cdots,m$ and $\alpha=m+1,\cdots,n$. We have also deduced the following tailor expansion for the exponential map</p><script type="math/tex; mode=display">\exp_x(u)=x+u+\frac{1}{2}\Pi(u,u)-\frac{1}{6}A_{\Pi(u,u)}(u)+\frac{1}{6}(\nabla_u\Pi)(u,u)+O(\|u\|^4)</script><p>Let $y=\exp<em>x(su)$ where $|u|=1$. We can compare the Euclidean distance $|y-x|$ and the Riemannian distance $s=d</em>\mathcal{M}(x,y)$. In fact, we have </p><script type="math/tex; mode=display">\|y-x\|^2=s^2+\frac{1}{4}\|\Pi(u,u)\|^2s^4-\frac{1}{3}\langle A_{\Pi(u,u)}(u),u\rangle s^4+o(s^4)</script><p>Therefore, </p><script type="math/tex; mode=display">\|y-x\|=s-\frac{1}{24}\|\Pi(u,u)\|^2s^3+o(s^3)</script><h3 id="Graph-Parametrization"><a href="#Graph-Parametrization" class="headerlink" title="Graph Parametrization"></a>Graph Parametrization</h3><p>Consider a parametrized surface $r(u,v)=(x(u,v),y(u,v),z(u,v))$. Without loss of generality assume that $\left[\begin{array}{cc}x_u&amp; y_u\ x_v&amp;y_v\end{array}\right]$ is non-singular at $p$. Then by implicit function theorem there will be an open neighborhood near $p$ such that $u=u(x,y),v=v(x,y)$. Then $r(x,y)=(x,y,z(x,y))$ is the graph of function $z=z(x,y)$. </p><p>Generally for any submanifold there exists local parametrization such that it is just the graph of some functions. Furthermore, if we place $p$ at the origin and let $T_p\mathcal{M}$ coincide with the $m$-coordinate space, assume that the parametrization is given by</p><script type="math/tex; mode=display">(u^1,\cdots,u^m)\mapsto(u^1,\cdots,u^m,f^1(u^1,\cdots,u^m),\cdots,f^{n-m}(u^1,\cdots,u^m))</script><p>Then by assumption $\partial_if^j(0)=0$ for $i,j=1,\cdots,m$. The normal vector field can be explicitly given by </p><script type="math/tex; mode=display">\xi^\alpha=(-\partial_1f^\alpha,\cdots,-\partial_mf^\alpha,0,\cdots,\overset{\alpha}{1},\cdots,0)</script><p>Thus the matrix representation of the shape operator $A<em>{e</em>\alpha}$ is $[\partial<em>i\xi^\alpha\cdot e_j]</em>{i,j}=Hess(f^\alpha)$. Note that $Hess(f^\alpha)$ also plays the role as the second fundamental form of the hypersurface</p><script type="math/tex; mode=display">(u^1,\cdots,u^m,f^\alpha(u^1,\cdots,u^m))\subseteq \mathbb{R}^{m+1}</script><p>Although the asymptotic of graph parametrization and the exponential map parametrization has a lot in common, they are actually different. For the graph parametrization, the inverse is simply given by the projection onto the tangent space, while for the exponential map this cannot be the case.</p>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Basics about Riemannian Submanifolds</title>
      <link href="/2020/04/25/Basics-about-Riemannian-Submanifolds/"/>
      <url>/2020/04/25/Basics-about-Riemannian-Submanifolds/</url>
      
        <content type="html"><![CDATA[<h2 id="The-Second-Fundamental-Form"><a href="#The-Second-Fundamental-Form" class="headerlink" title="The Second Fundamental Form"></a>The Second Fundamental Form</h2><p>Let $(\tilde{M},\tilde{g})$ be a Riemannian manifold and $M\subset \tilde{M}$ be a submanifold. By restricting $\tilde{g}$ on $M$ the submanifold is assigned with a natural Riemannian metric.  At each point $p$ we have the orthogonal decomposition with respect to $\tilde{g}_p$,</p><script type="math/tex; mode=display">T_p\tilde{M}=T_pM\oplus T_p^\perp M</script><p>where $T<em>p^\perp M$, called the normal space at $p$, denotes the orthogonal complement of $T_pM$ in $T_p\tilde{M}$. Let $T^\perp M=\cup</em>{p\in M}T_p^\perp M$. It can be verified that $T^\perp M$ is a vector bundle on $M$, called the normal bundle. The idea to study Riemannian submanifolds is straightforward: <strong>we differentiate the vector fields along $M$ and look at its tangent components and normal components respectively</strong>.<a id="more"></a> Let $\tilde{\nabla}$ be the metric connection on $\tilde{M}$ and $X\in\Gamma(TM)$ be a tangent vector field on $M$. On the one hand, for any vector fields $X,Y\in \Gamma(TM)$, we have </p><script type="math/tex; mode=display">\tilde{\nabla}_XY=(\tilde{\nabla}_XY)^\top+(\tilde{\nabla}_XY)^\perp</script><p>The first term is nothing but the covariant derivative of $Y$ on $M$. Let $\nabla$ be the metric connection on $M$ with respect to the induced Riemannian metric. Then $(\tilde{\nabla}_XY)^\top=\nabla_XY$. Define </p><script type="math/tex; mode=display">h(X,Y)=(\tilde{\nabla}_XY)^\perp=\tilde{\nabla}_XY-\nabla_XY</script><p>It can be verified that $h$ is a normal-bundle-valued symmetric tensor field on $M$, called <strong>the second fundamental form</strong>. Equation (3) is called <strong>the Gauss formula</strong>.</p><p><strong>Remark.</strong> Given $\xi\in \Gamma(T^\perp M)$, we can define a tensor field $\Pi_\xi(X,Y)=\tilde{g}(h(X,Y),\xi)$. In literature $\Pi$ is also called the second fundamental form. </p><p>On the other hand, for a vector field $\xi\in \Gamma(T^\perp M)$, we have </p><script type="math/tex; mode=display">\tilde{\nabla}_X\xi=(\tilde{\nabla}_X\xi)^\top+(\tilde{\nabla}_X\xi)^\perp</script><p>Define the connection $\nabla^\perp:\Gamma(T^\perp M)\times\Gamma(TM)\to \Gamma(T^\perp M)$ by $(\nabla^\perp)_X\xi=(\tilde{\nabla}_X\xi)^\perp$. It can be verified that $\nabla^\perp$ is a connection on the normal bundle $T^\perp M$ which is compatible with the bundle metric, called <strong>the normal connection</strong>. Define </p><script type="math/tex; mode=display">A_\xi(X)=-(\tilde{\nabla}_X\xi)^\top</script><p>The operator $A_\xi:T_pM\to T_pM$ is called <strong>the shape operator</strong> or <strong>the Weingarten map</strong>. The minus sign is chosen so that the following equation holds</p><script type="math/tex; mode=display">\tilde{g}(A_\xi(X),Y)=\tilde{g}(h(X,Y),\xi)</script><p>The equation</p><script type="math/tex; mode=display">\tilde{\nabla}_X\xi=-A_\xi(X)+\nabla^\perp_X\xi</script><p>is called <strong>the Weingarten formula</strong>.</p><p><strong>Example.</strong> Let $M^{d-1}\subseteq \mathbb{E}^d$ be a hypersurface, $\xi$ be a unit normal vector field on $M$. <strong>The Gauss map</strong> $g:M\to \mathbb{S}^{d-1}$ is defined by $g(p)=\xi_p$. For any $X\in T_pM$, let $\gamma$ be a smooth curve such that $\gamma’(0)=X$. Then</p><script type="math/tex; mode=display">A_\xi X=-(\xi(\gamma)'(0))^\top=-g(\gamma)'(0)=-g_*(X)</script><p>that is, $-A<em>\xi=g</em>*$ is the differential of Gauss map. It is clear from this example that for hypersurfaces the normal connection is trivial.</p><p>Let $e<em>1,\cdots,e_m$ be a basis of the tangent space and $\xi_1,\cdots,\xi</em>{d-m}$ be a basis of the normal space. Assume</p><script type="math/tex; mode=display">A_{\xi_\alpha}e_i=\sum_{j=1}^mA_{\alpha i}^je_j</script><p>Easy computation shows that</p><script type="math/tex; mode=display">\begin{aligned}&\Pi_{\xi_\alpha}(e_i,e_j)=A_{\alpha i}^j\\&h(e_i,e_j)=\sum_{\alpha=1}^{d-m}A_{\alpha i}^j\xi_\alpha\end{aligned}</script><p>Define the mean curvature vector field on $M$ by $H=\frac{1}{m}trace(h)$ where $m$ is the dimension of $M$. By definition $H$ is independent of the choice of basis. Under the above notation we have</p><script type="math/tex; mode=display">H=\frac{1}{m}\sum_{i=1}^m h(e_i,e_i)=\frac{1}{m}\sum_{i=1}^m\sum_{\alpha=1}^{d-m}A_{\alpha i}^i\xi_\alpha=\sum_{\alpha=1}^{d-m}(\frac{1}{m}\text{trace}(A_{\alpha})\xi_{\alpha})=\sum_{\alpha=1}^{d-m}H^\alpha\xi_\alpha</script><p>where $H^\xi=\tilde{g}(H,\xi)$ is called the mean curvature along $\xi$. The value $|H|=(\sum<em>\alpha|H^\alpha|^2)^{1/2}=\frac{1}{m}(\sum</em>{\alpha=1}^{d-m}\text{trace}(A<em>\alpha)^2)^{1/2}$ is called the mean curvature. If $M$ is a hypersurface, $|H|=\frac{1}{m}|\text{trace}(A</em>\alpha)|$. For a surface in 3-dimensional Euclidean space, this coincides with <strong>the absolute mean curvature</strong>. </p><h2 id="Fundamental-Equations"><a href="#Fundamental-Equations" class="headerlink" title="Fundamental Equations"></a>Fundamental Equations</h2><p>To involve the curvature tensor, we need second order derivative. Thus, we differentiate the Gauss formula and the Weingarten formula. For Gauss formula we have</p><script type="math/tex; mode=display">\begin{aligned}\tilde{\nabla}_X\tilde{\nabla}_YZ&=\tilde{\nabla}_X\nabla_YZ+\tilde{\nabla}_Xh(Y,Z)\\&=\nabla_X\nabla_Y+h(X,\nabla_YZ)-A_{h(Y,Z)}(X)+\nabla^\perp_Xh(Y,Z)\end{aligned}</script><p>If we define the curvature tensor to be $\tilde{R}(X,Y)=[\tilde{\nabla}<em>X,\tilde{\nabla}_Y]-\tilde{\nabla}</em>{[X,Y]}$, we have</p><script type="math/tex; mode=display">\begin{aligned}\tilde{R}(X,Y)Z=&R(X,Y)Z+h(X,\nabla_YZ)-h(Y,\nabla_XZ)-h([X,Y],Z)\\&-A_{h(Y,Z)}(X)+A_{h(X,Z)}(Y)+\nabla^\perp_Xh(Y,Z)-\nabla^\perp_Yh(X,Z)\end{aligned}</script><p>Define the covariant differentiation of $h$ to be</p><script type="math/tex; mode=display">(\nabla_Xh)(Y,Z)=\nabla^\perp_Xh(Y,Z)-h(\nabla_XY,Z)-h(\nabla_XZ,Y)</script><p>Then the tangent component of $\tilde{R}(X,Y)Z$ is </p><script type="math/tex; mode=display">(\tilde{R}(X,Y)Z)^\top=R(X,Y)Z+A_{h(X,Z)}(Y)-A_{h(Y,Z)}(X)</script><p>while the normal component is</p><script type="math/tex; mode=display">(\tilde{R}(X,Y)Z)^\perp=(\nabla_Xh)(Y,Z)-(\nabla_Yh)(X,Z)</script><p>Equation (12) is called <strong>the Gauss equation</strong> and equation (13) is called <strong>the Codazzi equation</strong>. We can rewrite the Gauss equation as </p><script type="math/tex; mode=display">\tilde{R}(X,Y,Z,W)=R(X,Y,Z,W)+\langle h(X,Z),h(Y,W)\rangle-\langle h(Y,Z),h(X,W)\rangle</script><p>Especially the sectional curvature can be expressed as </p><script type="math/tex; mode=display">\tilde{K}(X,Y)=K(X,Y)-\langle h(X,X),h(Y,Y)\rangle+\|h(X,Y)\|^2</script><p>If the ambient space is Euclidean, then $\overline{R}$ vanishes identically. Let $e<em>i,e_j$ span a two-plane $\pi</em>{ij}$, then the sectional curvature of $\pi_{ij}$ is </p><script type="math/tex; mode=display">K(\pi_{ij})=-\|h(e_i,e_j)\|^2+\langle h(e_i,e_i),h(e_j,e_j)\rangle=\sum_{\alpha=1}^{d-m}(-(A_{\alpha i}^j)^2+A_{\alpha i}^iA_{\alpha j}^j)</script><p>From $A<em>\alpha$ we extract the $2\times 2$ submatrix $A</em>\alpha|<em>{\pi</em>{ij}}$ with $i$th and $j$th row and column. Then </p><script type="math/tex; mode=display">K(\pi_{ij})=\sum_{\alpha=1}^{d-m}\det(A_\alpha|_{\pi_{ij}})</script><p><strong>Example.</strong> If $M$ is a hypersurface of $\mathbb{E}^d$ with a unit normal vector field $\xi$, the sectional curvature is $K(\pi<em>{ij})=\det(A</em>\xi|<em>{\pi</em>{ij}})$. If $e<em>1,\cdots,e</em>{d-1}$ is a basis that diagonalizes the shape operator with eigenvalues $\lambda<em>1,\cdots,\lambda</em>{d-1}$, then $K(\pi<em>{ij})=\lambda_i\lambda_j$.  The eigenvalues are called <strong>principal curvature</strong> and eigenvectors are called <strong>principal directions</strong>. The determinant of $A</em>\xi$ is called <strong>the Gauss-Kronecker curvature</strong>.</p><p>Similarly we can differentiate the Weingarten formula, which yields</p><script type="math/tex; mode=display">\begin{aligned}\tilde{\nabla}_X\tilde{\nabla}_Y\xi&=-\tilde{\nabla}_XA_\xi(Y)+\tilde{\nabla}_X\nabla^\perp_Y\xi\\&=-\nabla_X A_\xi(Y)-h(X,A_\xi(Y))-A_{\nabla^\perp_Y\xi}(X)+\nabla^\perp_X\nabla^\perp_Y\xi\end{aligned}</script><p>Define the curvature tensor on the normal bundle by $R^\perp(X,Y)\xi=[\nabla^\perp<em>X,\nabla^\perp_Y]\xi-\nabla^\perp</em>{[X,Y]}\xi$, and define the covariant differentiation of the shape operator by $(\nabla<em>XA)</em>\xi(Y)=\nabla<em>X(A</em>\xi Y)-A<em>{\nabla^\perp_X\xi}(Y)-A</em>\xi(\nabla_XY)$. We obtain that</p><script type="math/tex; mode=display">\begin{aligned}\tilde{R}(X,Y)\xi=&R^\perp(X,Y)\xi-(\nabla_X A)_\xi(Y)+(\nabla_Y A)_\xi(X)\\&-h(X,A_\xi(Y))+h(Y,A_\xi(X))\end{aligned}</script><p>The tangent component is </p><script type="math/tex; mode=display">(\tilde{R}(X,Y)\xi)^\top=-(\nabla_X A)_\xi(Y)+(\nabla_Y A)_\xi(X)</script><p>while the normal component is </p><script type="math/tex; mode=display">(\tilde{R}(X,Y)\xi)^\perp=R^\perp(X,Y)\xi+h(X,A_\xi(Y))-h(Y,A_\xi(X))</script><p>Equation (19) is called <strong>the Ricci equation</strong>. Note that equation (18) is equivalent to the Codazzi equation as follows</p><script type="math/tex; mode=display">\begin{aligned}\langle\tilde{R}(X,Y)\xi,Z\rangle=&\langle-(\nabla_X A)_\xi(Y),Z\rangle+\langle(\nabla_Y A)_\xi(X),Z\rangle\\=&\langle\nabla_X(A_\xi Y)-A_{\nabla^\perp_X\xi}(Y)-A_\xi(\nabla_XY),Z\rangle\\&+\langle\nabla_Y(A_\xi X)-A_{\nabla^\perp_Y\xi}(X)-A_\xi(\nabla_YX),Z\rangle\\=&-\langle\nabla^\perp_X\xi,h(Y,Z)\rangle-\langle\nabla^\perp_Y\xi,h(X,Z)\rangle-\langle h(\nabla_XY,Z),\xi\rangle\\&-\langle h(\nabla_YX,Z),\xi\rangle+X\langle h(Y,Z),\xi\rangle-\langle h(\nabla_XZ,Y),\xi \rangle\\&+Y\langle h(X,Z),\xi\rangle-\langle h(\nabla_YZ,X),\xi\rangle\\=&\langle\nabla_X^\perp h(Y,Z),\xi\rangle+\langle\nabla_Y^\perp h(X,Z),\xi\rangle-\langle h(\nabla_XY,Z),\xi\rangle\\&-\langle h(\nabla_YX,Z),\xi\rangle-\langle h(\nabla_XZ,Y),\xi \rangle-\langle h(\nabla_YZ,X),\xi\rangle\\=&\langle(\nabla_X h)(Y,Z)-(\nabla_Y h)(X,Z),\xi\rangle\\=&\langle\tilde{R}(X,Y)Z,\xi\rangle\end{aligned}</script><p>The three equations (Gauss, Codazzi, Ricci) are called fundamental equations for a submanifold $M\hookrightarrow\tilde{M}$. </p><h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><h3 id="Planar-Curves"><a href="#Planar-Curves" class="headerlink" title="Planar Curves"></a>Planar Curves</h3><p>A planar curve is a 1-dimensional manifold embedded in the 2-plane. Let $\mathbf{t}$ be the tangent vector field and $\mathbf{n}$ be the normal vector field. We have </p><script type="math/tex; mode=display">A_{\mathbf{n}}\mathbf{t}=-(\overline{D}_{\mathbf{n}}\mathbf{t})^\top=\kappa \mathbf{t}</script><p>where $\kappa$ is the curvature. Thus the estimation can be simply given by </p><script type="math/tex; mode=display">\hat{A}=\frac{\Delta\mathbf{n}\cdot\mathbf{t}}{\Delta p\cdot\mathbf{t}}\approx \kappa</script><h3 id="Space-Curves"><a href="#Space-Curves" class="headerlink" title="Space Curves"></a>Space Curves</h3><p>A space curve is a 1-dimensional manifold embedded in 3-space. Let $\mathbf{t}$ be the tangent vector field, $\mathbf{n}$ be the normal vector field, and $\mathbf{b}$ be the binormal vector field. We have the Frenet formula</p><script type="math/tex; mode=display">\frac{d}{ds}\left(\begin{array}{c}\mathbf{t}\\ \mathbf{n}\\ \mathbf{b}\end{array}\right)=\left(\begin{array}{ccc}0&\kappa&0\\ -\kappa&0&\tau\\ 0&-\tau&0\end{array}\right)\left(\begin{array}{c}\mathbf{t}\\ \mathbf{n}\\ \mathbf{b}\end{array}\right)</script><p>where $\kappa$ is curvature and $\tau$ is torsion. Let $\xi=\cos(\theta)\mathbf{n}+\sin(\theta)\mathbf{b}$ be a unit normal vector field. We have</p><script type="math/tex; mode=display">A_\xi\mathbf{t}=\cos(\theta)A_\mathbf{n}\mathbf{t}+\sin(\theta)A_\mathbf{b}\mathbf{t}=\cos(\theta)\kappa\mathbf{t}</script><p>Similarly, let $\xi_\perp=-\sin(\theta)\mathbf{n}+\cos(\theta)\mathbf{b}$ be the unit normal vector perpendicular to $\xi$. Then</p><script type="math/tex; mode=display">A_{\xi_\perp}\mathbf{t}=-\sin(\theta)\kappa\mathbf{t}</script><p>By definition the mean vector field is $H=\cos(\theta)\kappa\xi-\sin(\theta)\kappa\xi_\perp=\kappa\mathbf{n}$. The mean curvature is $|H|=\kappa$.</p><h3 id="Surfaces"><a href="#Surfaces" class="headerlink" title="Surfaces"></a>Surfaces</h3><p>A surface is a 2-dimensional manifold embedded in 3-space. The shape operator coincides with the differential of Gauss map (with an additional minus sign). Thus the norm of mean curvature vector $|H|$ is in fact the absolute value of mean curvature for surfaces. The sectional curvature is Gaussian curvature for surfaces. </p><h3 id="Clifford-Torus"><a href="#Clifford-Torus" class="headerlink" title="Clifford Torus"></a>Clifford Torus</h3><p>Let $f:\mathbb{R}^2\to\mathbb{R}^4$ be defined by </p><script type="math/tex; mode=display">f(\theta,\phi)=\frac{1}{\sqrt{2}}(\cos(\sqrt{2}\theta),\sin(\sqrt{2}\theta),\cos(\sqrt{2}\phi),\sin(\sqrt{2}\phi))</script><p>The image of $f$ is $\mathbb{S}^1(\sqrt{1/2})\times\mathbb{S}^1(\sqrt{1/2})$, hence a torus. By computation the tangent vector fields are</p><script type="math/tex; mode=display">\begin{aligned}& f_\theta = (-\sin(\sqrt{2}\theta),\cos(\sqrt{2}\theta),0,0)\\& f_\phi = (0,0,-\sin(\sqrt{2}\phi),\cos(\sqrt{2}\phi))\end{aligned}</script><p>The Riemannian metric is given by </p><script type="math/tex; mode=display">g = d\theta^2+d\phi^2</script><p>which is a flat metric, implying the sectional curvature is identically zero. The normal vector fields are given by</p><script type="math/tex; mode=display">\begin{aligned}& \xi = \frac{1}{\sqrt{2}}(-\cos(\sqrt{2}\theta),-\sin(\sqrt{2}\theta),\cos(\sqrt{2}\phi),\sin(\sqrt{2}\phi))\\& \nu =\frac{1}{\sqrt{2}}(\cos(\sqrt{2}\theta),\sin(\sqrt{2}\theta),\cos(\sqrt{2}\phi),\sin(\sqrt{2}\phi))\end{aligned}</script><p>By definition the shape operators are given by </p><script type="math/tex; mode=display">\begin{aligned}&A_\xi[f_\theta,f_\phi]=[f_\theta,f_\phi]\left[\begin{array}{cc}1&0\\0&-1\end{array}\right]\\&A_\nu[f_\theta,f_\phi]=[f_\theta,f_\phi]\left[\begin{array}{cc}-1&0\\0&-1\end{array}\right]\end{aligned}</script><p>Therefore, the mean curvature vector field is $H=-\nu$ and the mean curvature is $1$.</p><h3 id="Rotation-Group"><a href="#Rotation-Group" class="headerlink" title="Rotation Group"></a>Rotation Group</h3><p>Let $SO(2)$ be the rotation group which consists of matrices in the form</p><script type="math/tex; mode=display">\left[\begin{array}{cc}\cos(\theta)&-\sin(\theta)\\ \sin(\theta)& \cos(\theta)\end{array}\right]</script><p>which can be viewed as a curve in the 4-space. Consider the parametrization</p><script type="math/tex; mode=display">f(\theta)=(\cos(\theta),\sin(\theta),-\sin(\theta),\cos(\theta))</script><p>Direct computation shows that the tangent vector field is </p><script type="math/tex; mode=display">e_\theta=1/\sqrt{2}(-\sin(\theta),\cos(\theta),-\cos(\theta),-\sin(\theta))</script><p>and consider the following basis of normal space</p><script type="math/tex; mode=display">\begin{aligned}& \xi_1=(\cos(\theta),\sin(\theta),0,0)\\& \xi_2=(0,0,\sin(\theta),-\cos(\theta))\\& \xi_3=1/\sqrt{2}(-\sin(\theta),\cos(\theta),\cos(\theta),\sin(\theta))\end{aligned}</script><p>Then the shape operators are</p><script type="math/tex; mode=display">A_{\xi_1}e_\theta=-1/\sqrt{2}e_\theta,A_{\xi_2}e_\theta=1/\sqrt{2}e_\theta,A_{\xi_3}e_\theta=0</script><p>Hence the mean vector field is $H(\theta)=-1/\sqrt{2}\xi_1+1/\sqrt{2}\xi_2=-1/\sqrt{2}f(\theta)$. </p><h3 id="Ellipsoid"><a href="#Ellipsoid" class="headerlink" title="Ellipsoid"></a>Ellipsoid</h3><p>Let $F:\mathbb{R}^{n+1}\to\mathbb{R}$ be the function </p><script type="math/tex; mode=display">F(x^1,\cdots,x^{n+1})=\sum_{i=1}^{n+1}(\frac{x^i}{a^i})^2-1</script><p>Since $\nabla F(x)\neq 0$ for all $x$ such that $F(x)=0$, the level set $M=F^{-1}(0)$ is a $n$-dimensional submanifold. $\nabla F/|\nabla F|$ serves as the unit normal vector field on $M$. The mean curvature is given by the following formula</p><script type="math/tex; mode=display">H=-\frac{1}{n}div(\frac{\nabla F}{\|\nabla F\|})</script>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Submanifolds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Random Walk and Graph Spectra</title>
      <link href="/2020/03/27/Random-Walk-and-Graph-Spectra/"/>
      <url>/2020/03/27/Random-Walk-and-Graph-Spectra/</url>
      
        <content type="html"><![CDATA[<p>Random walks arise in many fields in mathematics and physics, recently in network analysis, graph learning, clustering etc. It also has a natural relation with spectral graph theory. The <a href="http://web.cs.elte.hu/~lovasz/erdos.pdf" target="_blank" rel="noopener">survey</a> written by L.Lovasz presents many beautiful results concerning random walks and graph spectra. We aim to write down some computation details missing in the original succinct paper.  <a id="more"></a></p><h2 id="Basic-Notions"><a href="#Basic-Notions" class="headerlink" title="Basic Notions"></a>Basic Notions</h2><p>Let $G=(V,E)$ be a connected, undirected, simple graph with $n$ nodes and $m$ edges. Consider the following random walk on $G$: start from a node $v_0$; at the $t$th step move to the neighbor of $v_t$ with probability $1/d(v_t)$. i.e. It is a Markov chain with transition matrix </p><script type="math/tex; mode=display">M=D^{-1}A</script><p>where $A$ is the adjacency matrix and $D$ is the diagonal matrix such that $D<em>{ii}=d(i)=\sum_jA</em>{ij}$ ($\textbf{Caveat:}$ in original paper $D$ is adopted as $D^{-1}$ here, thus all the indices over $D$ are reversed in this note). Let $P_t$ be the distribution at time $t$. Then </p><script type="math/tex; mode=display">(P_t)_i=\sum_j (P_{t-1})_jM_{ji}</script><p>If we assume $P_t$’s are column vectors, the matrix form should be written as </p><script type="math/tex; mode=display">P_t=M^TP_{t-1}</script><p>A random walk is called stationary if $P_0=P_1$. We denote the stationary random walk by $\pi$. A simple calculation shows that $\pi=D\mathbf{1}/2m$ is a stationary distribution of $M$. The uniqueness and convergence are shown by the following.</p><h2 id="Stationary-Distribution"><a href="#Stationary-Distribution" class="headerlink" title="Stationary Distribution"></a>Stationary Distribution</h2><p>Before discussing about the stationary distribution of the random walk $M$, we state the <a href="[https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem](https://en.wikipedia.org/wiki/Perron–Frobenius_theorem">Perron-Frobenius theorem</a>) which reveals some important properties about nonnegative matrices.</p><p>A nonnegative matrix $A$ is called irreducible if there does <strong>NOT</strong> exist a permutation matrix $R$ such that </p><script type="math/tex; mode=display">RAR^{-1}=\left[\begin{array}{cc}E&F\\ 0&G\end{array}\right]</script><p>For an irreducible matrix $A$, the spectral radius $\rho(A)=r&gt;0$ is an eigenvalue of $A$, whose corresponding (left /right) eigenspace is unidimensional. Moreover, $A$ has an (left/right) eigenvector $v$ associated with $r$ whose components are all positive, and conversely,  eigenvectors whose components are all positive are associated with $r$.  </p><p>Consider the special case where $A$ is the adjacency matrix of an undirected simple graph. $A$ is irreducible if and only if there does not exist a nontrivial partition ${1,2,\cdots, n}=I\cup J$ such that $A_{ij}=0$ for $i\in I$ and $j\in J$. i.e. $A$ is irreducible if and only if the graph $G$ is connected. This criterion easily generalizes to any similarity matrices on graphs. For example, consider $N=D^{-1/2}AD^{-1/2}$. It is irreducible if and only if $G$ is connected. Since $N$ is symmetric, write $N$ in spectral form </p><script type="math/tex; mode=display">N=\sum_k\lambda_kv_kv_k^T</script><p>where $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_n$ are eigenvalues of $N$ and $v_1,\cdots,v_n$ are corresponding  eigenvectors of unit length. Note that $A\mathbf{1}=D\mathbf{1}$, which gives $ND^{1/2}\mathbf{1}=D^{1/2}\mathbf{1}$. i.e. $D^{1/2}\mathbf{1}$ is an eigenvector of $N$ whose components are all positive. By Perron-Frobenius theorem, $v_1=\frac{D^{1/2}\mathbf{1}}{\sqrt{2m}}$ and $\lambda_1=1&gt;\lambda_2\ge\cdots\ge\lambda_n\ge-1$. </p><p>A simple calculation shows that $M=D^{-1/2}ND^{1/2}$ is similar to $N$. Now we have</p><script type="math/tex; mode=display">M^t=D^{-1/2}v_1v_1^TD^{1/2}+\sum_{k\ge 2}\lambda_k^tD^{-1/2}v_kv_k^TD^{1/2}=Q+\Lambda^t</script><p>where $Q_{ij}=d(j)/2m=\pi(j)$ is such that $Q^TP=\pi$ for any probability distribution $P$. Hence if we can prove $\Lambda^t\to 0$ as $t\to\infty$ we obtain that $\pi$ is the stationary distribution. In addition, since $M$ is similar to $N$ and $\lambda_1=1$ is a simple eigenvalue of $N$, we see that $\pi$ will be the only probability distribution such that $M^T\pi=\pi$. Apparently, it suffices to exclude the case where $\lambda_n=-1$.</p><p>Let $L=D-A$ be the Laplacian matrix, $\mathcal{L}=D^{-1/2}LD^{-1/2}$ be the normalized Laplacian. Note that $\mathcal{L}=I-N$. Thus, $\lambda_n(N)=1-\lambda_1(\mathcal{L})$. By Rayleigh-Ritz theorem, </p><script type="math/tex; mode=display">\lambda_1(\mathcal{L})=\sup_f\frac{f^T\mathcal{L}f}{f^Tf}</script><p>Set $g=D^{-1/2}f$. Then by the fact $(g_i-g_j)^2\le 2(g_i^2+g_j^2)$,</p><script type="math/tex; mode=display">\lambda_1(\mathcal{L})=\sup_g\frac{1/2\sum_{i,j}a_{ij}(g_i-g_j)^2}{\sum_ig_i^2d_i}\le\sup_g\frac{\sum_{i,j}a_{ij}(g_i^2+g_j^2)}{\sum_ig_i^2d_i}=2</script><p>where equality holds if and only if $g<em>i=-g_j$ if $a</em>{ij}\neq0$. Since $g\neq 0$, the components of $g$ is partitioned into two parts. Hence $\lambda_n(N)=-1$ if and only if $\lambda_1(\mathcal{L})=-2$ if and only if $G$ is bipartite. </p><p>Overall we proved the following claim about stationary distribution.</p><blockquote><p>Let $G$ be an undirected, simple, connected, non-bipartite graph, with transition matrix $M=D^{-1}A$. It possesses a unique stationary distribution $\pi=D\mathbf{1}/2m$ such that $M^T\pi=\pi$ and $\lim_{t\to\infty}P_t=\pi$ for any initial distribution $P_0$.</p></blockquote><h2 id="Spectra-and-Access-Time"><a href="#Spectra-and-Access-Time" class="headerlink" title="Spectra and Access Time"></a>Spectra and Access Time</h2><p>The access time (or hitting time) $H_{ij}$ is the expected number of steps the first time meeting node $j$ starting from node $i$. i.e. </p><script type="math/tex; mode=display">H_{ij}=\sum n\times \text{Prob(meet node j in n steps)}</script><p>By definition $H_{ii}=0$. For $i\neq j$, by conditional expectation formula</p><script type="math/tex; mode=display">H_{ij}=\frac{1}{d_i}(\sum_{k\sim i}1+H_{kj})=1+\frac{1}{d_i}\sum_{k\sim i}H_{kj}</script><p>Let $J=\mathbf{1}\mathbf{1}^T$ be the all-one matrix. According to the above equation, the matrix $F=J+MH-H$ is a diagonal matrix. Direct computation shows that $F^T\pi=\mathbf{1}$. Hence $F=2mD^{-1}$. The access time matrix satisfies </p><script type="math/tex; mode=display">(I-M)H=J-2mD^{-1}</script><p>By the above discussion, the kernel of $I-M$ is unidimensional. If $X$ is a solution of (11), then $X+\mathbf{1}a^T$ gives all the solutions of (11). By the restriction $H_{ii}=0$, we can solve for the unique $H$. Let $V=[v_1,\cdots,v_n]$ be the orthogonal matrix such that </p><script type="math/tex; mode=display">N=V\left[\begin{array}{ccc}\lambda_1&&\\&\ddots&\\&&\lambda_n\end{array}\right]V^T</script><p>Set $U=D^{-1/2}V$. Then we have </p><script type="math/tex; mode=display">I-M=U\left[\begin{array}{ccc}0&&\\&\ddots&\\&&1-\lambda_n\end{array}\right]U^{-1}</script><p>Note the right side of (11) has nothing to do with $\lambda_i$. We try to get rid of these eigenvalues. Suppose the solution $X$ has a factor </p><script type="math/tex; mode=display">U\left[\begin{array}{ccc}0&&\\&\ddots&\\&&\frac{1}{1-\lambda_n}\end{array}\right]U^{-1}</script><p>The multiplication with (13) gives </p><script type="math/tex; mode=display">\sum_{k\ge 2}D^{-1/2}v_kv_k^TD^{1/2}=-D^{-1/2}v_1v_1^TD^{1/2}+I=JD/2m+I</script><p>In addition, multiplying (15) with $J-2mD^{-1}$ gives</p><script type="math/tex; mode=display">(\frac{JD}{2m}+I)(J-2mD^{-1})=J-2mD^{-1}</script><p>Therefore one solution of (11) is</p><script type="math/tex; mode=display">\begin{aligned}X&=U\left[\begin{array}{ccc}0&&\\&\ddots&\\&&\frac{1}{1-\lambda_n}\end{array}\right]U^{-1}(J-2mD^{-1})\\&=D^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})D^{1/2}\mathbf{1}\mathbf{1}^T-2mD^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})D^{-1/2}\\&=\sqrt{2m}D^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})v_1\mathbf{1}^T-2mD^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})D^{-1/2}\\&=-2mD^{-1/2}(\sum_{k\ge 2}\frac{v_kv_k^T}{1-\lambda_k})D^{-1/2}\end{aligned}</script><p>Finally, we have to subtract the diagonals of $X$. Since </p><script type="math/tex; mode=display">X_{ii}=-2m\sum_{k\ge 2}\frac{v_{ki}^2}{(1-\lambda_k)d_i}</script><p>The access time matrix is $H=X-\mathbf{1}[X<em>{11},\cdots,X</em>{nn}]^T$. The elements are</p><script type="math/tex; mode=display">H_{ij}=2m\sum_{k\ge 2}\frac{1}{1-\lambda_k}(\frac{v_{kj}^2}{d_j}-\frac{v_{ki}v_{kj}}{\sqrt{d_id_j}})</script><p>This equation has many applications. For example, we can quickly find the symmetric properties of $H$. </p><blockquote><p>For any three nodes $u,v$ and $w$, $H<em>{uv}+H</em>{vw}+H<em>{wu}=H</em>{uw}+H<em>{wv}+H</em>{vu}$.</p></blockquote><p>Direct computation shows that the sum $H<em>{uv}+H</em>{vw}+H_{wu}$ is </p><script type="math/tex; mode=display">\sum_{k\ge 2}\frac{m}{1-\lambda_k}\left((\frac{v_{ku}}{\sqrt{d_u}}-\frac{v_{kv}}{\sqrt{d_v}})^2+(\frac{v_{kv}}{\sqrt{d_v}}-\frac{v_{kw}}{\sqrt{d_w}})^2+(\frac{v_{kw}}{\sqrt{d_w}}-\frac{v_{ku}}{\sqrt{d_u}})^2\right)</script><p>which is invariant under permutation.</p><blockquote><p>Let $\pi$ be the stationary walk. Then $\sum<em>j\pi_jH</em>{ij}=\sum_{k\ge 2}\frac{1}{1-\lambda_k}$.</p></blockquote><p>Use the fact that $\pi=D\mathbf{1}/2m$ and $v_1=D^{1/2}\mathbf{1}/\sqrt{2m}$ which is orthogonal to all $v_k$. In the summation,</p><script type="math/tex; mode=display">\sum_j\pi_jH_{ij}=\sum_{k\ge 2}\frac{1}{1-\lambda_k}(\sum_jv_{kj}^2-\frac{v_{ki}}{\sqrt{d_i}}\sum_jv_{kj}v_{1j}\sqrt{2m})=\sum_{k\ge 2}\frac{1}{1-\lambda_k}</script><p>Note that this equality is independent of the starting point. Thus we can explain the formula as follows: in a stationary walk, no matter where we start on the graph, the average number of steps before meeting all the other nodes is a fix number which related to the graph spectra. Thus this formula nicely connects random walk with geometry of the graph.</p><blockquote><p>Let $\pi$ be the stationary walk. Then $\sum<em>i\pi_iH</em>{ij}\ge \frac{(1-\pi_j)^2}{\pi_j}$</p></blockquote><p>Using again $v_1$ is orthogonal to $v_k$, we have</p><script type="math/tex; mode=display">\sum_i\pi_iH_{ij}=\frac{2m}{d_j}\sum_{k\ge 2}\frac{1}{1-\lambda_k}v_{kj}^2</script><p>By Cauchy-Schwartz inequality,</p><script type="math/tex; mode=display">\sum_{k\ge 2}\frac{v_{kj}^2}{1-\lambda_k}\sum_{k\ge 2}(1-\lambda_k)v_{kj}^2\ge(\sum_{k\ge 2}v_{kj}^2)^2</script><p>Note that $\sum<em>{k\ge 2}v</em>{kj}^2=1-\pi(j)$, and $\sum<em>{k\ge2}(1-\lambda_k)v</em>{kj}^2=\sum<em>{k}(1-\lambda_k)v</em>{kj}^2=1-N_{jj}=1$. Thus we obtain the claim.</p><p>To give an example where the access time is calculable, consider the $k$-cube $Q_k$. Let $\mathbf{0}=(0,0,\cdots,0)$ and $\mathbf{1}=(1,1,\cdots,1)$. The nodes are elements in ${0,1}^k$ and two nodes are connected by an edge if they differ by one digit (see <a href="https://en.wikipedia.org/wiki/Hypercube_graph" target="_blank" rel="noopener">hypercube graph</a> ). To find all the eigenvalues and eigenvectors of $M$ (or $A$ since $D$ is a scalar matrix), L.Lovasz’s paper gives a beautiful construction: for every vector $b\in{0,1}^k$, define $(v_b)_x=(-1)^{b\cdot x}$. i.e. the component of $v_b$ at node $x$ is $1$ if $b$ and $x$ share even number of $1$’s  and is $0$ otherwise. Note that </p><script type="math/tex; mode=display">\sum_ja_{ij}(v_b)_j=\sum_{j:|i-j|=1}(-1)^{b\cdot j}</script><p>Now if $j$ changes one digit of $i$ where the component of $b$ is $0$, $b\cdot j=b\cdot i$. This gives $k-b\cdot\mathbf{1}$ number of $(-1)^{b\cdot i}$. Otherwise  (the component of $b$ is $1$) each change from $j$ will yields $b\cdot j=b\cdot i\pm 1$, giving $b\cdot\mathbf{1}$ number of $-(-1)^{b\cdot i}$. Hence $v_b$ is an eigenvector of $M$ with eigenvalue $1-\frac{2}{k}b\cdot\mathbf{1}$. Normalizing $v_b$ and using the formula for $H$ we have </p><script type="math/tex; mode=display">\begin{aligned}H(\mathbf{0},\mathbf{1})&=\sum_{b\in\{0,1\}^k,b\neq 0}\frac{k}{b\cdot\mathbf{1}}(1-(-1)^{b\cdot\mathbf{1}})\\&=k\sum_{j=1}^kC_k^j\frac{1}{2j}(1-(-1)^j)\\&\ge \frac{k}{2(k+1)}\sum_{j=1}^{k}C_{k+1}^{j+1}(1-(-1)^j)\\&=\frac{k}{k+1}(2^k-1)\end{aligned}</script><p>To obtain an upper bound, use the identity $C<em>k^j=\sum</em>{p=0}^{k-1}C_p^{j-1}$, where $C_p^{j-1}=0$ for $p&lt;j-1$. We have </p><script type="math/tex; mode=display">\begin{aligned}H(\mathbf{0},\mathbf{1})&=k\sum_{j=1}^k\sum_{p=0}^{k-1}C_{p}^{j-1}\frac{1}{2j}(1-(-1)^j)\\&=k\sum_{j=1}^k\sum_{p=0}^{k-1}\frac{1}{2(p+1)}C_{p+1}^{j}(1-(-1)^j)\\&=k\sum_{p=0}^{k-1}\frac{1}{2(p+1)}\sum_{j=1}^kC_{p+1}^j(1-(-1)^j)\\&= k\sum_{p=0}^{k-1}\frac{1}{2(p+1)}2^{p+1}\\&\le k2^{k/2-1}\sum_{p<k/2-1}\frac{1}{1+p}+2\sum_{p\ge k/2-1}2^p\\&\le 2^{k+1}+k\log k2^{k/2}\end{aligned}</script><p>Thus for large $k$ the exact value of $H(\mathbf{0},\mathbf{1})$ is between $2^k$ and $2^{k+1}$.</p><h2 id="Spectra-and-Mixing-Rate"><a href="#Spectra-and-Mixing-Rate" class="headerlink" title="Spectra and Mixing Rate"></a>Spectra and Mixing Rate</h2><p>Let $\lambda=\max{|\lambda_2|,|\lambda_n|}$. By equation (6) we have</p><script type="math/tex; mode=display">P_t=(M^T)^tP_0=Q^TP_0+(\Lambda^T)^tP_0=\pi+(\Lambda^T)^tP_0</script><p>For a random walk starting at node $i$, i.e. $P_0(i)=1$, we have</p><script type="math/tex; mode=display">P_t(j)=\pi(j)+\sum_{k\ge 2}\lambda_k^t\sqrt{\frac{d(j)}{d(i)}}v_{kj}v_{ki}</script><p>Thus by geometric-arithmetic inequality,</p><script type="math/tex; mode=display">|P_t(j)-\pi(j)|\le\lambda^t\sqrt{\frac{d(j)}{d(i)}}\sum\frac{v_{kj}^2+v_{ki^2}}{2}\le \sqrt{\frac{d(j)}{d(i)}}\lambda^t</script><p>Moreover, for any subset $S\subset{1,\cdots,n}$, by Cauchy inequality $\sum<em>{s\in S}\sqrt{d(s)}v</em>{ks}\le\sqrt{\sum<em>{s\in S}d(s)}\sqrt{\sum</em>{s\in S}v_{ks}}$, we have</p><script type="math/tex; mode=display">|P_t(S)-\pi(S)|\le \frac{|S|+1}{2}\sqrt{\frac{\pi(S)}{\pi(i)}}\lambda^t</script><p>Note that $P_t(j)$ is nothing but the probability of transferring from node $i$ to node $j$ in $t$ steps. Define the mixing rate</p><script type="math/tex; mode=display">\mu=\limsup_{t\to\infty}\max_{i,j}|M^t(i,j)-\pi(j)|</script><p>From equation (25) we immediately have $\mu=\lambda$. Thus we obtain</p><blockquote><p>The mixing rate of a random walk on a non-bipartite graph is $\lambda=\max{|\lambda_2|,|\lambda_n|}$.</p></blockquote><p>To get rid of $\lambda_n$, consider the following lazy random walk</p><script type="math/tex; mode=display">M_L=\frac{1}{2}I+\frac{1}{2}D^{-1}A=\frac{1}{2}(I+M)</script><p>That is, we have half of the probability to stay rather than move, which is also equal to adding $d(i)$ loops at each node $i$. Set $N_L=D^{-1/2}(D+A)D^{-1/2}$. We have $D^{1/2}M_LD^{-1/2}=\frac{1}{2}N_L=\frac{1}{2}(I+N)$. If $Nv=\lambda v$, it holds $M_LD^{-1/2}v=\frac{1+\lambda}{2}D^{-1/2}v$. Thus we have $\lambda(M_L)=(1+\lambda(M))/2\ge0$. Furthermore,</p><script type="math/tex; mode=display">M_L^t=D^{-1/2}v_1v_1^TD^{1/2}+\sum_{k\ge 2}(\frac{1+\lambda_k(M)}{2})^tD^{-1/2}v_kv_k^TD^{1/2}</script><p>we see that lazy random walk always converges to the stationary distribution, with rate $(1+\lambda_2(M))/2&gt;\lambda_2$, which makes sense that lazy walk is slower.</p><p> Thus the mixing rate is closely related to $\lambda_2$, or equivalently, the spectral gap $\lambda_1-\lambda_2=1-\lambda_2$. Let $\emptyset\neq S\subset V$ and $\nabla(S)$ be the set of edges connecting $S$ to $V\backslash S=\bar{S}$, i.e. $\nabla(S)$ is the edge cut of the graph with respect to $S$ (see this <a href="http://yueqicao.top/2020/03/10/Spectral-Graph-Partitioning/" target="_blank" rel="noopener">post</a> ). Define the conductance of the set $S$ by </p><script type="math/tex; mode=display">\Phi(S)=\frac{|\nabla(S)|}{2m\pi(S)\pi(V\backslash S)}</script><p>Here $\pi(S)=\sum_{s\in S}d(s)/2m=vol(S)/2m$. The conductance of the graph is defined by </p><script type="math/tex; mode=display">\Phi=\min_S\Phi(S)</script><p>It turns out that the spectral gap is controlled by the graph conductance. </p><blockquote><p>$\frac{\Phi^2}{16}\le 1-\lambda_2\le \Phi$</p></blockquote><p>Recall that the normalized Laplacian is $\mathcal{L}=D^{-1/2}LD^{-1/2}=I-N$, which implies $\lambda(\mathcal{L})=1-\lambda(N)=1-\lambda(M)$. The second largest eigenvalue of $M$ is the second smallest eigenvalue of $\mathcal{L}$. By Rayleigh-Ritz theorem,</p><script type="math/tex; mode=display">1-\lambda_2=\min_{f\perp D^{1/2}\mathbf{1}}\frac{f^T\mathcal{L}f}{f^Tf}</script><p>Let $g=D^{-1/2}f$ and set $g^TDg=1$. By the basic property of graph Laplacian,</p><script type="math/tex; mode=display">1-\lambda_2=\min\{\sum_{i\sim j}(x_i-x_j)^2:\sum_i d(i)x_i=0,\sum_i d(i)x_i^2=1\}</script><p>Let $S$ be the set with minimum conductance, and consider the vector given by </p><script type="math/tex; mode=display">x_i=\left\{\begin{array}{l}a,i\in S\\ b,i\in\bar{S}\end{array}\right.</script><p>where</p><script type="math/tex; mode=display">a=\sqrt{\frac{\pi(\bar{S})}{2m\pi(S)}},b=-\sqrt{\frac{\pi(S)}{2m\pi(\bar{S})}}</script><p>Then</p><script type="math/tex; mode=display">\sum_{i\sim j}(x_i-x_j)^2=\sum_{i\in S,j\in\bar{S}}\frac{\pi(\bar{S})}{2m\pi(S)}+\frac{\pi(S)}{2m\pi(\bar{S})}+\frac{1}{m}=\frac{|\nabla(S)|}{2m\pi(S)\pi(\bar{S})}=\Phi</script><p>This proves the upper bound. To obtain the lower bound, we first prove the following lemma</p><blockquote><p>Let $y\in\mathbb{R}^{|V|}$ such that $\pi({i:y<em>i&gt;0})\le 1/2$, $\pi({i:y_i&lt;0})\le 1/2$ and $\sum_i\pi(i)|y_i|=1$. Then $\sum</em>{i\sim j}|y_i-y_j|\ge m\Phi$. </p></blockquote><p>Sort the vector $y$ such that $y<em>1\le y_2\le\cdots\le y_t&lt;0=y</em>{t+1}=\cdots=y<em>s&lt;y</em>{s+1}\le\cdots y<em>n$. Consider the sets defined by $S_i={1,\cdots,i}$. For a fixed node $i$, in the sum $\sum</em>{i\sim j}|y<em>j-y_i|$ the quantity $y</em>{i+1}-y<em>i$ is counted for $|\nabla(S_i)|$ times if we substitute $|y_j-y_i|$ by $y</em>{j}-y<em>{j-1}+\cdots+y</em>{i+1}-y_i$. We have </p><script type="math/tex; mode=display">\sum_{i\sim j}|y_i-y_j|=\sum_{i=1}^{n-1}|\nabla(S_i)|(y_{i+1}-y_i)\ge2m\Phi\sum_{i=1}^{n-1}(y_{i+1}-y_i)\pi(S_i)\pi(\bar{S}_i)</script><p>Since $\pi(S_i)\le 1/2$ for $i\le t$ and $\pi(S_i)\ge 1/2$ for $i\ge s+1$, we have </p><script type="math/tex; mode=display">\begin{aligned}\sum_{i\sim j}|y_i-y_j|&\ge m\Phi\sum_{i=1}^t(y_{i+1}-y_i)\pi(S_i)+m\Phi\sum_{i=s}^{n-1}(y_{i+1}-y_i)\pi(\bar{S}_i)\\&=m\Phi\sum_{i=1}^t(-y_i)\pi(i)+m\Phi\sum_{i=s+1}^{n}y_i\pi(i)\\&=m\Phi\sum_{i=1}^n|y_i|\pi(i)=m\Phi\end{aligned}</script><p>Return to the proof of lower bound. Let $x$ be any vector satisfying </p><script type="math/tex; mode=display">\sum_i d(i)x_i=0,\sum_id(i)x_i^2=1</script><p>Assume $x<em>1\ge \cdots\ge x_n$. Let $k$ be the index such that $\pi({1,\cdots,k-1})\le 1/2$ and $\pi({1,\cdots,k})&gt;1/2$. Therefore, $x_1-x_k\ge\cdots\ge x</em>{k-1}-x<em>k\ge0\ge x</em>{k+1}-x_k\ge\cdots\ge x_n-x_k$. Set $z_i=\max{0,x_i-x_k}$. Then $z_i=0$ for $i\ge k$. </p><script type="math/tex; mode=display">\sum_i \pi_i(x_i-x_k)^2=\sum_{i}\pi_iz_i^2+\sum_{i>k}\pi_i(x_i-x_k)^2</script><p>By replacing $x$ with $-x$ we may assume $\sum<em>{i}\pi_iz_i^2\ge\sum</em>{i&gt;k}\pi_i(x_i-x_k)^2$. Thus we have</p><script type="math/tex; mode=display">\sum_i\pi_iz_i^2\ge \frac{1}{2}\sum_i\pi_i(x_i-x_k)^2\ge\frac{1}{4m}</script><p>Apply the above lemma to $y_i=z_i^2/\sum_i\pi_iz_i^2$. We obtain</p><script type="math/tex; mode=display">\sum_{i\sim j}|z^2_i-z_j^2|\ge m\Phi\sum_i\pi_iz_i^2</script><p>On the other hand, by Cauchy inequality</p><script type="math/tex; mode=display">(\sum_{i\sim j}|z_i^2-z_j^2|)^2\le(\sum_{i\sim j}(z_i-z_j)^2)(\sum_{i\sim j}(z_i+z_j)^2)</script><p>Furthermore,</p><script type="math/tex; mode=display">\sum_{i\sim j}(z_i+z_j)^2\le 2\sum_{i\sim j}z_i^2+z_j^2=\sum_{i,j}a_{ij}(z_i^2+z_j^2)=4m\sum_i\pi_iz_i^2</script><p>Combining these results together, we have</p><script type="math/tex; mode=display">\sum_{i\sim j}(z_i-z_j)^2\ge \frac{m^2\Phi^2}{4m}\sum_i\pi_iz_i^2\ge\frac{\Phi^2}{16}</script><p>The theorem follows from the fact that $\sum<em>{i\sim j}(x_i-x_j)^2\ge\sum</em>{i\sim j}(z_i-z_j)^2$.</p><p>We remark that conductance is similar to what is so called <a href="https://en.wikipedia.org/wiki/Cheeger_constant_(graph_theory" target="_blank" rel="noopener">Cheeger constant</a>) defined as </p><script type="math/tex; mode=display">h_G=\min_S\frac{|\nabla(S)|}{2m\min\{\pi(S),\pi(\bar{S})\}}</script><p>the lower bound lemma goes without modification for $h_G$. Thus we can feel free to  replace $\Phi$ with $h_G$.</p>]]></content>
      
      
      <categories>
          
          <category> Network-Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Random Walk </tag>
            
            <tag> Spectral Graph Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spectral Graph Partitioning</title>
      <link href="/2020/03/10/Spectral-Graph-Partitioning/"/>
      <url>/2020/03/10/Spectral-Graph-Partitioning/</url>
      
        <content type="html"><![CDATA[<p>Informally, the problem of graph partitioning, or graph clustering, can be stated as follows: we want to partition a graph into several parts such that there are fewer edges between groups but more edges within groups. If there is a measurement on edges, i.e. the edges are weighted, then it can also be stated that we want a partition that nodes within groups are more similar. Graph partitioning can be encountered in many areas such as <a href="http://www.stat.cmu.edu/~larry/=sml/" target="_blank" rel="noopener">statistical learning</a>, <a href="http://www.leonidzhukov.net/hse/2015/networks/" target="_blank" rel="noopener">network analysis</a>, <a href="https://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">graph learning</a>. Other than data science, it is also related to graph theory and computer vision. One recalls that there is a classical problem called <a href="https://en.wikipedia.org/wiki/Minimum_cut" target="_blank" rel="noopener">minimum cut</a> in graph theory, which can be solved in polynomial time by the <a href="[https://en.wikipedia.org/wiki/Stoer%E2%80%93Wagner_algorithm](https://en.wikipedia.org/wiki/Stoer–Wagner_algorithm">Stoer-Wagner algorithm</a>). Graph partitioning can be regarded as a strengthened min-cut problem, since we not only want to find the minimal cut but also an ‘equitable’ partition.</p><p> <a href="https://en.wikipedia.org/wiki/Spectral_clustering" target="_blank" rel="noopener">Spectral clustering</a> is one of the most popular approaches to solve graph partitioning. It has a long history which can date back to 1970s and it is popularized in data science as a machine learning method. Results obtained by spectral clustering often outperform traditional methods. Moreover, it is very simple to implement and the mathematics is nothing but standard linear algebra. A comprehensive <a href="https://arxiv.org/pdf/0711.0189.pdf" target="_blank" rel="noopener">tutorial</a> written by U. von Luxburg provides many different aspects to understand spectral clustering including random walk, perturbation theory and so on. Here we may cover the basics from the easiest way: linear algebra. <a id="more"></a></p><p><img src="/2020/03/10/Spectral-Graph-Partitioning/demo.jpg" alt="demo"></p><p>Suppose we want to partition the graph into two parts. Let $A\subset V$ be a subset and $\bar{A}$ be the compliment of $A$ in $V$. Define the (unnormalized) graph cut to be </p><script type="math/tex; mode=display">cut(A,\bar{A})=\sum_{i\in A,j\in\bar{A}}w_{ij}</script><p>where $w<em>{ij}$ is the weight of edge $e</em>{ij}$. $w_{ij}=0$ if and only if $i$ and $j$ are not connected. Let $s$ be the indicator vector such that if $v_i\in A$ then $s_i=1$ and if $v_i\notin A$ then $s_i=-1$. We can rewrite graph cut as</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}cut(A,\bar{A})&=\frac{1}{4}\sum_{i\in A,j\in\bar{A}}w_{ij}(s_i-s_j)^2\\&=\frac{1}{8}\sum_{i,j}w_{ij}(s_i-s_j)^2\\&=\frac{1}{4}\sum_{i,j}((\sum_{j}w_{ij})s_i^2\delta_{ij}-w_{ij}s_is_j)\end{aligned}\end{equation}</script><p>Let $D$ be the diagonal matrix such that $d<em>{ii}=\sum</em>{j}w<em>{ij}$. The graph Laplacian is defined as $L=D-W$. From the computation we see that $s^TLs=\frac{1}{2}\sum</em>{ij}w_{ij}(s_i-s_j)^2$ for any vector $s$ and the graph cut is $cut(A,\bar{A})=\frac{1}{4}s^TLs$. By construction $\sum_i s_i^2=n$. In addition, we want ‘equitable’ partition. Therefore we have the so called balanced cut constraint $\sum_i s_i=0$. i.e. $|A|=|\bar{A}|$. This integer minimization problem is, however, NP-hard. An accurate solution is not attainable. So we need to find ways to approximate the solution. One way is to relax the indicator vector $s$ to be any vector $x\in\mathbb{R}^n$ satisfying the constraints. Then we can deal with a much easier continuous optimization problem.</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\min\quad& Q(x)=\frac{1}{4}x^TLx\\s.t.\quad& \sum_i x_i=0,\sum_i x_i^2=n\end{aligned}\end{equation}</script><p>Let $\mathbf{1}$ be the constant vector. The first constraint $\sum_i x_i=0$ is equivalent to the condition $x\perp \mathbf{1}$. The second constraint is equivalent to that $x$ is on a sphere. Then optimization is equivalent to  </p><script type="math/tex; mode=display">\begin{equation}\min_{x\perp \mathbf{1}}\frac{x^TLx}{x^Tx}\end{equation}</script><p>By Rayleigh-Ritz theorem, we just pick the eigenvector with respect to the second least eigenvalue (note that $\mathbf{1}$ corresponds to the eigenvalue 0 of $L$). </p><p>The last step is to revert the eigenvector $x$ to the indicator vector $s$. A simple criterion is to map positive numbers to $1$ and negative numbers to $-1$. In general, one can apply $k$-means clustering to $x$ (here $k=2$), which is a common method if we want to obtain a $k$ partition.</p><p>The relaxation from integral optimization to real optimization seems to weaken the restriction $|A|=|\bar{A}|$. For integral optimization it is equivalent to $\sum s_i=0$. However, for real optimization $\sum x_i=0$ may not imply $|{x_i&gt;0}|=|{x_i&lt;0}|$. One way to overcome this shortcoming is to place the balance restriction to the objective function. We define the ratio cut </p><script type="math/tex; mode=display">\begin{equation}Rcut(A,\bar{A})=\frac{cut(A,\bar{A})}{|A|}+\frac{cut(A,\bar{A})}{|\bar{A}|}\end{equation}</script><p>Define the indicator vector $f=(f_1,\cdots,f_n)$ such that </p><script type="math/tex; mode=display">\begin{equation}f_i=\left\{\begin{array}{l}\sqrt{|\bar{A}|/|A|},v_i\in A\\ -\sqrt{|A|/|\bar{A}|},v_i\in\bar{A} \end{array}\right.\end{equation}</script><p>Then the ratio cut is equivalent to </p><script type="math/tex; mode=display">\begin{equation}Rcut(A,\bar{A})=\frac{1}{n}f^TLf\end{equation}</script><p>By definition $f$ satisfies </p><script type="math/tex; mode=display">\begin{equation}\sum_i f_i=0, \|f\|=\sqrt{n}\end{equation}</script><p>When we use the same relaxation method we can find that the problem is the same as (3). Therefore, we have not made any progress than what we have done at first.</p><p>However, one can easily generalize the ratio cut to arbitrary $k$-partition. We define</p><script type="math/tex; mode=display">\begin{equation}Rcut(A_1,\cdots, A_k)=\sum_{i=1}^k\frac{cut(A_i,\bar{A}_i)}{|A_i|}\end{equation}</script><p>Unlike the previous methods, we define $k$ indicator vectors $h^1,\cdots, h^k$. For each $h^i$, $h^i<em>j=1/\sqrt{|A_i|}$ if $v_j\in A_i$ and $h^i_j=0$ otherwise. Let $H=[h^1,\cdots,h^k]$ be the $n\times k$ matrix. It satisfies $H^TH=I</em>{k\times k}$. A direct computation shows that </p><script type="math/tex; mode=display">\begin{equation}Rcut(A_1,\cdots,A_k)=Tr(H^TLH)\end{equation}</script><p>Therefore, we come to the optimization </p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\min\quad& Tr(H^TLH)\\s.t\quad& H^TH=I_{k\times k}\end{aligned}\end{equation}</script><p>By Rayleigh-Ritz theorem, the minimizer is the matrix consisting the eigenvectors corresponding to the $k$ least eigenvalues. The last problem is that how to revert the matrix $H$ to partitions of the graph. View $H$ as $n$ points in the $k$ plane $\mathbb{R}^k$. That is, we construct a ‘dimension reduction’ map $v_i\to H_i$. Then we can cluster the points using $k$-means clustering.</p><p>In ratio cut, the size of partition is measured by the number of vertices. One can also measure the size by the total weights of edges. Let $vol(A)=\sum<em>{i\in A}d</em>{ii}$. The normalized cut is </p><script type="math/tex; mode=display">\begin{equation}Ncut(A_i,\cdots,A_k)=\sum_{i=1}^k\frac{cut(A_i,\bar{A}_i)}{vol(A_i)}\end{equation}</script><p>Similarly, we define $k$ indicator vectors $h^1,\cdots, h^k$ such that $h^i_j=1/\sqrt{vol(A_i)}$ for $v_i\in A_i$ and 0 otherwise. Let $H=[h^1,\cdots,h^k]$. Then $H^TDH=I$, and</p><script type="math/tex; mode=display">\begin{equation}Ncut(A_1,\cdots,A_k)=Tr(H^TLH)\end{equation}</script><p>Let $T=D^{1/2}H$ and $L_s=D^{-1/2}LD^{-1/2}$ be the normalized graph Laplacian. We want to solve</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\min\quad& Tr(T^TL_sT)\\s.t\quad& T^TT=I_{k\times k}\end{aligned}\end{equation}</script><p>which is in the same form as ratio cut. To revert to the discrete case, we apply $k$ means clustering to $H$. </p><p>One remarkable thing about normalized cut is that it has a beautiful explanation from the perspective of random walk. Let $p<em>{ij}=w</em>{ij}/d_i$ be the one-step transition probability from node $i$ to node $j$. If the graph is connected and non-bipartite, this random walk will always possess a stationary distribution $\pi$ such that $\pi_i=d_i/vol(V)$. Let $P(A|B)=P(v_i\in A|v_j\in B)$ denote the probability that points in $B$ walk to $A$ in one step. Then </p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}P(A,\bar{A})&=\sum_{i\in A,j\in \bar{A}}\pi_ip_{ij}\\&=\sum_{i\in A,j\in \bar{A}}\frac{d_i}{vol(V)}\frac{w_{ij}}{d_i}\\&=\sum_{i\in A,j\in \bar{A}}\frac{w_{ij}}{vol(V)}\end{aligned}\end{equation}</script><p>Therefore,</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}P(A|\bar{A})&=\frac{P(A,\bar{A})}{P(\bar{A})}\\&=\frac{\sum_{i\in A,j\in\bar{A}}w_{ij}/vol(V)}{vol(A)/vol(V)}\\&=\sum_{i\in A,j\in\bar{A}}\frac{w_{ij}}{vol(A)}\\&=\frac{cut(A,\bar{A})}{vol(A)}\end{aligned}\end{equation}</script><p>We see that $Ncut(A,\bar{A})=P(A|\bar{A})+P(\bar{A}|A)$. When we are minimizing the normalized cut we are trying to find a partition such that points have low probability that transfer from one community to the other.</p><p><img src="/2020/03/10/Spectral-Graph-Partitioning/spec.jpg" alt="spec"></p>]]></content>
      
      
      <categories>
          
          <category> Network-Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spectral Graph Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Forman-Ricci Curvature on Complexes</title>
      <link href="/2020/02/16/Forman-Ricci-Curvature-on-Complexes/"/>
      <url>/2020/02/16/Forman-Ricci-Curvature-on-Complexes/</url>
      
        <content type="html"><![CDATA[<h2 id="Bochner-Weitzenb-bf-ddot-o-ck-Formula-on-Riemannian-Manifolds"><a href="#Bochner-Weitzenb-bf-ddot-o-ck-Formula-on-Riemannian-Manifolds" class="headerlink" title="Bochner-Weitzenb$\bf\ddot{o}$ck Formula on Riemannian Manifolds"></a>Bochner-Weitzenb$\bf\ddot{o}$ck Formula on Riemannian Manifolds</h2><p>Let $(M,g)$ be an $n$-dimensional Riemannian manifold, and $\nabla$ be the Levi-Civita connection. Recall that for a smooth function (or 0-form) $f\in C^\infty(M)=\mathcal A^0(M)$, the <em>Laplacian of smooth functions</em> is defined as </p><script type="math/tex; mode=display">\Delta_0(f)=\text{tr}\nabla^2(f)=\text{div}(\text{grad}(f))</script><p>The definition $\Delta_0=\text{tr}\nabla^2$ can be generalized to arbitrary $p$-forms and is called the <em>connection Laplacian</em>. Suppose in addition $M$ is oriented. Let $d:\mathcal A^p(M)\to\mathcal A^{p+1}(M)$ be the differential operator. We can define the adjoint operator $d^{*}$ of $d$ using the <em>Hodge star operator</em>. i.e. $d^{*}:\mathcal A^{p+1}(M)\to\mathcal A^p(M)$ is the operator satisfying $\langle d\alpha,\beta\rangle=\langle\alpha,d^{*}\beta\rangle=\int_Md\alpha\wedge {*}\beta$ for $\alpha\in\mathcal A^p(M)$ and $\beta\in\mathcal A^{p+1}$. The <em>Hodge Laplacian</em> is defined by </p><script type="math/tex; mode=display">\Delta=dd^*+d^*d</script><p>By direct computation, we can see that $\Delta=-\Delta_0$ on $C^\infty(M)=\mathcal A^0(M)$. However, it is much more complicated in higher dimensions.<a id="more"></a> The <em>Bochner-Weitzenb$\ddot{o}$ck formula</em> tells that, on $\mathcal A^p(M)$ for $p&gt;0$, a term involving curvature tensor should be added. More precisely, we have </p><script type="math/tex; mode=display">\Delta=-\Delta_0+\sum_{i,j}\omega^i\wedge i_{E_j}R_{E_iE_j}</script><p>where $E<em>1,\cdots,E_n$ is a local orthonormal frame and $w^1,\cdots,\omega^n$ is the corresponding coframe. $i</em>{E<em>j}$ is the interior multiplication and $R</em>{E<em>iE_j}=\nabla</em>{[E<em>i,E_j]}-[\nabla</em>{E<em>i},\nabla</em>{E_j}]$ is the curvature tensor. For 1-forms the following equation is commonly presented in textbooks.</p><script type="math/tex; mode=display">\frac{1}{2}\Delta_0|\omega|^2=|\nabla\omega|^2-\langle\Delta\omega,\omega\rangle+\text{Ric}(\omega_*,\omega_*)</script><p>Using this formula one is able to prove the following Bochner’s theorem</p><blockquote><p>Let $M$ be a compact, oriented Riemannian manifold, whose Ricci curvature tensor is nonnegative and positive at one point. Then $H^1(M;\mathbb R)$=0.</p></blockquote><p>From Hodge theorem it suffices to prove that every harmonic 1-form is zero. Let $\omega$ be a harmonic 1-form. Integrate equation (4) on both sides and note that on any compact manifold we have $\int_M \Delta_0(f)dv=0$. We have </p><script type="math/tex; mode=display">\nabla\omega=0 \text{ and } \text{Ric}(\omega_*,\omega_*)=0</script><p>For any vector field $X$ we have $X|\omega|^2=2\langle\nabla<em>X\omega,\omega\rangle=0$. Hence $|\omega|$ is constant. If $\omega\neq 0$ at one point then $\omega\neq 0$ on $M$. But $\text{Ric}$ is positive at one point which means at this point $\text{Ric}(\omega</em>*,\omega_*)&gt;0$, which is a contradiction.</p><p>This ‘standard’ proof is not the same as Bochner’s original proof. In fact, Bochner defined Laplacian in the following way. Since $\nabla:\Gamma(\bigwedge^*(M))\to\Gamma(\bigwedge^*(M)\otimes T^*M)$ is a linear map between inner product spaces, it admits an adjoint $\nabla^*$. The <a href="https://en.wikipedia.org/wiki/Laplace_operators_in_differential_geometry" target="_blank" rel="noopener"><em>Bochner Laplacian</em></a> is defined by $\Delta_B=\nabla^*\nabla$, which is also called <em>rough Laplacian</em> in literature (see <a href="https://www.springer.com/gp/book/9783540653172" target="_blank" rel="noopener">Berger</a>). It is easy to see that $\Delta_B$ is nonnegative definite and $\text{Ker}(\Delta_B)=\text{Ker}(\nabla)$ whose elements are parallel forms. By computation one verifies that $\Delta_B=-\Delta_0$. Therefore,</p><script type="math/tex; mode=display">\Delta=\Delta_B+\text{Curv}(R)</script><p>If $\text{Curv}(R)$ is nonnegative, then $\text{Ker}(\Delta)=\text{Ker}(\Delta_B)\cap\text{Ker}(\text{Curv}(R))$. But parallel forms are completely determined at one point. Therefore, if $\text{Curv}(R)$ is positive at one point, the harmonic forms will be identically zero. </p><h2 id="Forman’s-Discretization-of-Ricci-Curvature"><a href="#Forman’s-Discretization-of-Ricci-Curvature" class="headerlink" title="Forman’s Discretization of Ricci Curvature"></a>Forman’s Discretization of Ricci Curvature</h2><p>Bochner’s proof is carefully studied by <a href="https://link.springer.com/article/10.1007%2Fs00454-002-0743-x" target="_blank" rel="noopener">Robin Forman</a> so as to place a combinatorial analogy in discrete cases. In 2003, a series of combinatorial invariants for quasiconvex CW complexes were proposed, named ‘curvature’ by Forman. In the simplest case, suppose $M$ is a simplicial complex. For each nonnegative integer $p$, define the $p$th curvature function by </p><script type="math/tex; mode=display">\mathcal F_p(\alpha)=\#\{(p+1)\text{-cofaces}\}+\#\{(p-1)\text{-faces}\}-\#\{\text{parallel neighbors}\}</script><p>where parallel neighbors of $\alpha$ are $p$-simplices sharing either a $(p+1)$-coface or $(p-1)$-face but not both. For $p=1$, $\mathcal F_1$ is called Ricci curvature by Forman, denoted by $\text{Ric}$. Using this analogy Forman could prove several Bochner-type theorems. For example, one has the following</p><blockquote><p>Let $M$ be a connected weighted quasiconvex CW complex with nonnegative Ricci curvature. Suppose, in addition, there exists a vertex $v$ such that $\text{Ric}(e)&gt;0$ for each coface $e$ of $v$. Then $H_1(M,\mathbb R)=0$.</p></blockquote><p>Forman’s idea comes from a simple observation. Though the classical Bochner-Weitzenb$\rm\ddot{o}$ck formula looks rather abstruse, with many abstract operators on Riemannian manifolds involved, but from the simplest perspective, one has</p><script type="math/tex; mode=display">\text{Laplacian}=\text{nonnegative definite operator}+\text{curvature}</script><p>For a simplicial complex $M$ the discrete Laplacian is well defined. i.e. Let $\partial<em>p:C_p(M;\mathbb R)\to C</em>{p-1}(M;\mathbb R)$ be the $p$th boundary operator. Place a metric on each chain vector space by declaring the simplices are orthogonal (<em>It is not necessary that they are orthonormal. Note that when simplices are assigned weights the adjoint operator is no longer the transpose</em>).  The boundary operator admits an adjoint $\partial<em>p^*:C</em>{p-1}(M;\mathbb R)\to C<em>p(M;\mathbb R)$, such that $\langle\partial_p\alpha,\beta\rangle</em>{p-1}=\langle\alpha,\partial_p^*\beta\rangle_p$. The <em>combinatorial Laplacian</em> is defined by </p><script type="math/tex; mode=display">\square_p=\partial_{p+1}\partial_{p+1}^*+\partial_p^*\partial_p:C_p(M;\mathbb R)\to C_p(M;\mathbb R)</script><p>From equation (8) we know we want to decompose $\square_p$ as a sum of a nonnegative definite matrix $B_p$ and a curvature-type matrix $F_p$. In fact we only need to construct $B_p$, since we do not know any property of $F_p$ so that we just put $F_p=\square_p-B_p$.</p><p>The way Forman defined $B_p$ is quite natural. Suppose $A$ is a symmetric matrix. Then </p><script type="math/tex; mode=display">\mathbb{B}(A)=\left\{\begin{array}{l}A_{ij},\text{ for }i\neq j\\ \sum_{j\neq i} |A_{ij}|,\text{ for }i=j\end{array}\right.</script><p>is a diagonally dominant matrix, thus is nonnegative definite. $\mathbb B(A)$ is called the Bochner matrix associated to $A$. Thereafter $\mathbb F(A)=A-\mathbb B(A)$ is called the curvature matrix associated to $A$. From this perspective we see that ‘curvature’ measures how <strong>a symmetric matrix deviates from a diagonally dominant matrix</strong>. This decomposition is useful in our case because $\mathbb B(\square)$ preserves the topological (or combinatorial) information of the simplicial complex $M$. Specifically, if $\alpha$ and $\beta$ are parallel neighbors, then $\square<em>{\alpha\beta}=\mathbb B(A)</em>{\alpha\beta}\neq 0$ by checking definition. Let $B$ be a symmetric $n\times n$ matrix. Define an equivalence relation on ${1,2,\cdots,n}$ by requiring $i\sim i$ and $i\sim j$ if and only if there is a sequence $i=k<em>0,k_1,\cdots,k_n=j$ such that $B(k_l,k</em>{l+1})\neq 0$. Let $\mathcal C(B)$ be the set of equivalence classes and $\mathcal N(B)=|\mathcal C(B)|$. The following nontrivial property of diagonally dominant matrices is used by Forman to prove Bochner’s theorem for 1-chains.</p><blockquote><p>Let $B$ be a diagonally dominant metrix, then</p><ol><li>$\text{dim(ker)}(B)\le \mathcal N(B)$;</li><li>Suppose $v=(v<em>1,\cdots,v_n)\in\text{ker}(B)$, if $B</em>{ij}\neq 0$, $v<em>j=-sign(B</em>{ij})v_i$. i.e. the components in the same equivalence class are completely determined at one element.</li></ol></blockquote><p>It suffices to prove 2. Let $c\in\mathcal C(B)$ be any class. Without loss of generality, assume $v<em>i=\max</em>{j\in c}|v_j|\ge0$. Then we have </p><script type="math/tex; mode=display">0=(Bv)_i=\sum_{j}B_{ij}v_j=B_{ii}v_i+\sum_{j\in c,j\neq i}B_{ij}v_j\ge\sum_{j\in c,j\neq i}|B_{ij}|(v_i-|v_j|)\ge0</script><p>The equality holds if and only if $v<em>j=0$ for all $j\in c$,  or, $B</em>{ii}=\sum<em>{j\neq i}|B</em>{ij}|$ and $v<em>j=-sign(B</em>{ij})v_i$ for all $j\neq c$. </p><p>Recall that for graph Laplacian $L=D-A$ the dimension of kernel is always equal to the connected components. This is because the constant vector $\mathbf{1}$ provides a nontrivial element in the kernel if the graph is connected. However, one can easily construct an invertible matrix $B$ with the diagonal equal to the sum of off-diagonal elements. Thus the first inequality can be strict.</p><p>Let us see how this property can be used in our case. Suppose $\alpha$ and $\beta$ are parallel $p$-neighbors, which implies $\square<em>p(\alpha,\beta)\neq 0$ and thus $\mathbb{B}(\square_p)(\alpha,\beta)\neq 0$. If there is a $p$-chain $c=\sum c</em>\gamma\gamma\in\text{Ker}(\mathbb{B}(\square<em>p))$ with $c</em>\alpha=0$, then $c_\beta=0$. This continuation property will be important in the proof of Bochner-type theorems.</p><p>Now suppose $\mathbb{F}(\square<em>p)$ is nonnegative definite, which is equivalent to say $\mathbb{F}(\square_p)</em>{ii}\ge 0$. Being a sum of two nonnegative definite matrices, we have $\text{Ker}(\square_p)=\text{Ker}(\mathbb{B}(\square_p))\cap\text{Ker}(\mathbb{F}(\square_p))$. In the simplest case, suppose $\mathbb{F}(\square_p)$ is positive definite, then $\text{Ker}(\square_p)={0}$. For $p=1$, we have the familiar statement: if $M$ has positive Ricci curvature (at present call $\mathbb{F}(\square_1)$ the Ricci curvature, which is positive if the diagonal elements are positive), then $H_1(M;\mathbb R)$ is trivial. </p><p>More generally, assume $M$ has nonnegative Ricci curvature and there is a vertex $v$ such that $\text{Ric}(e)&gt;0$ for all $e\succ v$. Let $c=\sum c_e e\in\text{Ker}(\square_1)=\text{Ker}(\mathbb{B}(\square_1))\cap\text{Ker(Ric)}$. Since $c\in\text{Ker(Ric)}$ we have $c_e=0$ for all $e\succ v$. The following lemma shows that $c$ will be identically zero.</p><blockquote><p>Suppose $c=\sum c_e e$ is a 1-chain such that $c\in\text{Ker}(\partial^*)\cap\text{Ker}(\mathbb{B}(\square_1))$. In addition, there is a vertex $v$ with $c_e=0$ for all $e\succ v$. Then $c=0$.</p></blockquote><p>Define $D:{\text{1-simplices}}\to\mathbb{Z}_{\ge 0}$ as: (1). $D(e)=0$ for $e\succ v$; (2). Inductively, if $D(e)$ is greater than $k$ and there is a 1-simplex $e_1$ such that $e\cap e_1\neq\emptyset$ and $D(e_1)=k$, set $D(e)=k+1$. </p><p>From hypothesis $c<em>e=0$ for $D(e)=0$. Suppose $c_e=0$ for all $D(e)\le k$. Let $e$ be a 1-simplex with $D(e)=k+1$. Then there exists 1-simplex $e_1$ such that $D(e_1)=k$ and $e\cap e_1\neq \emptyset$. If $e$ and $e_1$ are parallel, by the continuation property of $\mathbb{B}(\square_1)$, $c_e=0$. If $e$ and $e_1$ are not parallel, there is a 2-simplex $f$ such that $f\succ e$ and $f\succ e_1$. Let the last edge of $f$ be $e_2$. Since $D(e_1)=k$, either $D(e)=k$ or $D(e_2)=k$. By assumption $D(e_2)=k$, thus $c</em>{e<em>1}=c</em>{e_2}=0$. Then </p><script type="math/tex; mode=display">\langle\partial f,c\rangle=\langle f,\partial^*c\rangle=0=\pm c_e\langle e,e\rangle</script><p>Hence $c=0$ by induction.</p><p>Now we give the explicit representation of $\square_p$ and the curvature function $\mathcal F_p$.</p><p>For each $(p+1)$-simplex $\beta$ with $\partial\beta=\sum<em>{\alpha\prec\beta}\epsilon</em>{\alpha\beta}\alpha$ where $\epsilon<em>{\alpha\beta}=\pm 1$ according to the orientation of $\beta$. By definition, $\langle\partial^*\alpha,\beta\rangle=\langle\alpha,\partial\beta\rangle=\epsilon</em>{\alpha\beta}w<em>\alpha$ where we assume that $\langle\alpha,\alpha\rangle=w</em>\alpha$. Thus $\partial^*\alpha=\sum<em>{\beta\succ\alpha}\epsilon</em>{\alpha\beta}\frac{w<em>\alpha}{w</em>\beta}\beta$. This gives</p><script type="math/tex; mode=display">\square_p\alpha_1=\sum_{\alpha_2}[\sum_{\beta\succ\alpha_1,\alpha_2}\epsilon_{\alpha_1\beta}\epsilon_{\alpha_2\beta}\frac{w_{\alpha_1}}{w_\beta}+\sum_{\gamma\prec\alpha_1,\alpha_2}\epsilon_{\gamma\alpha_1}\epsilon_{\gamma\alpha_2}\frac{w_\gamma}{w_{\alpha_2}}]\alpha_2</script><p>We may work on orthonormal basis. Note that for different basis the definition of curvature will be different. Let $\alpha^*=\alpha/\sqrt{w_\alpha}$. Then </p><script type="math/tex; mode=display">\langle\square_p\alpha_1^*,\alpha_2^*\rangle=\sum_{\beta\succ\alpha_1,\alpha_2}\epsilon_{\alpha_1\beta}\epsilon_{\alpha_2\beta}\frac{\sqrt{w_{\alpha_1}w_{\alpha_2}}}{w_\beta}+\sum_{\gamma\prec\alpha_1,\alpha_2}\epsilon_{\gamma\alpha_1}\epsilon_{\gamma\alpha_2}\frac{w_\gamma}{\sqrt{w_{\alpha_2}w_{\alpha_1}}}</script><p>Denote by $\square_p(\alpha_1^*,\alpha_2^*)$. Then under orthonormal basis, the curvature matrix is </p><script type="math/tex; mode=display">\mathbb{F}(\square_p)(\alpha_1^*,\alpha_2^*)=\left\{\begin{array}{l}0,\text{ if }\alpha_1\neq\alpha_2\\ \square_p(\alpha_1^*,\alpha_2^*)-\sum_{\alpha^*\neq\alpha_2^*}|\square_p(\alpha^*,\alpha_1^*)|,\text{ if }\alpha_1=\alpha_2\end{array}\right.</script><p>Define the $p$th curvature function by $\mathcal{F}<em>p(\alpha)=w</em>\alpha\mathbb{F}(\square_p)(\alpha^*,\alpha^*)$. Thus</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{F}_p(\alpha)=w_\alpha(&\sum_{\beta\succ\alpha}\frac{w_\alpha}{w_\beta}+\sum_{\gamma\prec\alpha}\frac{w_\gamma}{w_\alpha}\\ &-\sum_{\eta\neq\alpha}|\sum_{\beta\succ\alpha,\eta}\epsilon_{\alpha\beta}\epsilon_{\eta\beta}\frac{\sqrt{w_{\alpha}w_{\eta}}}{w_\beta}+\sum_{\gamma\prec\alpha,\eta}\epsilon_{\gamma\alpha}\epsilon_{\gamma\eta}\frac{w_\gamma}{\sqrt{w_{\alpha}w_{\eta}}}|)\end{aligned}</script><p>When $p=1$ and all weights are 1, the formula simplifies as what is given in the beginning. </p>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Forman Curvature </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Local-to-Global Convexity</title>
      <link href="/2020/01/26/Local-to-Global-Convexity/"/>
      <url>/2020/01/26/Local-to-Global-Convexity/</url>
      
        <content type="html"><![CDATA[<h2 id="Convex-Functions"><a href="#Convex-Functions" class="headerlink" title="Convex Functions"></a>Convex Functions</h2><p>Recall that a function $f:I\to \mathbb R$ is called convex if </p><script type="math/tex; mode=display">f(tx+(1-t)y)\le tf(x)+(1-t)f(y)</script><p>for any $t\in[0,1]$ and $x,y\in I$ where $I$ is an interval in $\mathbb R$. In some <a href="https://arxiv.org/pdf/1601.03363.pdf" target="_blank" rel="noopener">context</a>, $f$ is called $p$-convex if $f^p$ is convex in the meaning of (1). $f$ is called quasi-convex if </p><script type="math/tex; mode=display">f(t)\le\max\{f(a),f(b)\}</script><p>for any $t\in [a,b]$. Note that if $f$ is convex then $f$ is necessarily quasi-convex. Quasi-convex functions are also called peakless in <a href="https://core.ac.uk/display/56641235" target="_blank" rel="noopener">Busemann’s articles</a>.</p><p>If $f$ is differentiable, then we have a simple rule to determine its convexity: $f$ is convex if and only if $f’$ is non-decreasing. If $f$ is twice differentiable, then $f$ is convex if and only if $f’’$ is non-negative. From these rules we can see that convexity is a <strong>local</strong> property for differentiable functions. i.e. say that $f$ is locally convex if for any $x$ there is a neighborhood such that $f$ is convex on that neighborhood. If $f$ is differentiable and locally convex, by the above discussions locally convexity implies global convexity.</p><a id="more"></a><p>The question is whether, without the presuppose of differentiability, local convexity implies global convexity. i.e if $f$ is defined on an interval such that any point has a neighborhood where condition (1) holds, is $f$ convex or not? Recall the following sufficient and necessary condition for $f$ being convex: $f$ is convex if and only if its epigraph is convex, where the epigraph of $f$ is defined by </p><script type="math/tex; mode=display">epi(f)=\{(t,y)|y\ge f(t)\}</script><p>We see that local convexity is equivalent to that each point in the epigraph has a neighborhood which is convex. Thus we come to the question: Let $X$ be a locally convex set in $\mathbb R^n$, is itself convex?</p><h2 id="Tietze-Nakajima-Theorem"><a href="#Tietze-Nakajima-Theorem" class="headerlink" title="Tietze-Nakajima Theorem"></a>Tietze-Nakajima Theorem</h2><p>The above question is answered by the classical <a href="https://arxiv.org/pdf/math/0701745.pdf" target="_blank" rel="noopener">Tietze-Nakajima theorem</a>.</p><blockquote><p>Let $X$ be a closed subset in $\mathbb R^n$. $X$ is convex if and only if $X$ is connected and locally convex.</p></blockquote><p>Necessity is clear since convexity implies connectedness and local convexity. Note that local convexity implies local path connectedness. Thus $X$ being connected is equivalent to $X$ being path connected. We can define a length structure on $X$ by defining admissible paths to be polygonal paths (the existence of polygonal paths is guaranteed by local convexity) and length to be the canonical length in Euclidean spaces. The induced metric on $X$ is <em>midpoint convex</em> (a special case of <em>Menger convex</em>). i.e. For any two point in $X$ there exists a midpoint (The existence of midpoint relies on the closeness and local convexity of $X$). Since $X$ is complete, by a theorem of <a href="https://www.amazon.com/Course-Metric-Geometry-Dmitri-Burago/dp/0821821296" target="_blank" rel="noopener">BBI</a>, $X$ is strictly intrinsic. i.e. the induced metric coincides with the usual Euclidean metric and there is a shortest path, which is clearly a straight line, connecting any two point. Therefore $X$ is convex.</p><p>Tietze-Nakajima theorem implies that the convexity of functions is a local property. In contrast, quasi-convexity is <strong>not</strong> a local property. i.e if a function is local quasi-convex, it may not be a quasi-convex function globally. For a counterexample, consider the exterior of ‘风’.</p><p>From the proof we see that the local-to-global convexity property for Euclidean spaces should have a natural generalization to distance geometry. However, one should be careful with the notion of ‘convexity’ and the choice of ‘proper’ spaces. See a series of questions discussed in StackExchange (<a href="https://math.stackexchange.com/questions/3517202/does-convexity-implies-contractibility-in-length-space" target="_blank" rel="noopener">Does convexity implies contractibility in length space?</a>; <a href="https://math.stackexchange.com/questions/479022/is-a-uniquely-geodesic-space-contractible-i" target="_blank" rel="noopener">Is a uniquely geodesic space contractible? I</a>; <a href="https://math.stackexchange.com/questions/481569/on-continuously-uniquely-geodesic-space" target="_blank" rel="noopener">On continuously uniquely geodesic space</a>; <a href="https://mathoverflow.net/questions/252605/are-small-varepsilon-balls-convex-in-geodesic-metric-spaces" target="_blank" rel="noopener">Are small $\epsilon$-balls convex in geodesic metric spaces?</a>)</p><h2 id="Generalized-Hadamard-Cartan-Theorem"><a href="#Generalized-Hadamard-Cartan-Theorem" class="headerlink" title="Generalized Hadamard-Cartan Theorem"></a>Generalized Hadamard-Cartan Theorem</h2><p>M. Gromov stated a theorem which generalized the classical <a href="https://en.wikipedia.org/wiki/Cartan%E2%80%93Hadamard_theorem" target="_blank" rel="noopener">Hadamard-Cartan theorem</a> in Riemannian geometry: </p><blockquote><p>A simply-connected, complete, locally convex geodesic space is globally convex; hence any two points are joined by a unique geodesic.</p></blockquote><p>A <strong>geodesic space</strong> is a space where any two points can be joined by a shortest path. A subset $A$ is <strong>convex</strong> if for any two scaled geodesics $\alpha,\beta:[0,1]\to A$ the function $d(\alpha(t),\beta(t))$ is convex. If $m<em>{pq}$ denotes the midpoint of a geodesic from $p$ to $q$. Then convexity is equivalent to $2d(m</em>{pq},m_{pr})\le d(q,r)$ for any three distinct points. Thus the generalized Hadamard-Cartan theorem shows that local-to-global convexity holds for simply-connected, complete, geodesic spaces. A detailed proof of this theorem is given by <a href="https://www.researchgate.net/publication/257947117_The_Hadamard-Cartan_theorem_in_locally_convex_metric_spaces" target="_blank" rel="noopener">S. Alexander and R. Bishop</a> in 1990. </p><p>Let $m\in X$ be a fixed point in the geodesic space. Let $\mathbb G_m$ be the space of geodesics starting at $m$, carrying the uniform distance $\mathbf d$. $m$ has no <em>conjugate points</em> if the endpoint map </p><script type="math/tex; mode=display">EP:\mathbb G_m\to X</script><p>defined by $EP(\gamma)=\gamma(1)$, is a local homeomorphism. i.e. $EP$ maps a neighborhood of each $\gamma$ homeomorphically onto a neighborhood of $\gamma(1)$. That is to say, each point has a neighborhood such that the geodesics vary continuously. The first theorem shown by AB is that </p><blockquote><p> A locally convex, complete geodesic space has no conjugate point.</p></blockquote><p>Using local convexity, it is easy to prove that $EP$ is an isometry on a neighborhood of $\gamma$ onto <strong>its image</strong>. The question is that whether this image is a neighborhood of $\gamma(1)$. AB proved that, in fact, $EP$ is an isomorphism from a ball of $\gamma$ to a ball of $\gamma(1)$ by using an induction method.</p><p>$X$ is said to have neighborhoods of bipoint uniqueness if there are neighborhoods such that any two points of which is joined by a unique shortest geodesic varying continuously with its endpoints. If $X$ is locally convex, then $X$ has neighborhoods of bipoint uniqueness by the following argument.</p><p><img src="/2020/01/26/Local-to-Global-Convexity/bp.jpg" alt="bipoint uniqueness"></p><p>The following lemma says $EP$ is a covering map</p><blockquote><p>Let $\phi:\overline{M}\to M$ be a local isometry. $\overline{M}$ and $M$ are complete intrinsic metric spaces and $M$ has neighborhoods of bipoint uniqueness. Then $\phi$ is a covering map.</p></blockquote><p> Since $\mathbb G_m$ is contractible thus connected and simply connected, it suffices to define a complete intrinsic metric $\bar{\mathbf d}$ on $\mathbb G_m$ so that $(\mathbb G_m,\bar{\mathbf d})$ is complete and $EP$ is local isometry. One verifies that the metric induced by $\mathbf d$ is exactly $\bar{\mathbf d}$. </p><h2 id="CAT-kappa-Spaces"><a href="#CAT-kappa-Spaces" class="headerlink" title="CAT($\kappa$) Spaces"></a>CAT($\kappa$) Spaces</h2><p><a href="http://cncc.bingj.com/cache.aspx?q=CAT+Spaces&amp;d=4932082848371635&amp;mkt=en-US&amp;setlang=en-US&amp;w=xMBtoXdf-irC2l_HmVovaAaEXyBHa_yw" target="_blank" rel="noopener">CAT$(\kappa)$ spaces</a> are spaces with curvature bounded above in the Alexandrov sense. Let $x,y,z$ be distinct points in a length space $X$ and call $\Delta xyz$ a geodesic triangle if the segments $xy,yz,zx$ are geodesics. For any $\kappa\in\mathbb R$, the comparison space of curvature $\kappa$ is the unique simply-connected Riemannian 2-manifold of constant sectional curvature $\kappa$, denoted by $M<em>\kappa$. Define the diameter of $M</em>\kappa$ as </p><script type="math/tex; mode=display">D_\kappa=\left\{\begin{array}{l}\infty, \kappa\le 0\\ \pi/\sqrt{\kappa},\kappa>0\end{array}\right.</script><p>For a $D<em>\kappa$-geodesic space $X$ (i.e. points within distance $D</em>\kappa$ are joined by a unique geodesic), a geodesic triangle $\Delta$ is said to satisfy the CAT$(\kappa)$ inequality if there exists an expanding map $f:\Delta\to M<em>\kappa$ such that $f|</em>{\partial\Delta}$ is an isometry. Then $X$ is called a CAT$(\kappa)$ space if every geodesic triangle with perimeter less than $2D_\kappa$ satisfies the CAT$(\kappa)$ inequality.</p><p>A subset $A$ in the CAT$(\kappa)$ space $X$ is called convex if for any two points $x,y\in A$ within distance $D_\kappa$, the unique geodesic joining $x,y$ is contained in $A$. One easily verifies that these generalize the convexity in Euclidean spaces.</p><p><a href="https://arxiv.org/pdf/1304.4147.pdf" target="_blank" rel="noopener">This paper</a> shows that local-to-global convexity holds for all CAT$(\kappa)$ spaces. Specifically, the following theorem holds</p><blockquote><p>Let $A\subset X$ be a closed, connected, locally convex subset in CAT$(\kappa)$ space. Denote with $l$ the induced length metric in $A$. Then if $diam<em>l(A)\le D</em>\kappa$, $l$ coincides with the original length distance and $A$ is convex.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Metric Convexity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gromov访谈</title>
      <link href="/2020/01/16/Gromov%E8%AE%BF%E8%B0%88/"/>
      <url>/2020/01/16/Gromov%E8%AE%BF%E8%B0%88/</url>
      
        <content type="html"><![CDATA[<p>翻译自2014年12月22日西蒙斯基金会对Gromov的<a href="https://www.simonsfoundation.org/2014/12/22/mikhail-gromov/" target="_blank" rel="noopener">采访</a>.</p><hr><p>在向米沙·格罗莫夫（米沙是米哈伊尔的昵称）征求了18个月的同意后，我终于到巴黎左岸中心的奥登地铁站度过了完美的一天，想着这个男人最后究竟会不会放我鸽子。</p><p>格罗莫夫在法国的高等科学研究所和纽约大学的库朗研究所任职。这使得他是一个很难亲自碰面的数学家。并且正如追随他的数学家所证实的，他的思想也不易让人捕捉。</p><p>我在繁忙的街道上发现他，他如同一只稀有的鸟。其实，格罗莫夫看上去更像一个观鸟者，他的白色棒球帽向上倾斜在达尔文式的毛发上，他的衬衫紧紧地塞了起来。我们去了咖啡馆并点了巴黎水，随后的事情则进展快速。</p><p>“什么是数学？这个问题太荒唐了”，他说。</p><p>在巴黎“咖啡馆哲学”的传统中，像我们这样的对话数不胜数，从圣诞老人谈到真理，从美谈到性和死亡，甚至也可能是征税。我想和格罗莫夫谈他的“叶子上的虫子”的概念，以及他称之为大脑如何做数学的“ERGO逻辑”。至少在最近，这些都是他思考了很多的事情。</p><p>在2009年他获得了被誉为数学诺贝尔奖的阿贝尔奖，“因为他对几何做出的革命性的贡献”。格罗莫夫本人也有一些改革者的姿态：不平凡，不正统，不墨守成规，狂野。</p><p>在接受阿贝尔奖之后，他被要求写一篇简短的自传体文章，出版在一本描述最近获奖者的书里面。约翰·米尔诺的文章一开始写道，“我在新泽西的枫林长大……”。约翰·泰特则开始道：“接下来是发生在我的教育和早期职业生涯中的一些事情的概述……”。格罗莫夫则以存在主义危机开头：“在经历了一些令人沮丧的失败的写自传的尝试后，我已经得到了一个不可避免的结论，那就是这是一个在逻辑上不可能的任务。请注意，这个‘不存在猜想’有很多反例……”</p><p>维基百科上这样写道：“米哈伊尔·列昂尼多维奇·格罗莫夫（也叫米哈伊尔·格罗莫夫或者米歇尔·格罗莫夫），是一位法籍俄罗斯数学家，在许多不同的数学领域都做出了重要贡献”。并且根据MacTutor数学史，格罗莫夫上过列宁格勒大学，在1965年获得硕士学位（相当于博士学位）。在1970年，他应邀参加了在法国尼斯举行的国际数学家大会，但苏联当局阻止他参加。尽管如此，他提出的演讲“一种构造微分方程和不等式解的拓扑技术”在会议论文集上发表，标志着他在数学世界舞台上的首次亮相。</p><p>格罗莫夫早期的工作，包括他的博士论文，扩展了现在在普林斯顿大学的约翰·纳什，加州大学伯克利分校的斯蒂芬·斯梅尔，纽约大学石溪分校的安东尼·菲利普斯提出的思想。“在他的论文将我的思想做出非凡地推广后”，菲利普斯回忆道，“他继续革新几何。他发明了非常一般的构造，给出新的洞见和结果。那是新的东西，新的数学。”</p><p>格罗莫夫的第一个重磅炸弹是同伦原理，或者“h原则”，一个求解偏微分方程的一般方法。“隐藏在h原则后的几何直观是类似于这样的东西”，麻省理工大学的拉里·古斯解释道，“如果你有一件毛衣并且你想把它放进盒子里面，有许许多多的方式去做这件事情。但是如果你必须写一个完全精确地描述怎样把毛衣放进盒子的说明，实际上这将是困难而且复杂的”，在数学中，问题是一些高维的物体能否嵌入到给定的空间，“至少在传统上，应付高维对象的唯一方法”，古斯说道，“是写下能精确描述其行为的方程，而那是难以做到的。正如在毛衣的情景中，我们能够描述怎样把毛衣放进盒子的唯一方式，是写下一列完全精确的说明，而这使得它看上去貌似很复杂。但是格罗莫夫发现了一个很好的方法来捕捉下面的想法：毛衣很柔软，因此你几乎怎么做都可以把它放到盒子里面。”</p><p>格罗莫夫的方法非常合适地被称之为“软”几何，相比较于其他的更加粗糙或坚硬的几何而言（当处理较难被压缩的物体例如金属棒甚至是一片纸屑的时候是必须的）。但是格罗莫夫本人一点也不软。他的身体苗条又健壮，年轻时是个十分小心谨慎的人。他的精力异常旺盛，速度非常快。他思维敏捷，说话快速。“他不能慢下来”，菲利普斯将同格罗莫夫的谈话比作站在一个全力打开的消防栓前，“对话一直继续”。</p><hr><p>当格罗莫夫开始考虑离开苏联的时候，菲利普斯在促进他移民到美国扮演了重要角色。“当我听说他在考虑移民的时候，我想，‘诶，为什么不让他来石溪呢？’我和吉姆·西蒙斯（当时是数学系系主任）取得联系，他非常地热情，他说，‘等他来了，我们将给他留一个位置。’我给他寄出一封加密的信，告诉他如果他能离开苏联，他可以来石溪，这里会给他一份工作。”</p><p>“每个人都被这个家伙震惊到了”，如今在科朗研究所的杰夫·齐格尔回忆道，“在和米沙聊了几个星期之后，我对丹尼斯·沙利文说，‘他给我的印象是，黎曼几何中一半已知的结果其实只有格罗莫夫知道。’”也就是说，齐格尔被留下了深刻印象，即便如此，几个月后，他还是对另一个同事戴特莱夫·格罗莫的敬畏之举感到惊讶，“他说，‘米沙是这个世纪最伟大的思想家之一。他以最简单的可能方式理解一切。我不知道他是如何做到的’。最初我的反应是，‘好吧，我们最好不要在这个地方迷失。’但思考了一会儿之后，我决定，‘哇，他也许有道理’。到现在，按普遍的共识，格罗莫夫确实被视为上个世纪最伟大的数学家之一”。</p><p>巴纳德学院的杜莎·麦克杜夫在石溪认识格罗莫夫时也印象深刻。“格罗莫夫以狂野著称，他有非常狂野的想法。他几乎凭空建立了这些令人惊讶的，有力的论据”</p><p>有一次，在解释复杂证明背后的想法时，格罗莫夫在一张纸上画了一个类似摩天大楼轮廓的花样。麦克杜夫说：“这是证明的主要思想：花样代表了一种非常尖锐且受控的形变，它使人们可以证明某些辛结构的存在，除了他以外，没有人能够想到。”</p><p>格罗莫夫在石溪取得的另一项革命性的贡献涉及他所谓的“辛马戏表演”，即通过一个小孔穿过一个大洞（通过一连串保持辛结构的运动）。关键问题是：这项壮举能否实现？正如石溪的布莱恩·劳森所回忆的那样，“格罗莫夫说，这个问题的答案是辛几何的关键。大约七年后，他终于弄清楚了。那篇论文催生了一个领域”，这个领域吸引了许多最聪明的年轻几何学家和拓扑学家，一个跨越几何和拓扑的领域，现在称为辛拓扑。劳森指出，从历史上讲，辛几何已经存在很长时间了。“但是，追溯到19世纪，人们一直从古典力学的角度思考这个问题。该领域的主要变化来自格罗莫夫，当时他提出了惊人的定理。他开始了一场革命。”</p><p>格罗莫夫还革新了某种被称为双曲群的群论，引入分析和微分几何，以及发展了威廉·瑟斯顿对于几何群论的“奇妙”拓扑观点（虽然据格罗莫夫说，只是“肤浅”的程度）。“我看待图像的方式是……”，格罗莫夫说，“我们采取了两种不同但是有时会有重叠的路线：瑟斯顿专注于该领域最美丽，最困难的部分（双曲三流形），而我则专注于最一般的（双曲群）”。</p><p>在与曲面不同但是相关的领域，他创建了现在被称为格罗莫夫-豪斯多夫收敛和格罗莫夫-豪斯多夫距离的工具，这些工具后来被证明有很多应用。俄罗斯数学家格里高利·佩雷尔曼在这方面小试牛刀，并用这些工具证明了庞加莱猜想。</p><p>除了具有革命性的才能外，格罗莫夫的作品还被认为具有一定的不可参透性（尽管这种观点带有一些偏向性，因为人们一直认为格罗莫夫的作品是正确而有见地的）。他的第一本书《偏微分关系》于1986年出版，具有巨大的影响力，但同时也令人难以理解——它使许多数学家花费精力去理解并向大众翻译。</p><p>古斯多次阅读了格罗莫夫的论文“填补黎曼流形”（Filling Riemannian Manifolds），并写了一篇译文（给数学同仁）和两篇综述性说明文章。“当我研究米沙的作品时，我认为我对他对事物的看法和他的贡献有一个好主意”，古斯说，“然后，每当我与他交谈的时候，我总是对他的话感到完全的震惊。原来我根本不了解他对事物的看法。”</p><p>“米沙的想象力非常宽松和自由”，劳森说，“他喜欢和事物玩耍，看看发生了什么，当他终于真正了解事物的真相时，他就带着它离开了。这是一种真正的原创精神。他本人就是这样。作为一个人，他充满了不断流动的想法。”</p><hr><p>确实，坐在巴黎咖啡馆的格罗莫夫旁边就像面对来自塞纳河的知识海啸。我仔细阅读了他最近写的“大脑中的数学电流”，这是他在“简单”——一个探索“数学和艺术实践的理想状态”的会议上发表的演讲的文章版本。他以笛卡尔的名言“我思故我在”开头(格罗莫夫特别强调“故（ERGO）”这一个字，因为将思想与存在联系在一起的过程——“因此”——非常重要)。</p><p>从这个地方他提出一系列的问题：“什么是数学，它是如何产生的？数学思想之流的源头在哪？大脑中数学的最终来源是什么？这些使人想起了古老的问题‘地球依托在哪里？’，我们的直觉将我们推向了‘在巨龟的背上’这个答案。”</p><p>我们的谈话过程就像“乌龟叠在乌龟上”的寓言一样递归下去——正如传说中在贝特朗·罗素关于宇宙无穷递归难题演讲上发言的小老太：“那是无穷尽的乌龟！”格罗莫夫拒绝回答他自己提出的什么是数学的问题因为它很荒唐。但是，他接着说道，“你可以问，‘它是怎么被创造的，如何被研究的，如何被学习的？’”</p><p>“我思考数学的方式”，他说，“是一个生理和心理的过程，而不是某种抽象”。因此产生了他“叶子上的虫子”的观念，这很好地展示了他的好奇心之旅，从适当的研究数学到生物学，进化，大脑结构以及科学观念如何进化的问题。</p><p>“叶子上的虫子显示出两种简单的现象”，他说，“它总是做同样的事情，迈出一条腿，然后迈另一条腿，一条腿，另一条腿，只是运动。很多事情都是这样完成的，包括欧氏几何，这是一点。另一点是信息理论。这就是说，相比于待在叶子内部，虫子会在叶子边缘花更多的时间，会在叶子尖端花更多的时间。当你观看图像的时候，你的眼睛会做同样的事情，你的眼睛会在图像的周围花更多的时间”</p><p>在格罗莫夫看来，这两个过程都带有漏洞和缺陷，都是通用机制运行的。“世界的逻辑迫使我们这样思考”，他说，“这是我近十年来一直在思考的问题：思考中的基本原理，特别是关于数学的基本思考。他称其为‘ERGO逻辑’，这是对传统逻辑的重新考虑，涵盖了‘ERGO系统’，‘ERGO大脑’和‘ERGO思维’”。</p><p>“生活类似于我们大脑中数学的组织方式。如果你不接受，生活是不可能继续的”，他说。最后一个我设法抓到的格罗莫夫片段是：“即使不可能，你无论如何也要尝试”。</p><p>在一个小时试图解析这些细微差别之后，我的时间到了。格罗莫夫并没有对离别表示留恋。我们起身离开，我重新检查了一下小费，然后像他为妻子跑腿一样快速地离开了。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Stability Theorems in Topological Data Analysis (4)</title>
      <link href="/2019/12/30/Stability-Theorems-in-Topological-Data-Analysis-4/"/>
      <url>/2019/12/30/Stability-Theorems-in-Topological-Data-Analysis-4/</url>
      
        <content type="html"><![CDATA[<p>Let $\mathbb X$ be a triangulable, compact metric space, $f,g:\mathbb X\to\mathbb R$ be tame functions (that is, $f$ and $g$ have finitely many homological critical values.) The sublevel sets of $f$ (resp. $g$) form a filtration of spaces. Using fields as coefficient groups, It gives a finite sequence of homology groups (vector spaces) in each dimension. A homology class is born if it is not an image from preceding homology groups, and is said to be died if it merges with other classes when entering succeeding homology groups. With the notion of birth and death, in each dimension one can associate $f$ (resp. $g$) a multiset in the two-plane called the persistence diagram, denoted by $D_k(f)$ (resp. $D_k(g)$) for $k=0,1,\cdots$. The bottleneck distance of $D_k(f)$ and $D_k(g)$ is defined as</p><script type="math/tex; mode=display">d_B(D_k(f),D_k(g)):=\inf_{\gamma \text{ bijection}}\sup_{x\in D_k(f)}||x-\gamma(x)||_{\infty}</script><p>It was proved that persistence diagrams are robust to small perturbations of functions. More specifically, <a href="https://link.springer.com/article/10.1007%2Fs00454-006-1276-5" target="_blank" rel="noopener">David Cohen-Steiner, Herbert Edelsbrunner, John Harer (‘2007’)</a> proved that, for each $k$ one has</p><script type="math/tex; mode=display">d_B(D_k(f),D_k(g))\le||f-g||_\infty</script><p>In analogy to Wasserstein distance, they note that bottleneck distance is a special case of a more general form. Let $D(f)$ be the union of all $D_k(f)$. Define the degree-$p$ Wasserstein distance between $D(f)$ and $D(g)$ by </p><script type="math/tex; mode=display">W_p(D(f),D(g))=\left[\inf_{\gamma \text{ bijections}}\sum_{x\in D(f)}||x-\gamma(x)||_\infty^p\right]^{\frac{1}{p}}</script><p>To prove a stability result for Wasserstein distance, additional conditions are required for the space $\mathbb X$ and functions $f$ and $g$. We shall explain the details about the work of <a href="https://link.springer.com/article/10.1007%2Fs10208-010-9060-6" target="_blank" rel="noopener">David Cohen-Steiner, Herbert Edelsbrunner, John Harer, Yuriy Mileyko (‘2010’)</a>. </p><a id="more"></a><hr><p><strong>Size of triangulation.</strong> </p><p>A crucial step in proving the main theorem is to bound the number of points in the diagram whose persistence exceeds some threshold. In fact, they observed that this quantity can be controlled by the size of triangulation. That is why we need to assume the space $\mathbb X$ is <strong>triangulable</strong>. Precisely, a triangulation of $\mathbb X$ is a finite simplicial complexes $K$ together with a homeomorphism $\nu:|K|\to \mathbb X$. For a simplex $\sigma $ in $K$ we can define its diameter $\text{diam}(\sigma)=\max<em>{x,y\in\sigma}d(\nu(x),\nu(y))$. The mesh of $K$ is defined as $\text{mesh}(K)=\max</em>{\sigma\in K}\text{diam}(\sigma)$. The size of $K$ is the number of simplices in $K$, denoted by $\text{card}(K)$. We are interested in the following size function</p><script type="math/tex; mode=display">N(r)=\min_{\text{mesh}(K)\le r}\text{card}(K)</script><p>A rough observation is that, $N(r)$  is a non-increasing function, and when $r$ tends to 0, $N(r)$ will tend to infinity. For ‘good’ spaces we can estimate that the order of divergence. For example, if $\mathbb X$ is an $n$-dimensional ball (or cube), then the cardinality      of simplices is proportional to $\text{vol}(\mathbb X)/r^n$. For compact Riemannian manifold of dimension $n$ the order also holds true. This inspires to propose the ‘<strong>polynomial growth</strong>‘ assumption: there are constants $C<em>0$, $R_0$ and $M$ such that for $r&lt;R</em>{0}$, $N(r)\le \frac{C_0}{r^M}$. </p><p>One clearly sees that the inequality cannot hold for all $r&gt;0$ since it leads to nonsense $N(r)\to 0$ as $r\to \infty$. In fact, the value of $N(r)$ for large $r$ is the minimal triangulation of the space $\mathbb X$. Even for the simplest case, say, an $n$-dimensional cube, the exact value of the minimal triangulation is not completely known. (See <a href="https://math.stackexchange.com/questions/474857/find-the-smallest-triangulation-of-the-n-dimensional" target="_blank" rel="noopener">this post</a>) .</p><p><strong>Lipschitz Functions</strong></p><p>Recall that $f:\mathbb X\to \mathbb R$ is a Lipschitz function if there is a constant $c$ such that $d(f(x),f(y))\le cd(x,y)$ for all $x,y\in \mathbb X$. The infimum over all such $c$ is called the Lipschitz norm of $f$, denoted by $\text{Lip}(f)$.</p><p>The reason to consider Lipschitz functions is that the sublevel sets of Lipschitz functions are well separated. For simplicity assume $f$ is non-expanding, i.e. $\text{Lip}(f)=1$. Then for $x\in f^{-1}(a)$ and $y\in f^{-1}(b)$ with $a&lt;b$ we have $d(x,y)\ge b-a$. Furthermore, denote $\mathbb X_a=f^{-1}((-\infty,a])$ and $\mathbb X_b=f^{-1}((-\infty,b])$. Let $\mathbb X_a^r={z\in\mathbb X|d(z,\mathbb X_a)\le r}$ be the set of points within distance $r$ to $\mathbb X_a$. Then $\mathbb X_a^{b-a}\subseteq \mathbb X_b$. This allows us to thicken the sublevel sets without breaking the order of inclusion. </p><p>Thickening a subset is helpful for us to find simplicial cycles in $K$ that approximate singular cycles in $\mathbb X$. This is best illustrated in the figure (copied from paper <a href="https://link.springer.com/article/10.1007%2Fs10208-010-9060-6" target="_blank" rel="noopener">‘2010’</a>)</p><p><img src="/2019/12/30/Stability-Theorems-in-Topological-Data-Analysis-4/thick.png" alt="cycle"></p><p><strong>Bounding Persistent Cycles</strong></p><p>Fix a tame non-expanding function $f$ on the triangulable, compact metric space $\mathbb X$. Let $P(f,\epsilon)$ be the number of points in the diagrams of $f$ whose persistence exceeds $\epsilon$.  Then the <em>Persistent Cycle Lemma</em> claims that $P(f,\epsilon)\le N(\epsilon)$ for <strong>any</strong> $\epsilon$.</p><p><strong>Proof.</strong> Let $K$ be a triangulation such that $\text{mesh}(K)\le \epsilon$. Let $\alpha$ be a homological class  in any dimension whose persistence exceeds $\epsilon$. i.e. $\text{per}(\alpha)=\text{d}(\alpha)-\text{b}(\alpha)\ge\epsilon$. Let $z(\alpha)$ be a closed cycle in $\mathbb X<em>{\text{b}(\alpha)}$. By separation of Lipschitz function, $\mathbb X</em>{\text{b}(\alpha)}^{\text{b}(\alpha)+\epsilon}\subset\mathbb X<em>{\text{b}(\alpha)+\epsilon}$. Thus, there is a closed cycle $\bar{z}(\alpha)$ in $K$ which is homologous to $z(\alpha)$ in $\mathbb X</em>{\text{b}(\alpha)+\epsilon}$.</p><p>Suppose there are $m$ points in the diagram of dimension $l$ whose persistence exceeds $\epsilon$. By construction, we can find $l$ dimensional cycles $\bar{z}<em>1,\bar{z}_2,\cdots,\bar{z}_m$. Assume the birth time are ordered $\text{b}_1\le\text{b}_2\le\cdots\le \text{b}_m$. We claim that these cycles are <strong>independent</strong>. Suppose $\bar{z}_i$ is not independent of its predecessors. i.e. $\bar{z}_i\sim\sum</em>{j=1}^{i-1}\lambda<em>j\bar{z}_j$. In $\mathbb X</em>{\text{b}<em>i+\epsilon}$ we have $z_i\sim\sum</em>{j=1}^{i-1}\lambda_jz_j$. Without loss of generality we assume $\text{b}_i&gt;\text{b}_j$ for $i&gt;j$ (or we can put the same $\text{b}_i$ together). By definition, this means that $z_i$ is dead before $\text{b}_i+\epsilon$, contradiction. By induction, these cycles are independent. Note that the number of independent cycles is bounded by the number of $l$ dimensional simplices. Summing over we see that $P(\epsilon,f)$ is bounded by $N(\epsilon)$.  </p><p><strong>Estimating Persistence Moments</strong></p><p>View a persistence diagram as a sum of Dirac measures. We want to estimate the moments</p><script type="math/tex; mode=display">\text{Per}_k(f)=\sum_{x\in D(f)}\text{per}(x)^k</script><p>Call it <em>degree-k total persistence</em> of $f$. More generally, we consider the following function</p><script type="math/tex; mode=display">\text{Per}_k(f,t)=\sum_{\text{per}(x)>t}\text{per}(x)^k</script><p>From this point of view, $P(t,f)$ is the measure of ${x|\text{per}(x)&gt;t}$. For simplicity, Let us project the diagram to the $t$-axis so that all functions are treated as functions on $\mathbb R$. Then $P(t,f)$ is a <strong>decreasing stair function</strong>. The derivative of $P(t,f)$ with respect to $t$ is the negative of Dirac function. Hence </p><script type="math/tex; mode=display">\text{Per}_k(f,t)=\int_{\epsilon>t}-P(\epsilon,f)'\epsilon^kd\epsilon</script><p>Integrating by parts we obtain</p><script type="math/tex; mode=display">\text{Per}_k(f,t)=t^kP(t,f)+k\int_t^{\text{Amp}(f)}P(\epsilon,f)\epsilon^{k-1}d\epsilon</script><p>where $\text{Amp}(f)=\max f-\min f\le \text{diam}(\mathbb X)$.</p><p>But $P(t,f)$ is bounded by $N(t)$ and $N(t)$ is assume to has a polynomial growth when $t$ is small. Suppose there are constants $C<em>{\mathbb X}$ and $M$ such that when $t\le \text{Amp}(f)$, $N(t)\le C</em>{\mathbb X}/t^M$. We see that </p><script type="math/tex; mode=display">\text{Per}_k(f,t)\le t^kC_{\mathbb X}/t^M+k\int_t^{\text{Amp}(f)}C_{\mathbb X}\epsilon^{k-1-M}d\epsilon</script><p>when $k=M+\delta$ with $\delta&gt;0$, </p><script type="math/tex; mode=display">\text{Per}_k(f,t)\le C_{\mathbb X}\text{diam}(\mathbb X)^\delta+\frac{M+\delta}{\delta}C_{\mathbb X}\text{diam}(\mathbb X)^\delta=\frac{M+2\delta}{\delta}C_{\mathbb X}\text{diam}(\mathbb X)^\delta</script><p>Thus we see that there is a uniform bound on $\text{Per}_k(f,t)$ which only depends on the space $\mathbb X$. Therefore, we abstract the following definition</p><p><strong>Definition.</strong> A metric space $\mathbb X$ implies bounded degree-k total persistence if $\text{Per}_k(f)$ is uniformly bounded for all non-expanding functions. </p><p>From the discussion above, a triangulable, compact metric space with polynomial growth size of order $M$ implies bounded degree-$(M+\delta)$ total persistence. </p><p><strong>Wasserstein Stability</strong></p><p>Let $\mathbb X$ be a triangulable, compact metric space which implies bounded degree-k total persistence. $f$ and $g$ are two tame non-expanding functions on $\mathbb X$. Then </p><script type="math/tex; mode=display">W_p(D(f),D(g))\le C_{\mathbb X}||f-g||_\infty^{1-k/p}</script><p>where $p\ge k$ and $C_{\mathbb X}$ is a constant depending only on $\mathbb X$.</p><p><strong>Proof.</strong> Let $||f-g||<em>\infty=\epsilon$. Let $\gamma:D(f)\to D(g)$ be a bijection such that $d_B(D(f),D(g))=||f-g||</em>\infty$. Note that $||x-\gamma(x)||\le 1/2(\text{per}(x)+\text{per}(\gamma(x)))$. Thus,</p><script type="math/tex; mode=display">W_p(D(f),D(g))^p\le \epsilon^{p-k}\sum ||x-\gamma(x)||_\infty^k\le\epsilon^{p-k}/2\sum (\text{per}(x)^k+\text{per}(\gamma(x))^k)</script><p>by the convexity of $x^k$. The stability inequality follows.</p><hr><p><strong>Wasserstein Stability for Point Clouds</strong></p><p>Recall that for a finite metric space $X={x_1,x_2,\cdots,x_n}$ with $n$ points. We have an isometric embedding</p><script type="math/tex; mode=display">\rho:X\to\mathbb (R^n,||\cdot||_\infty)</script><p>by $\rho(x<em>i)=(d_X(x_1,x_i),\cdots,d_X(x_n,x_i))$. If $X,Y$ are finite metric spaces with Gromov-Hausdorff distance $\epsilon$, we can find a compact metric space $Z$ such that $X,Y$ isometrically embed into $Z$ with Hausdorff distance $\epsilon$. Then we can embed $X\cup Y$ into $\mathbb R^{n_X+n_Y}$. The Cech filtration of $X$ and $Y$ are the sublevel set filtration of $\delta_X$ and $\delta_Y$. Note that $||\delta_X-\delta_Y||</em>\infty\le\epsilon$. However, the space $\mathbb R^{n_X+n_Y}$ is not compact. The Wasserstein stability theorem cannot apply directly. </p><p>One way to overcome this difficulty is to restrict $\delta_X$ and $\delta_Y$ to a subset of the total space. It is easy to check that $X\cup Y$ is bounded by a cube of radius $a=\min{\text{diam}(X),\text{diam}(Y)}+2\epsilon$. We see that this cube satisfies the polynomial growth condition $N(r)\le C_n\frac{a^n}{r^n}$ where $n=n_X+n_Y$ (just divide the cube into $a^n/r^n$ small cubes where each cube can be triangulated by $C_n$ simplices). Hence the cube implies $k$-total persistence with constant $C_n a^{k}$ where $k&gt;n$. At this moment we can use Wasserstein stability to obtain a bound by the Gromov-Hausdorff distance of point clouds.  </p><p>However, this result is not of much importance since the stability depends on the number of point clouds. The problem is that the point clouds we are considering are too general while the Wasserstein stability for spaces are restrictive. A reasonable assumption is to consider finite samples from a $k$-dimensional Riemannian manifold (or Finsler manifold). But one should be careful with Nerve theorem since it will not be true for general open covers.</p><p>To start up, let us consider the simplest case where point clouds are <em>Euclidean</em>, i.e. finite samples from Euclidean spaces. Now the embedding is fixed and Nerve theorem is true for any convex cover. The only question is to find a suitable metric for point clouds.  </p><p>Let $X,Y$ be compact subsets of $\mathbb R^d$ where $d$ is fixed. Let $\mathcal E(d)$ be the Euclidean group acting on $\mathbb R^d$. Consider the distance given by </p><script type="math/tex; mode=display">d^{\mathbb R^n}_\mathcal{EH}(X,Y)=\inf_{T\in \mathcal E(d)}d^{\mathbb R^n}_{\mathcal H}(X,T(Y))</script><p>This distance defines a metric on the set of isometric classes of  compact subsets in $\mathbb R^n$ (see <a href="https://people.math.osu.edu/memolitechera.1/papers/dgh-euclidean.pdf" target="_blank" rel="noopener">Memoli</a>). This distance is also commonly used in image/point cloud registration problem. It is often be approximated by Iterative Closet Poing (ICP) algorithm.</p><p>Obviously $d<em>\mathcal{GH}\le d</em>\mathcal{EH}$ and it is often the case where the inequality is strict. However, it is proved that $d<em>\mathcal{EH}\le cd</em>\mathcal{GH}^{1/2}$ thus two distances are equivalent. For our case $d<em>\mathcal{EH}$ is more convenient. Let $a=d</em>\mathcal{EH}$. For any $\epsilon&gt;0$ we can find an isometry $T\in \mathcal E(d)$ such that the Hausdorff distance between $X$ and $T(Y)$ is less than $a+\epsilon$. Therefore, $X\cup T(Y)$ lies in a ball of radius $2(\max{\text{diam}(X),\text{diam}(Y)}+a+\epsilon)$. This ball clearly satisfies the polynomial growth condition and the distance function $\delta_X$ restricted on the ball has the same filtration homotopy of $\delta_X$ on the whole space. According to the Nerve theorem, the sublevel filtration of $\delta_X$ is the Cech filtration of point clouds. Now apply Wasserstein Stability theorem and we obtain an inequality between the Wasserstein distance of Cech persistence diagrams and Euclidean-Hausdorff distance between $X$ and $Y$. </p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我是如何发现怪球的</title>
      <link href="/2019/11/28/%E6%88%91%E6%98%AF%E5%A6%82%E4%BD%95%E5%8F%91%E7%8E%B0%E6%80%AA%E7%90%83%E7%9A%84/"/>
      <url>/2019/11/28/%E6%88%91%E6%98%AF%E5%A6%82%E4%BD%95%E5%8F%91%E7%8E%B0%E6%80%AA%E7%90%83%E7%9A%84/</url>
      
        <content type="html"><![CDATA[<p>翻译自John Milnor的短文<a href="https://www.maths.ed.ac.uk/~v1ranick/papers/milnor.pdf" target="_blank" rel="noopener">Classification of $(n-1)$-connected $2n$-dimensional manifolds and the discovery of exotic spheres</a>。</p><hr><p>（上个世纪）50年代在普林斯顿的时候，我对理解高维流形拓扑的基本问题非常感兴趣。我尤其关注$n-1$连通的$2n$维流形，因为那看起来像是一个人最有希望取得进展的最为简单的例子。（当然，和球面有相同伦型的流形更简单。然而为了理解这类流形所提出的广义庞加莱问题似乎也太难了，我都不知从何下手。）对一个闭的$2n$维流形$M^{2n}$，并且其低于中间维数的同伦群平凡，已经有大量的技术和成果可以用来去研究它。首先，你可以轻易地描述这样一个流形的伦型。在同伦的意义下，它可以用这样的方式构造出来：取有限个相交于一点的$n$维球面，然后将一个$2n$维胞腔$e^{2n}$用一个从边界$\partial e^{2n}$到这些$n$维球面的映射粘贴到上面去，这样</p><script type="math/tex; mode=display">M^{2n}\simeq (S^n\vee \cdots\vee S^n)\cup_{f}e^{2n}</script><p>粘贴映射$f$表示一个来自同伦群$\pi_{2n-1}(S^n\vee \cdots\vee S^n)$中的同伦类。至少在维数较低的时候，这个同伦群是很好用的。因此这类流形的同伦理论在我们的掌控之中。通过上同调，我们可以更好地理解它。如果使用整系数，$M^{2n}$的上同调在零维是无限循环群，在中间维数是自由阿贝尔群，每个球面都是一个生成元，在最高维是无限循环群，并且最高维的胞腔对应一个上同调类，也即</p><script type="math/tex; mode=display">H^0(M^{2n})\cong\mathbb{Z},\, H^n(M^{2n})\cong\mathbb{Z}\oplus\cdots\oplus\mathbb{Z},\, H^{2n}(M^{2n})\cong\mathbb{Z},</script><p>粘贴映射$f$决定了一个上积：对中间维数的任意两个上同调类我们能对应一个最高维的上同调类，换而言之（如果流形是定向的），对应一个整数。这给出了$H^n\otimes H^n$到整数的一个双线性的配对。由庞加莱对偶，这个配对在$n$为偶数的时候是对称的，在$n$为奇数的时候是反对称的，并且其行列式为$\pm 1$。$n$为奇数时配对是一个极其简单的代数对象。然而$n$为偶数时，这个对称的配对，或等价地，整数上的二次型，构成了一个被广泛研究的困难的课题（见[7]，比较[5]。）它的一个基本的不变量是<em>符号差</em>：在实数域上对二次型对角化，然后用正的对角元的数目减去负的对角元的数目。</p><p>到目前为止都还是单纯的同伦理论，但如果流形有一个光滑的结构，那么我们还有示性类，特别的，在被四整除的维数有庞特里亚金类，</p><script type="math/tex; mode=display">p_i\in H^{4i}(M).</script><p>这些是在50年代我作为一个长期项目试图去了解的问题的开始。让我努力描述一下当时的拓扑学的知识状况。一些基本的工具已经发展出来。我非常幸运地从诺曼.斯廷罗德那学到了上同调理论和纤维丛理论，他是这个领域的领袖。这两个概念在示性类的理论中被结合起来，底空间的上同调类对应到特定的纤维丛。另外一个基本工具是阻碍理论，它给出了系数在特定同伦群中的上同调类。然而，阻碍理论在50年代初期是一个症结点，因为尽管每个人都很清楚地知道怎么使用上同调，除开一些特殊情形，没有人知道要怎么计算同伦群：绝大多数同伦群是完全不清楚的。第一个重大突破来源于塞尔的论文，在这篇论文中为了理解同伦群他发展了一个代数的工具。塞尔理论中的一个代表性的结果是球面的稳定同伦群</p><script type="math/tex; mode=display">\Pi_n=\pi_{n+k}(S^k)\quad(k>n+1)</script><p>都是有限群。50年代初期另外一个突破来自托姆的配边理论，其中基本的对象是以流形的等价类为元素的群。他说明了这些群可以用特定的空间的同伦群来计算。紧接着他的工作之后，希策布鲁赫证明了他猜测的一个公式，这个公式将流形的示性类和符号差联系起来。对任意闭的定向$4m$维流形，使用实系数，我们可以讨论上积配对的符号差</p><script type="math/tex; mode=display">H^{2m}(M^{4m};\mathbb{R})\otimes H^{2m}(M^{4m};\mathbb{R})\to H^{4m}(M^{4m};\mathbb{R})\cong\mathbb{R}</script><p>如果流形是光滑的，那么它也有庞特里亚金类。将不同的庞特里亚金类乘起来使其进入最高维上同调群，我们得到不同的庞特里亚金数。这些证书依赖于切丛的结构。希策布鲁赫猜想了一个公式将符号差表达为庞特里亚金书的有理线性组合。比如</p><script type="math/tex; mode=display">\text{signature}(M^4)=\frac{1}{3}p_1[M^4]</script><p>以及</p><script type="math/tex; mode=display">\text{signature}(M^8)=\frac{1}{45}(7p_2-(p_1)^2)[M^8].</script><p>证明所需要的东西都包含在托姆关于配边的论文中。前两个情形是直接处理的，他又提供了一些工具去证明希策布鲁赫更一般的公式。</p><p>以上这些就是我试图用来去理解$n-1$连通的$2n$维流形的结构的工具。在最简单的情形，也就是中间维数的贝蒂数为零，这些构造没什么帮助。然而在次简单的情形，也就是中间维数仅有一个生成元并且$n=2m$为偶数，它们确实提供了不少关于结构的信息。如果我们想要去构造这样一个流形，就同伦论而言，我们必须从一个$2m$维流形开始，往上粘贴一个$4m$维的胞腔，最后它应该同伦等价于一个$4m$维的流形</p><script type="math/tex; mode=display">S^{2m}\cup e^{4m}\simeq M^{4m}</script><p>关于这些对象我们能说些什么呢？一些特定的例子是知道的；最简单的是四维的复射影平面—我们可以想象它是一个二维球面（也即复射影直线）贴上一个四维的胞腔。类似的在八维有四元数射影平面，我们认为它是一个四维球面贴上一个八维胞腔，在十六维有凯莱射影平面，它的性质是类似的。（我们现在已经知道这样的流形只能在这些特殊的维数存在。）</p><p>考虑一个光滑流形$M^{4m}$，假设它的伦型可以用上述方式描述。那么它可以是什么样子？我们从容易理解的$2m$维球面开始。惠特尼告诉我们，至少在$m&gt;1$的时候，这个球面可以光滑地嵌入为一个子集$S^{2m}\subset M^{4m}$，并生成了中间维数的同调群。我们考察嵌入球面的一个管状邻域，等价地，考察其$2m$维圆盘法丛$E^{4m}$。一般而言当我们环绕球面走一圈这个丛必定会扭转——它不能简简单单是个乘积流形否则它的那些性质就不对了。在纤维丛理论中，我们可以用下述方式进行考察：把$2m$维球面切成两个半球$D^{2m}<em>+$和$D^{2m}</em>-$，这两个球面相交于公共的边界$S^{2m-1}$。在每个半球上我们一定有乘积丛，并且我们一定能把两个乘积粘贴在一起形成</p><script type="math/tex; mode=display">E^{4m}=(D^{2m}_+\times D^{2m})\cup_{F}(D^{2m}_-\times D^{2m})</script><p>这里粘贴映射$F(x,y)=(x,f(x)y)$由一个从$D^{2m}<em>+\cap D^{2m}</em>-$到$D^{2m}$的旋转群的映射$f:S^{2m-1}\to SO(2m)$决定。因此扩宽$2m$球面最一般的方式可以由同伦群$\pi_{2m-1}(SO(2m))$中的元素描述。在维数较低的时候，这个群被了解地很透彻。</p><p>从最简单的情形$4m=4$开始，$S^2$上的$D^2$丛由群$\pi_1(SO(2))\cong\mathbb{Z}$中的元素决定。不难验证，不考虑定向的情况下，唯一能够以粘贴四维胞腔构作出丛的方式得到的四维流形是标准的复射影平面：这个构造没有给出什么新的东西。接下来的情形则变得更为有趣。在八维$S^4$上的$D^4$丛由群$\pi_3(SO(4))$中的元素描述。在二叶复叠的意义下，群$SO(4)$只是两个三维球面的笛卡尔积，因此$\pi_3(SO(4))\cong\mathbb{Z}\oplus\mathbb{Z}$。更确切的，将$S^3$视为为四元数中的单位三维球面。用左乘任意一个单位四元数我们得到从$S^3$到自身的映射，同样地，用右乘我们得到另一个映射。将这两个操作放在一起，一个一般的$(f)\in\pi_3(SO(4))$可以由映射$f(x)y=x^iyx^j$表示，其中$x$和$y$是单位四元数而$(i,j)\in\mathbb{Z}\oplus\mathbb{Z}$是任意一对整数。</p><p>因此每个整数对$(i,j)$我们都能配以一个显式的四维球面上的四维圆盘丛。我们想要它是一个闭八维流形中的管状邻域，这意味着我们想要能够在上面粘贴一个八维胞腔来给出一个光滑流形。为使其成立，边界$M^7=\partial E^8$必须是一个七维的球面$S^7$。这样问题变成了：对哪些$i$和$j$这个边界同构于$S^7$？不难决定什么时候它有对的伦型：事实上，$M^7$有$S^7$的伦型当且仅当$i+j$等于$\pm 1$。为了固定我们的想法，假设$i+j=+1$。这仍然给出了无穷多种$i$的选择。对每个$i$，注意到$j=1-i$是确定的，并且我们得到一个作为边界的流形$M^7=\partial E^8$，它是$S^4$上的$S^3$丛有着$S^7$的伦型。那么它是流形$S^7$吗？</p><p>让我们回到八维的希策布鲁赫-托姆符号差公式。它告诉我们一个理论上的八维流形的符号差可以从$(p_1)^2$和$p_2$算出来。但是这个符号差必须是$\pm 1$（记住二次型的行列式永远是$\pm 1$），并且我们可以选择定向使其是$+1$。因为限制同态将$H^4(M^8)$同构地映到$H^4(S^4)$，庞特里亚金类$p_1$被四维球面的邻域的切丛所完全决定，因此也被整数$i$和$j$完全决定。事实上$p_1$等于$2(i-j)=2(2i-1)$乘上$H^4(M^8)$的生成元，因此$p_1^2[M^8]=4(2i-1)^2$。我们没有直接的方式计算$p_2$，它依赖于整个流形。然而，我们可以从符号差公式中反解出$p_2[M^8]$，从而得到公式</p><script type="math/tex; mode=display">p_2[M^8]=\frac{p_1^2[M^8]+45}{7}=\frac{4(2i-1)^2+45}{7}</script><p>对$i=1$它得到$p_2[M^8]=7$，对四元数射影平面它是对的。但是对于$i=2$我们得到$p_2[M^8]=\frac{81}{7}$，这不可能！因为$p_2$是带整系数的上同调类，这个庞特里亚金数$p_2[M^8]$，不管是多少，一定是一个整数。</p><p>那么什么出错了？如果我们选择$p_1$使得$p_2[M]$不是一个整数，那么不可能有这样的光滑流形。流形$M^7=\partial E^8$肯定是存在的并且伦型是七维球面，然而我们不能把一个八维胞腔粘到$E^8$上去从而得到一个光滑流形。当时我以为这个$M^7$给出了七维的广义庞加莱猜想的一个反例：我以为$M^7$有七维球面的伦型，但是不同胚于标准的七维球面。</p><p>然后我更进一步地探究并考察$M^7$上精细的几何。这个流形是一个相当简单的东西：一个用四元数乘法显式构造的$S^4$上的$S^3$丛。我发现我事实上能证明它同胚于标准的七维球面，我的情况看上去变得更糟了！在$M^7$上，我可以找到一个光滑的实值函数仅有两个极值点：一个非退化的最大值点和一个非退化的最小值点。这个函数的水平集是六维球面，沿着法向形变它我们得到这个流形和标准$S^7$的一个同胚（这是里布的一个定理：如果一个闭$k$维流形有一个仅有两个极值点的莫尔斯函数，那么它同胚于七维球面。）这时我才明白这个构造并不是我最初以为的是庞加莱猜想的反例。这个$M^7$确实是一个拓扑球面，但是有一个奇怪的光滑结构。</p><p>这是一个更让人惊讶的结论。假设我们沿一个水平集把流形切开，这样</p><script type="math/tex; mode=display">M^7=D^7_+\cup_fD^7_-</script><p>其中$D_{\pm}^7$微分同胚于七维圆盘。它们沿公共边界用微分同胚$f:S^6\to S^6$粘起来。<em>因此这个流形可以用两个七维圆盘在边界用微分同胚粘接得到</em>。同时，这个证明说明存在$S^6$到自身的微分同胚其本身非常怪异：它不能用光滑的同痕变为恒等映射，因为如果能这么做那么$M^7$就会微分同胚于标准的七维球面，进而与上述讨论矛盾。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>The Space of Persistence Diagrams</title>
      <link href="/2019/11/24/The-Space-of-Persistence-Diagrams/"/>
      <url>/2019/11/24/The-Space-of-Persistence-Diagrams/</url>
      
        <content type="html"><![CDATA[<p><strong>Notation.</strong> In the following $\mathbb R^2$ is equipped with $\infty$-norm. Let $\Delta={(x,x)|x\in\mathbb R}$ denote the diagonal, $\Delta<em>+={(x,y)|y&gt;x}$ denote the upper off-diagonal points, and $\overline{\Delta}</em>+=\Delta\cup\Delta<em>+$ is the closure of $\Delta</em>+$. Set $\overline{\mathbb N}={1,2,\cdots}\cup{\infty}$.</p><hr><p><strong>Definition.</strong> A persistence diagram $\mathcal T$ is a pair $(S<em>{\mathcal T},i</em>{\mathcal T})$ where $S<em>{\mathcal T}$ is a subset of $\mathbb R^2$ and $i</em>{\mathcal T}:S_{\mathcal T}\to \overline{\mathbb N}$ is a function such that </p><ol><li>$\Delta\subset S<em>{\mathcal T}\subset \overline{\Delta}</em>+$;</li><li>$i_{\mathcal T}(\Delta)={\infty}$.</li></ol><p>$S<em>{\mathcal T}$ is called the support of $\mathcal T$ while $i</em>{\mathcal T}$ is called the multiplicity function of $\mathcal T$.  </p><p>Two persistence diagrams are said to be equal if they have the same support and multiplicity function. The graph of $i<em>{\mathcal T}$, defined by $G(i</em>{\mathcal T})={^k{\bf x}:=({\bf x},k)\in S<em>{\mathcal T}\times \overline{\mathbb N}|k\le i</em>{\mathcal T}({\bf x})}$, is called the bundle space over $\mathcal T$. The projection $\pi<em>{\mathcal T}:G(i</em>{\mathcal T})\to S<em>{\mathcal T}$ to the first factor defines a map from the bundle space to the support with fiber $\pi</em>{\mathcal T}^{-1}({\bf x})={1,2,\cdots,i<em>{\mathcal T}({\bf x})}$ for each point ${\bf x}\in S</em>{\mathcal T}$. It is an easy consequence that two persistence diagrams are equal if and only if they have the same bundle spaces.</p><a id="more"></a><p><strong>Definition.</strong> Let $\mathcal T<em>1$ and $\mathcal T_2$ be two persistence diagrams. Their sum $\mathcal T_1+\mathcal T_2$ is a persistence diagram where $S</em>{\mathcal T<em>1+\mathcal T_2}=S</em>{\mathcal T<em>1}\cup S</em>{\mathcal T<em>2}$ is the union of two supports and $i</em>{\mathcal T<em>1+\mathcal T_2}=i</em>{\mathcal T<em>1}+i</em>{\mathcal T<em>2}$ is the natural extension on $S</em>{\mathcal T_1+\mathcal T_2}$. </p><p>Let $0<em>{\Delta}$ be the trivial persistence diagram whose support is $\Delta$ and multiplicity function is the constant function $\infty$. It is clear that $0</em>{\Delta}$ is the neutral element for addition. Therefore, the set of persistence diagrams together with addition forms a monoid.</p><p><strong>Definition.</strong> Let $\lambda\ge0$ be a nonnegative number. Define the scalar product  $\lambda \mathcal T$ to be the persistence diagram with $S<em>{\lambda\mathcal T}=\lambda S</em>{\mathcal T}={\lambda{\bf x}|{\bf x}\in S<em>{\mathcal T}}$ and $i</em>{\lambda\mathcal T}(\lambda{\bf x})=i_{\mathcal T}({\bf x})$.</p><p><strong>Caveat.</strong> It is easy to check that $\lambda(\mathcal T_1 +\mathcal T_2)=\lambda\mathcal T_1+\lambda\mathcal T_2$. However, for $\lambda_1,\lambda_2\neq 0$, $(\lambda_1+\lambda_2)\mathcal T$ may not equal to $\lambda_1\mathcal T+\lambda_2\mathcal T$.</p><p><strong>Definition.</strong> Let $\mathcal T_1$ and $\mathcal T_2$ be two persistence diagrams and $p\ge 1$. The $p$th Wasserstein distance between $\mathcal T_1$ and $\mathcal T_2$ is defined as</p><script type="math/tex; mode=display">W_p(\mathcal T_1,\mathcal T_2)=(\inf_{\gamma}\{\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma(^k{\bf x}))||_{\infty}^p\})^{1/p}</script><p>where $\gamma$ ranges over all the bijections from $G(i<em>{\mathcal T_1})$ to $G(i</em>{\mathcal T<em>2})$. The summation is interpreted as integration with respect to the counting measure on $G(i</em>{\mathcal T_1})$. If such a bijection does not exist, the distance is defined to be $\infty$.</p><p>$W_p$ is not a metric on the space of persistence diagrams, since $W_p(\mathcal T_1,\mathcal T_2)=0$ does not imply $\mathcal T_1=\mathcal T_2$. For example, consider $\mathcal T_1$ with support ${(0,y)|y\in(0,1]}\cup\Delta$ and for each $(0,y)$ the multiplicity is $1$. Excise the point $(0,1)$ and we obtain another persistence diagram $\mathcal T_2$ such that $W_p(\mathcal T_1,\mathcal T_2)=0$. However, $\mathcal T_1$ and $\mathcal T_2$ are not equal. </p><p><strong>Definition.</strong> Let $\mathcal T$ be a persistence diagram and $p\ge 1$. The $p$th Wasserstein norm of $\mathcal T$ is defined as $||\mathcal T||<em>{W_p}:=W_p(\mathcal T, 0</em>{\Delta})$. The $p$th finite persistence space is defined as the collection of persistence diagrams with finite $p$th Wasserstein norm, i.e. $\mathcal D<em>p={\mathcal T|||\mathcal T||</em>{W_p}&lt; \infty}$. </p><p><strong>Example.</strong> Let ${\bf x}=(x,y)\in\mathbb R^2$ with $y&gt;x$. Define the indicator diagram at ${\bf x}$ to be the diagram$\chi^{{\bf x}}_1$ with support $\Delta\cup\{{\bf x}\}$ and $i_{\chi^{\bf x}_1}({\bf x})=1$. Then $||\chi^{{\bf x}}_1||_{W_p}=(y-x)/2$. Similarly, we can define $\chi_k^{{\bf x}}=\sum^k_{r=1}\chi^{\bf x}_1$, called the indicator diagram at ${\bf x}$ in multiplicity $k$. Then $||\chi_k^{\bf x}||_{W_p}=k^{1/p}(y-x)/2$. More generally, we have the following lemma.</p><p><strong>Lemma.</strong> Let $\mathcal T=\sum_{r=1}^n\chi^{{\bf x}_r}_{1}$ be a finite sum of indicator diagrams. Then $||\mathcal T||_{W_p}=(\sum_{r=1}^n||\chi_1^{{\bf x}_r}||^p_{W_p})^{1/p}$. As a consequence, for any $\mathcal T\in\mathcal D<em>p$ and ${\bf x}\in S</em>{\mathcal T}\backslash\Delta$, the  multiplicity of ${\bf x}$ is finite. i.e. $i_{\mathcal T}({\bf x})&lt;\infty$.  </p><p><strong>Remark.</strong> In convention, the degree-$p$ total persistence of a persistence diagram $\mathcal T$ is defined to be $(2W<em>p(\mathcal T,0</em>{\Delta}))^p$. Thus a persistence diagram has finite $p$th Wasserstein norm if and only if its degree-$p$ total persistence is finite. This is why we call $\mathcal D_p$  $p$th finite persistence space. </p><p><strong>Theorem.</strong> $W_p$ is a metric on the space $\mathcal D_p$ for $p\ge 1$.</p><p><strong>Proof.</strong> Symmetry is obvious from definition. Let $\mathcal T<em>1,\mathcal T_2\in\mathcal D_p$. Assume $W_p(\mathcal T_1,\mathcal T_2)=0$. Let $^k{\bf z}\in G(i</em>{\mathcal T<em>1})$ and ${\bf z}$ be the corresponding point in $S</em>{\mathcal T<em>1}$. If ${\bf z}\in\Delta$, then it is clear $^k{\bf z}\in G(i</em>{\mathcal T<em>2})$. Suppose ${\bf z}\notin\Delta$. For any $\epsilon&gt;0$, there is a bijection $\gamma</em>{\epsilon}$ from $G(i<em>{\mathcal T_1})$ to $G(i</em>{\mathcal T_2})$ such that </p><script type="math/tex; mode=display">\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma_{\epsilon}(^k{\bf x}))||_{\infty}^p<\epsilon</script><p>Therefore we have $||{\bf z}-\pi<em>{\mathcal T_2}(\gamma</em>{\epsilon}(^k{\bf z}))||<em>{\infty}^p&lt;\epsilon$. Take $\epsilon=1/2^{np}$ for $n=1,2,\cdots$ and denote $\pi</em>{\mathcal T<em>2}(\gamma_n(^k{\bf z}))$ by ${\bf z}_n$. If ${\bf z}_n={\bf z}$ for some large $n$, then ${\bf z}\in S</em>{\mathcal T<em>2}$ and $i</em>{\mathcal T<em>1}({\bf z})=i</em>{\mathcal T<em>2}(\bf z)$. If ${\bf z}_n$’s are distinct from ${\bf z}$, we have $||{\bf z}-{\bf z}_n||</em>{\infty}<1 2^n$. writing in coordinates we find that $||\chi^{{\bf z}_n}_1||_{w_p}>||\chi^{\bf z}_1||_{W_p}-1/2^n$. Then $||\mathcal T_2||_{W_p}\ge (\sum_{r=n}^N||\chi_1^{{\bf z}_r}||_{W_p}^p)^{1/p}\ge (N-n)^{1/p}||\chi^{\bf z}_1||_{W_p}-(N-n)^{1/p-1}\frac{1}{2^{n-1}}$. The last inequality holds since $x^p$ is convex for $p\ge 1$. Let $N$ tend to infinity and we have a contradiction that $||\mathcal T<em>2||</em>{W<em>p}=\infty$. Over all, we conclude that $^k{\bf z}\in G(i</em>{\mathcal T<em>2})$. Hence $G(i</em>{\mathcal T<em>1})\subset G(i</em>{\mathcal T<em>2})$ and symmetry implies that $G(i</em>{\mathcal T<em>2})\subset G(i</em>{\mathcal T_1})$. i.e. $\mathcal T_1=\mathcal T_2$.</1></p><p>Let $\mathcal T<em>1,\mathcal T_2,\mathcal T_3\in\mathcal D_p$. If $W_p(\mathcal T_1,\mathcal T_2)=\infty$ or $W_p(\mathcal T_2,\mathcal T_3)=\infty$, then triangle inequality trivially holds. Otherwise, for any $\epsilon&gt;0$, there are bijections $\gamma_1:G(i</em>{\mathcal T<em>1})\to G(i</em>{\mathcal T<em>2})$ and $\gamma_2:G(i</em>{\mathcal T<em>2})\to G(i</em>{\mathcal T_3})$ such that </p><script type="math/tex; mode=display">\begin{aligned}&(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))||_{\infty}^p)^{1/p}<W_p(\mathcal T_1,\mathcal T_2)+\epsilon/2\\&(\sum_{^k{\bf y}\in G(i_{\mathcal T_2})}||\pi_{\mathcal T_2}(^k{\bf y})-\pi_{\mathcal T_3}(\gamma_2(^k{\bf y}))||_{\infty}^p)^{1/p}<W_p(\mathcal T_2,\mathcal T_3)+\epsilon/2\end{aligned}</script><p>Note that $\gamma<em>2\circ\gamma_1$ is a bijection from $G(i</em>{\mathcal T<em>1})$ to $G(i</em>{\mathcal T_3})$. Therefore, we have</p><script type="math/tex; mode=display">\begin{aligned}& W_p(\mathcal T_1,\mathcal T_3)\le(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_3}(\gamma_2\circ\gamma_1(^k{\bf x}))||_{\infty}^p)^{1/p}\\&\le(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}(||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))||_{\infty}+||\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))-\pi_{\mathcal T_3}(\gamma_2\circ\gamma_1(^k{\bf x}))||_{\infty})^p)^{1/p}\\&\le(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}(||\pi_{\mathcal T_1}(^k{\bf x})-\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))||_{\infty}^p)^{1/p}+(\sum_{^k{\bf x}\in G(i_{\mathcal T_1})}(||\pi_{\mathcal T_2}(\gamma_1(^k{\bf x}))-\pi_{\mathcal T_3}(\gamma_2\circ\gamma_1(^k{\bf x}))||_{\infty}^p)^{1/p}\\&\le W_p(\mathcal T_1,\mathcal T_2)+W_p(\mathcal T_2,\mathcal T_3)+\epsilon\end{aligned}</script><p>where the third line used Minkowski’s inequality. Since $\epsilon$ is arbitrary, the triangle inequality holds.</p><p><strong>Corollary.</strong> </p><ol><li>$W_p(\mathcal T_1,\mathcal T_2)&lt;\infty$ for $\mathcal T_1,\mathcal T_2\in \mathcal D_p$;</li><li>Note that symmetry and triangle inequality holds for any persistence diagrams, but positive definiteness requires finiteness assumption; </li><li>An off-diagonal point of $\mathcal T\in \mathcal D_p$ can not be a cluster point. i.e. for any off-diagonal point ${\bf x}$ in $S_{\mathcal T}$ there is an open ball $B_{\epsilon}({\bf x})$ such that $B_{\epsilon}({\bf x})\cap S_{\mathcal T}=\{{\bf x}\}$. As a consequence, for any $\mathcal T\in\mathcal D_p$, $G(i_{\mathcal T})\backslash G(0_{\Delta})$ is at most countable.</li></ol><p><strong>Definition.</strong> A persistence diagram $\mathcal T\in\mathcal D<em>p$ is called constructible if $S</em>{\mathcal T}\backslash\Delta$ consists of finitely many points. The collection of all constructible persistence diagrams is denoted by $\mathcal C_p$. </p><p><strong>Remark.</strong> The name ‘constructible’ comes from the fact that any constructible persistence diagram can be obtained as a persistence diagram of a finite filtration of simplicial complexes.</p><p><strong>Theorem.</strong> Let $\mathcal T\in\mathcal D<em>p$ be arbitrary and $\epsilon&gt;0$ be any given positive number. There exists $\mathcal T_c\in\mathcal C_p$ and $\mathcal T_s\in\mathcal D_p$ such that $\mathcal T=\mathcal T_c+\mathcal T_s$ with $||\mathcal T_s||</em>{W_p}&lt;\epsilon$.</p><p><strong>Proof.</strong> Enumerate the off-diagonal points $\{{\bf x}_1,{\bf x}_2,\cdots\}=S_{\mathcal T}\backslash\Delta$. By definition,</p><script type="math/tex; mode=display">||\mathcal T||_{W_p}=(\sum_{n=1}^{\infty}||\chi^{\bf x_n}_{k_n}||_{W_p}^p)^{1/p}=(\lim_{l\to\infty}\sum_{||\chi^{\bf x_n}_1||_{W_p}\ge\frac{1}{l}}||\chi^{\bf x_n}_{k_n}||_{W_p}^p)^{1/p}<\infty</script><p>Thus, for $\epsilon&gt;0$, there exists $l&gt;0$ such that $||\mathcal T||<em>{W_p}^p-\sum</em>{||\chi^{\bf x<em>n}_1||</em>{W<em>p}\ge\frac{1}{l}}||\chi^{\bf x_n}</em>{k<em>n}||</em>{W<em>p}^p&lt;\epsilon^p$. Let $\mathcal T_u$ be the persistence diagram such that $S</em>{\mathcal T<em>u}$ consists of those points in $\mathcal T$ with $||\chi^{\bf x}_1||</em>{W<em>p}\ge 1/l$ and $i</em>{\mathcal T<em>u}$ is the restriction of $i</em>{\mathcal T}$ on $S<em>{\mathcal T_u}$, $\mathcal T_s$ be such that $S</em>{\mathcal T<em>s}$ consists of points with $||\chi^{\bf x}_1||</em>{W<em>p}&lt;1/l$ and $i</em>{\mathcal T<em>s}$ is the restriction of $i</em>{\mathcal T}$ on $S<em>{\mathcal T_s}$. It is immediate that $\mathcal T_u\in\mathcal C_p$ and $||\mathcal T_s||</em>{W_p}&lt;\epsilon$ and $\mathcal T=\mathcal T_u+\mathcal T_s$.</p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Some Applications of Wasserstein Distances</title>
      <link href="/2019/11/09/Some-Applications-of-Wasserstein-Distances/"/>
      <url>/2019/11/09/Some-Applications-of-Wasserstein-Distances/</url>
      
        <content type="html"><![CDATA[<h2 id="OMT-I"><a href="#OMT-I" class="headerlink" title="OMT I"></a>OMT I</h2><p>The original setting for optimal mass transportation (OMT) consists of three objects.</p><ol><li>Two probability spaces $(X,\mu)$ and $(Y,\nu)$. i.e. $\mu(X)=\nu(Y)=1$;</li><li>A measurable map $T:X\to Y$ such that $T_*\mu=\nu$. i.e. for any Borel set $B\subset Y$ it holds $\nu(B)=\mu(T^{-1}(B))$.</li><li>A cost function $c:X\times Y\to \mathbb R_+\cup{\infty}$ such that for any $T$ in 2. the map $c_T(x)=c(x,T(x))$ is measurable. The integration $\int_X c(x,T(x))d\mu$ is called the total cost.</li></ol><a id="more"></a><p>We require that $X$ and $Y$ are probability spaces. However, in practicing many examples do not arise naturally as probability spaces. In those cases we require $\mu(X)=\nu(Y)$ for finite or infinite measures. In the second setting note that we use the <strong>pushforward</strong> of $\mu$ but not the pullback of $\nu$. This has a practical meaning, see <a href="https://bookstore.ams.org/gsm-58/" target="_blank" rel="noopener">Villani</a> for a reference. </p><p>The original problem is raised by Monge that one needs to find a ‘transference plan’ $T$ to minimize the total cost $\int_X c(x,T(x))d\mu$.</p><p>Despite solving Monge’s problem, consider the special case where $(X,d)$ is a metric space. $\mu$ and $\nu$ are two measures on $X$ with finite $p$th moments, i.e. $\int_Xd(x,x_0)^pd\mu&lt;\infty,\int_Xd(x,x_0)^pd\nu&lt;\infty$ for any $x_0\in X$. Let $T:X\to X$ range over all measure preserving map so that we can obtain the quantity $(\inf{\int_Xd(x,T(x))^p})^{1/p}$ which is called the $p$th Wasserstein distance between $\mu$ and $\nu$, denoted by $W_p(\mu,\nu)$. It can be proved that $W_p$ is a metric on the space of measures with finite $p$th order on $X$.</p><h2 id="Shape-Classification"><a href="#Shape-Classification" class="headerlink" title="Shape Classification"></a>Shape Classification</h2><p>Here is an example that Wasserstein distance is used to classify shapes. See <a href="https://www.researchgate.net/publication/305887246_Surface-based_shape_classification_using_Wasserstein_distance" target="_blank" rel="noopener">this paper</a> for details of methods, and <a href="https://www.researchgate.net/publication/332180388_A_Geometric_View_of_Optimal_Transportation_and_Generative_Model" target="_blank" rel="noopener">this paper</a> for details of proofs.</p><p>Let $\mathbb D$ be the unit disk in $\mathbb R^2$ (generally any compact and convex set in $\mathbb R^n$ can be considered), equipped with Lebesgue measure $m$. Let $Y={y<em>1,\cdots,y_k}$ be a discrete subset in $\mathbb R^2$ with weighted counting measure $\delta=\sum</em>{i=1}^k b<em>i\delta</em>{y<em>i}$ such that $\delta(Y)=\pi$. In other words, we have two measures on $\mathbb R^2$ where the first is supported on $\mathbb D$ and the second is supported on $Y$. Monge’s problem states that a map $f:\mathbb D\to Y$ such that $f</em>*m=\nu$ is to be find to minimize $\int_{\mathbb D}|x-f(x)|^2dx$, where we use the standard metric on $\mathbb R^2$. Note that $Y$ is discrete. This problem is equivalent to partition $\mathbb D$ into $k$ subsets $D_1\cup\cdots\cup D_k$ such that $m(D_i)=b_i$. Therefore, we can use techniques in <a href="https://www.fmf.uni-lj.si/~lavric/hug&amp;weil.pdf" target="_blank" rel="noopener">convex geometry</a>.</p><p>Let ${\bf h}=(h<em>1,\cdots,h_k)\in\mathbb R^k$. Define a piecewise linear convex function $u</em>{\bf h}(x)=\max{x\cdot y<em>i+h_i}$. Let $G({\bf h})$ be the graph of $u</em>{\bf h}$. Then $G({\bf h})$ is a piecewise hyperplane. The projection of $G({\bf h})$ to $\mathbb D$ gives a cell decomposition $\mathbb D=\cup_{i=1}^k W_i$, where each $W_i$ corresponds to a piece of plane ${(x,x\cdot y_i+h_i)|x\in W_i}$. Assign each $W_i$ to $y_i$. By moving ${\bf h}$ we can find the request assignment with $m(W_i)=b_i$. The fact is that, this is the unique map minimizing the total cost (see <a href="https://www.researchgate.net/publication/227632352_Polar_Factorization_and_Monotone_Rearrangement_of_Vector-Valued_Functions" target="_blank" rel="noopener">Brenier</a>, <a href="https://www.researchgate.net/publication/220616351_Power_Diagrams_Properties_Algorithms_and_Applications" target="_blank" rel="noopener">Aurenhammer</a>). Therefore, the problem reduces to find ${\bf h}$ in some suitable space $H_0$. This is done by using Newton’s method after defining an objective function $E({\bf h})$ which is twice differentiable and the hessian is given in a closed form.  </p><p>For two arbitrary surfaces $M_1,M_2$ in $\mathbb R^3$ (represented by meshes), we use conformal maps to map $M_i$ to unit disks $\mathbb D_i$. For the first disk $\mathbb D_1$ we equip it with Lebesgue measure. For the second disk $\mathbb D_2$ we assign each point (projected from $M_2$) a value so that it is equipped with a weighted counting measure. The Wasserstein distance from $\mathbb D_1$ to $\mathbb D_2$ (in fact. $m$ to $\delta$) measures the difference between $M_1$ and $M_2$.</p><h2 id="OMT-II"><a href="#OMT-II" class="headerlink" title="OMT II"></a>OMT II</h2><p>Consider the following example: $X={x<em>1,\cdots,x_n}$ and $Y={y_1,\cdots,y_n}$ are finite sets with the same cardinality. Equip $X$ and $Y$ with counting measure. Then a measure preserving map $f:X\to Y$ is exactly a permutation. In many cases it is not adequate to use ‘maps’ only, since each $x_i$ cannot be ‘split’ under maps.  If we are permitted to partition each $x_i$ into pieces, then a transference plan is a matrix $\Pi=[\pi</em>{ij}]$ where $\sum<em>j \pi</em>{ij}=1$ for each $i$ and $\sum<em>{i} \pi</em>{ij}=1$ for each $j$, i.e. $\Pi$ is a bistochastic matrix. This idea was raised by Kantorovich and the more general setting for OMT is</p><ol><li>$(X,\mu)$ and $(Y,\nu)$ are probability spaces;</li><li>$\pi$ is a probability measure on $X\times Y$ such that the maginal distributions are $\mu$ on $X$ and $\nu$ on $Y$ respectively;</li><li>$c:X\times Y\to \mathbb R<em>+\cup{\infty}$ is a $\pi$ measurable function. The integration $\int</em>{X\times Y}c(x,y)d\pi$ is called the total cost.</li></ol><p>Kantorovich’s problem is to find a probability measure $\pi$ so that the total cost is minimized. Note that it is a linear programming problem so we can formulate this in a dual form. The Kantorovich dual problem is </p><script type="math/tex; mode=display">W_c(\mu,\nu)=\max_{\phi,\psi}\{\int_X\phi(x)d\mu+\int_Y\psi(y)d\nu\}</script><p>where $\phi$ and $\psi$ are functions on $X$ and $Y$ respectively and $\phi(x)+\psi(y)\le c(x,y)$. Define $\phi^c(y)=\inf_{x\in X}{c(x,y)-\phi(x)}$. Then (1) is equivalent to </p><script type="math/tex; mode=display">W_c(\mu,\nu)=\max_{\phi}\{\int_X\phi(x)d\mu+\int_Y\phi^c(y)d\nu \}</script><h2 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h2><p>In <a href="http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf" target="_blank" rel="noopener">this paper</a> Wasserstein distance is first introduced to Generative Adversarial Network and the model is called WGAN. A resent <a href="https://www.researchgate.net/publication/332180388_A_Geometric_View_of_Optimal_Transportation_and_Generative_Model" target="_blank" rel="noopener">paper</a> by Gu Xianfeng etc. used geometry to interprete the role of Wasserstein distance in WGAN.</p><p>A GAN consists of a generator (G) and a discriminator (D).  (G) generates artificial data and (D) discriminate them from real data. Suppose real data lies in a high dimensional space $\Chi$ and its distribution $\nu$ supports around a low dimensional manifold $\mathcal M$. A local chart $(U,\tau)$ is an open set in $\mathcal M$ together with a map $\tau:U\to Z$  from $U$ to the latent space $Z$. The inverse of $\tau$ is called a parametrization. If $\mu$ is a distribution on $Z$ then a local parametrization pushes forward $\mu$ to be a distribution on $\mathcal M$. i.e. the local parametrization <strong>is</strong> generator (G). Assume the training parameter for (G) is $\theta$ and denote the parametrization by $g<em>\theta$. (G) generates data on $\mathcal M$ whose distribution is $(g</em>\theta)<em>*\mu$. But how to discriminate it from the real distribution $\nu$? The answer is Wasserstein distance $W((g</em>\theta)_*,\nu)$. More specifically, by Kantorovich dual we need to compute</p><script type="math/tex; mode=display">W_c((g_\theta)_*\mu,\nu)=\max_{\phi}\{\int_Z\phi(g_\theta(z))d\mu+\int_Y\phi^c(y)d\nu\}</script><p>Suppose the training parameter for (D) is $\xi$. Then we can rewrite (3) as</p><script type="math/tex; mode=display">W_c((g_\theta)_*\mu,\nu)=\max_{\xi}(\mathbb E_{z\sim \mu}\phi(g_\theta(z))+\mathbb E_{y\sim \nu}\phi^c(y))</script><p>Therefore, the discriminator (D) <strong>is</strong> a calculator computing the Wasserstein distance. Above all, we can write down the objective function of a WGAN</p><script type="math/tex; mode=display">\min_{\theta}\max_{\xi}(\mathbb E_{z\sim \mu}\phi(g_\theta(z))+\mathbb E_{y\sim \nu}\phi^c(y))</script>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Wasserstein Distances </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stability Theorems in Topological Data Analysis (3)</title>
      <link href="/2019/10/02/Stability-Theorems-in-Topological-Data-Analysis-3/"/>
      <url>/2019/10/02/Stability-Theorems-in-Topological-Data-Analysis-3/</url>
      
        <content type="html"><![CDATA[<p>Perhaps the most important case in TDA is point cloud data, i.e. finite metric spaces. The collection of all compact metric spaces makes up a metric space under <a href="[https://en.wikipedia.org/wiki/Gromov%E2%80%93Hausdorff_convergence](https://en.wikipedia.org/wiki/Gromov–Hausdorff_convergence">Gromov-Hausdorff metric</a>). If we construct Rips filtrations for point clouds,  we obtain a map between metric spaces by sending each point cloud to its persistence diagram. It is natural to ask which properties will this map have. From the work of <a href="http://fodava.gatech.edu/files/reports/FODAVA-09-24.pdf" target="_blank" rel="noopener">F. Memoli etc.</a>, it is in fact a <strong>Lipschitz</strong> map. Therefore, the bottleneck distance between two persistence diagrams is bounded by the Gromov-Hausdorff distance between two point clouds. This stability shows that one can use TDA to classify different point clouds, which is a major object in shape analysis.</p><p>Let $(Z,d_Z)$ be a (compact) metric space and $X,Y$ be subsets in $Z$. The Hausdorff distance between $X$ and $Y$ is defined as</p><script type="math/tex; mode=display">d_H^Z(X,Y)=\max\{\max_{x\in X}\min_{y\in Y}d_Z(x,y),\max_{y\in Y}\min_{x\in X}d_Z(x,y)\}.</script><p>The intuition is follows: For each $x\in X$, we can compute the distance between $x$ and $Y$, which is $\min_{y\in Y}d_Z(x,y)$. Then we take the largest value as the distance from $X$ to $Y$. The distance is the so called one-sided Hausdorff distance. Symmetrically, we compute the distance from $Y$ to $X$. The maximum is the so called (two-sided) Hausdorff distance.</p><a id="more"></a><p>If $(X,d_X)$ and $(Y,d_Y)$ are different metric spaces, we cannot compare the distance between $X$ and $Y$ unless they are subspaces of another space $(Z,d_Z)$. Therefore, we try to embed $(X,d_X)$ and $(Y,d_Y)$ into a larger space so that they can be compared using Hausdorff distance. This motivates the following definition of Gromov-Hausdorff distance    </p><script type="math/tex; mode=display">d_{GH}((X,d_X),(Y,d_Y))=\inf_{X,Y\hookrightarrow Z}d_H^Z(X,Y).</script><p>Let $\mathcal{X}={(X,d<em>X):X\text{ is compact metric space}}$. Define an equivalence relation on $\mathcal{X}$: $(X,d_X)\sim(Y,d_Y)$ if they are isometric. The quotient space is still denoted by $\mathcal{X}$. Then $(\mathcal{X},d</em>{GH})$ is a complete metric space (see <a href="https://www.amazon.com/Course-Metric-Geometry-Dmitri-Burago/dp/0821821296" target="_blank" rel="noopener">this book</a> for reference). Furthermore, the infimum is in fact a minimum, that is, we can always find $Z$ for compact spaces.</p><p>Given a finite metric space $(X,d<em>X)$ and a parameter $\alpha&gt;0$, the Rips complex $R</em>\alpha(X,d<em>X)$ is an abstract simplicial complex with vertex set $X$. A $k$-simplex is in $R</em>\alpha(X,d_X)$ if and only if the diameter is no more than $2\alpha$. (<strong>Caveat.</strong> The factor 2 makes a difference in the main theorem. Note that it is a classical issue that there is no universal parametrization on Rips complex.) When $\alpha$ ranges from $0$ to $\infty$, the nested family is called Rips filtration, denoted by $\mathcal{R}(X,d_X)$.</p><p>Moreover, if $X$ is a finite subset of $(\mathbb{R}^n,l^\infty)$, there is another construction called $\check{C}ech$ complex which is the nerve of $l^\infty$ balls, denoted by $C_\alpha(X,l^\infty)$. It happens that the two constructions are the same.</p><hr><p><strong>Lemma</strong>. For any finite set $X$ in $(\mathbb{R}^n,l^\infty)$ and $\alpha&gt;0$, $C<em>\alpha(X,l^\infty)=R</em>\alpha(X,l^\infty)$.</p><p><strong>proof</strong>. If ${x<em>1,\cdots,x_k}$ is in $C</em>\alpha$, then there is $\bar{x}$ such that  $|x<em>i-\bar{x}|</em>\infty\le \alpha$. By triangle inequality $|x<em>i-x_j|</em>\infty\le 2\alpha$. ${x<em>1,\cdots,x_k}$ is in $R</em>\alpha$. On the other hand, if $|x<em>i-x_j|</em>\infty\le 2\alpha$, for each coordinate $l$ we take $\bar{x}^{(l)}=1/2(x^{(l)}<em>{\max}+x^{(l)}</em>{\min})$. It is easy to verify $|x<em>i-\bar{x}|</em>\infty\le \alpha$. ${x<em>1,\cdots,x_k}$ is in $C</em>\alpha$.</p><hr><p>Another useful lemma can be found in the <a href="https://www.amazon.com/Course-Metric-Geometry-Dmitri-Burago/dp/0821821296" target="_blank" rel="noopener">book</a> which says that any finite metric space of cardinality $n$ can be isometrically embedded into $(\mathbb{R}^n,l^\infty)$. The proof is also straightforward. Put the distances in the coordinates in $\mathbb{R}^n$. One also finds a comprehensive introduction to embedding of finite metric spaces <a href="https://sites.cs.ucsb.edu/~suri/cs235/MatousekMetric.pdf" target="_blank" rel="noopener">here</a>.</p><p>Now we can state and prove the stability theorem.</p><hr><p><strong>Theorem</strong>. For any finite metric spaces $(X,d_X)$ and $(Y,d_Y)$, for any $k\in \mathbb{N}$, </p><script type="math/tex; mode=display">d^\infty_B(D_k\mathcal{R}(X,d_X),D_k\mathcal{R}(Y,d_Y))\le d_{GH}((X,d_X),(Y,d_Y)).</script><p><strong>proof</strong>. Let $\epsilon=d_{GH}((X,d_X),(Y,d_Y))$. By definition, there is a compact metric space $(Z,d_Z)$ and inclusions $\gamma_X:X\to Z$ and $\gamma_Y:Y\to Z$ such that $d_H^Z(X,Y)\le \epsilon$. Since $\gamma_X(X)\cup\gamma_Y(Y)$ is finite, they can be isometrically embedded into $(\mathbb{R}^n,l^\infty)$ for some $n$. Denote the embedding by $\gamma$. Let $\delta_X$ be the distance function to $\gamma(\gamma_X(X))$. That is,  </p><script type="math/tex; mode=display">\delta_X(z)=\min_{x\in X}|z-\gamma(\gamma_X(x))|_\infty.</script><p>Similarly, let $\delta_Y$ be the distance function to $\gamma(\gamma_Y(Y))$. Note that</p><script type="math/tex; mode=display">|\delta_X(z)-\delta_Y(z)|\le d_H^\infty(\gamma(\gamma_X(X)),\gamma(\gamma_Y(Y)))=\epsilon.</script><p>(<em>I have doubt in this inequality. The closest point in $X$ and $Y$ may not correspond in Hausdorff distance</em>).</p><hr><p><em>update.</em> This inequality is true. Note that $\delta_X(z)=d(z,X)$. For any point $y\in Y$ it holds $d(z,X)\le d(z,y)+d(y,X)$. Since $\inf_y(d(z,y)+d(y,X))\le \inf_yd(z,y)+\sup_yd(y,X)$, we obtain $d(z,X)-d(z,Y)\le H(X,Y)$. The other side is similar.</p><hr><p>Since $\delta_X$ and $\delta_Y$ are $\epsilon$-close, by the theorem of <a href="https://link.springer.com/article/10.1007%2Fs00454-006-1276-5" target="_blank" rel="noopener">Edelsbrunner etc.</a>, the persistence diagrams of $\delta_X$ and $\delta_Y$ are $\epsilon$-close.</p><p>However, the level set $\delta<em>X([0,\alpha])$ is nothing but the union of $\alpha$-balls in $(\mathbb{R}^n,l^\infty)$. By persistence nerve theorem the sublevel filtration of $\delta_X$ and the $\check{C}ech$ filtration of $\gamma(\gamma_X(X))$ yield same persistence diagrams. From the above lemma, $C</em>\alpha(\gamma(\gamma<em>X(X)),l^\infty)=R</em>\alpha(\gamma(\gamma<em>X(X)),l^\infty)=R</em>\alpha(\gamma<em>X(X),d_Z)=R</em>\alpha(X,d_X)$. Therefore, the persistence diagram of $\delta_X$ is exactly the persistence diagram $D\mathcal{R}(X,d_X)$. Apply the same argument to $Y$. Hence the inequality holds.</p><hr>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stability Theorems in Topological Data Analysis (2)</title>
      <link href="/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/"/>
      <url>/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/</url>
      
        <content type="html"><![CDATA[<p>One observes that when proving the $L^{\infty}$ stability theorem, the functions $f$ and $g$ provide nothing but an interleaved inclusion of level sets $f^{-1}((-\infty,a])\subseteq g^{-1}((-\infty,a+\epsilon])\subseteq f^{-1}((-\infty.a+2\epsilon])$. This further gives the following commutative diagram.</p><a id="more"></a><p><img src="/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/interleavedmodules.png" alt="interleaved modules"></p><p>Instead of taking functions and spaces into account, let us forget about geometry and work on algebra directly. That is, we consider two sequences of modules such that the above diagram commutes. In the terminology of <a href="https://geometry.stanford.edu/papers/ccggo-ppmd-09/ccggo-ppmd-09.pdf" target="_blank" rel="noopener">F. Chazal etc.</a>, the two sequences of modules are called <em>(weakly/strongly) $\epsilon$-interleaved</em>. It would be no surprise that the distance between two persistence diagrams are bounded by $\epsilon$. However, when working in an algebraic approach, we can drop many restrictions on spaces and functions. Furthermore, we generalize the definition of persistence diagrams hence the conclusion is also strengthened. </p><hr><p>Let $A$ be a subset of $\mathbb{R}$ and $R$ be a field. A <strong>persistence module</strong> $\mathcal{F}<em>A$ is a collection of vector spaces ${F</em>{\alpha}}<em>{\alpha\in A}$ together with linear maps ${f</em>{\alpha}^{\alpha’}:F<em>\alpha\to F</em>{\alpha’}}$ such that for any $\alpha\le \alpha’\le\alpha’’$, we have $f^{\alpha’’}<em>\alpha=f^{\alpha’’}</em>{\alpha’}\circ f^{\alpha’}<em>\alpha$ and $f^\alpha</em>\alpha=id<em>{F</em>\alpha}$. A persistence module is called <strong>tame</strong> if each vector space is of finite dimension. In the following all persistence modules are assumed to be tame.</p><p>Let $\Delta$ be the diagonal ${(x,x)|x\in\bar{\mathbb{R}}}$ and $\Delta<em>{+}$ be the space on and upon the diagonal ${(x,y)|y\ge x}$. If $A$ is discrete with no accumulation point, we can define the <strong>persistence diagram</strong> of $\mathcal{F}_A$ to be a multiset $\mathcal{DF}_A$ in $\Delta</em>{+}$. The points in $\mathcal{DF}_A$ are in the form $(\alpha,\alpha’)$ where $\alpha\le \alpha’\in A$. For each point on the diagonal $\Delta$ the multiplicity is $\infty$. For each point off the diagonal, the multiplicity is </p><script type="math/tex; mode=display">\mu(\alpha_i,\alpha_j)=\left\{\begin{array}{l}rank(f^{\alpha_{j-1}}_{\alpha_i})-rank(f^{\alpha_j}_{\alpha_i}),\alpha_i=\inf A\\rank(f^{\alpha_{j-1}}_{\alpha_i})-rank(f^{\alpha_j}_{\alpha_i})+rank(f^{\alpha_j}_{\alpha_{i-1}})-rank(f^{\alpha_{j-1}}_{\alpha_{i-1}}),else\end{array}\right.</script><p>This coincides with the <a href="http://yueqicao.top/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/" target="_blank" rel="noopener">previous definition</a>. To define the persistence diagram of an arbitrary index set needs some preparation.</p><p>Let $B\subseteq A$ be a subset with no accumulation point. Let $\Gamma<em>B$ be the grid ${(\beta_i,\beta_j)|\beta\in B}$. To each grid $\Gamma_B$ is associated with a <em>B-pixelization map</em> $pix_B:\Delta</em>{+}\to \Gamma_B\cup\Delta$. If a cell does not hit the diagonal, $pix_B$ sends each point in the cell to the upper right grid point. If a cell hits the diagonal, then each point is send to the closest point on the diagonal.</p><p><img src="/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/pixelization.png" alt="pixelizationmap"></p><p>Since we can always pick out a subsequence in the persistence module, it is natural to define the persistence diagram of $\mathcal{F}_A$ through an approximation of $\mathcal{DF}_B$ with $B\subseteq A$. </p><p>Let us say a persistence module $\mathcal{F}_B$ is $\epsilon$-periodic is the index set $B=\alpha_0+\epsilon \mathbb{Z}$. Then we have the following lemma</p><blockquote><p> Let $B,C\subseteq A$. The pixelization map $pix<em>B$ defines a multi-bijection between $\mathcal{DF}</em>{B\cup C}$ and $\mathcal{DF}_B$. Furthermore, for any $\epsilon$-periodic persistence modules $\mathcal{F}_B$ and $\mathcal{F}_C$, we have $d_B^{\infty}(\mathcal{DF}_B,\mathcal{DF}_A)\le \epsilon$. </p></blockquote><p>Therefore, Set $\epsilon=2^{-n}$, the persistence diagrams $\mathcal{DF}_{B_n}$ will form a Cauchy sequence under bottleneck distance. The diagram of $\mathcal{F}_A$ is defined to be the limit of this sequence.</p><hr><p>As mentioned in the beginning, the interleaving between level sets of spaces become interleaving between persistence modules. Two persistence modules $\mathcal{F}<em>{\mathbb{R}}$ and $\mathcal{G}</em>{\mathbb{R}}$ are <strong>strongly $\epsilon$-interleaved</strong> if there exists two families of homomorphisms ${\phi<em>\alpha:F</em>\alpha\to G<em>{\alpha+\epsilon}}$ and ${\psi</em>{\alpha}:G<em>\alpha:\to F</em>{\alpha+\epsilon}}$ such that the diagram in the beginning commutes. Then we have the following theorem about stability</p><blockquote><p>Let  $\mathcal{F}<em>{\mathbb{R}}$ and $\mathcal{G}</em>{\mathbb{R}}$ be tame persistence modules. If  $\mathcal{F}<em>{\mathbb{R}}$ and $\mathcal{G}</em>{\mathbb{R}}$ are strongly $\epsilon$-interleaved, then $d<em>B^{\infty}(\mathcal{DF}</em>{\mathbb R},\mathcal{DG}_{\mathbb R})\le \epsilon$.</p></blockquote><p>The proof of this theorem is lengthy. However, for persistence diagrams with finite points off the diagonal, the idea is nothing but an algebraic analogy to the geometric stability proof — one constructs a family of persistence modules $\mathcal{H}^s$ which interpolates between  $\mathcal{F}<em>{\mathbb{R}}$ and $\mathcal{G}</em>{\mathbb{R}}$. Then, for close enough $\mathcal{H}^s$ and $\mathcal{H}^{s’}$ the distance between persistence diagrams can be bounded by the <a href="http://yueqicao.top/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/" target="_blank" rel="noopener">Box Lemma</a>. The bound for $\mathcal{DF}<em>{\mathbb R}$ and $\mathcal{DG}</em>{\mathbb R}$ is obtained by triangle inequality.    </p><p>For general persistence diagram (with possibly infinite points off the diagonal), more techniques are involved. Two persistence diagrams $\mathcal{F}_A$ and $\mathcal{G}_B$ are <strong>weakly $\epsilon$-interleaved</strong> if there exists some $\alpha_0$ such that $\alpha_0+2\epsilon\mathbb Z\subseteq A$ and $\alpha_0+\epsilon+2\epsilon\mathbb Z\subseteq B$ and there are homomorphisms such that the following diagram commutes</p><p><img src="/2019/09/12/Stability-Theorems-in-Topological-Data-Analysis-2/weaklyinter.png" alt="weakly interleaving"></p><p>One needs to prove a weakly stability theorem to assist the strong one.</p><blockquote><p>Let  $\mathcal{F}<em>{A}$ and $\mathcal{G}</em>{B}$ be tame persistence modules. If  $\mathcal{F}_A$ and $\mathcal{G}_B$ are weakly $\epsilon$-interleaved, then $d_B^{\infty}(\mathcal{DF}_A,\mathcal{DG}_B)\le 3\epsilon$, and the bound is tight.</p></blockquote><p>To obtain the $3\epsilon$ bound we need to investigate the pixelization map carefully. But we can use the pixelization lemma to prove a loose bound easily. Let $\mathcal{H}$ be the persistence module</p><script type="math/tex; mode=display">\cdots\to G_{\alpha_0+(2n-1)\epsilon}\to F_{\alpha_0+2n\epsilon}\to G_{\alpha_0+(2n+1)\epsilon}\to\cdots</script><p>Then $\mathcal{F}<em>{\alpha_0+2\epsilon\mathbb Z}$ and $\mathcal{G}</em>{\alpha<em>0+\epsilon+2\epsilon\mathbb Z}$ are subcollections of $\mathcal{H}$. By the pixelization lemma, we have $d_B^{\infty}(\mathcal{DF}</em>{\alpha<em>0+2\epsilon\mathbb Z},\mathcal{DG}</em>{\alpha<em>0+\epsilon+2\epsilon\mathbb Z})\le 2\epsilon$. Similarly, we have $d_B^{\infty}(\mathcal{DF}</em>{\alpha<em>0+2\epsilon\mathbb Z},\mathcal{DF}_A)\le 2\epsilon$ and $d_B^{\infty}(\mathcal{DG}</em>{\alpha_0+\epsilon+2\epsilon\mathbb Z},\mathcal{DG}_B)\le 2\epsilon$. By triangle inequality, we have the bound $6\epsilon$.</p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Curvature of Warped Products</title>
      <link href="/2019/09/03/Curvature-of-Warped-Products/"/>
      <url>/2019/09/03/Curvature-of-Warped-Products/</url>
      
        <content type="html"><![CDATA[<h2 id="Curvature-of-Coupled-Planes"><a href="#Curvature-of-Coupled-Planes" class="headerlink" title="Curvature of Coupled Planes"></a>Curvature of Coupled Planes</h2><p>Consider the product of two planes $\mathbb R^2\times\mathbb R^2$, with the following Riemannian metric</p><script type="math/tex; mode=display">ds^2=dx^2+dy^2+e^{2f(x,y)}(du^2+dv^2),</script><p>where $f(x,y)$ is a smooth function. We compute the sectional/Ricci/scalar curvature of the ‘coupled’ planes using orthonormal frames. Basics about moving frames are referred to <a href="https://link.springer.com/book/10.1007/978-3-319-55084-8" target="_blank" rel="noopener">Loring W. Tu</a>.    </p><a id="more"></a><p>Consider the following orthonormal bases</p><script type="math/tex; mode=display">e_1=\partial_x,e_2=\partial_y,e_3=e^{-f}\partial_u,e_4=e^{-f}\partial_v.</script><p>The dual 1-forms are </p><script type="math/tex; mode=display">\theta^1=dx,\theta^2=dy,\theta^3=e^{f}du,\theta^4=e^fdv.</script><p>There exists a unique skew-symmetric matrix of 1-forms $[\omega^i_j]$ such that</p><script type="math/tex; mode=display">d\theta^i+\omega^i_j\wedge\theta^j=0.</script><p>The $\omega^i_j$’s are called the connection 1-forms, and the above equation is called <em>the first structure equation</em>. Suppose $\omega^i_j=a^i_jdx+b^i_jdy+c^i_jdu+d^i_jdv$. Substituting into the first structure equation, we solve all the connection 1-forms</p><script type="math/tex; mode=display">\begin{aligned}&\omega^1_2=\omega^3_4=0,\\&\omega^1_3=-e^ff_xdu,\omega^1_4=-e^ff_xdv,\\&\omega^2_3=-e^ff_ydu,\omega^2_4=-e^ff_ydv.\end{aligned}</script><p>The curvature 2-forms are defined by <em>the second structure equation</em></p><script type="math/tex; mode=display">\Omega^i_j=d\omega^i_j+\omega^i_k\wedge\omega^k_j.</script><p>Therefore, we write out all the curvature forms</p><script type="math/tex; mode=display">\begin{aligned}&\Omega^1_2=0,\\&\Omega^1_3=-e^f(f_{xx}+f_x^2)dx\wedge du-e^f(f_{xy}+f_xf_y)dy\wedge du,\\&\Omega^1_4=-e^f(f_{xx}+f_x^2)dx\wedge dv-e^f(f_{xy}+f_xf_y)dy\wedge dv,\\&\Omega^2_3=-e^f(f_{xy}+f_xf_y)dx\wedge du-e^f(f_{yy}+f_y^2)dy\wedge du,\\&\Omega^2_4=-e^f(f_{xy}+f_xf_y)dx\wedge dv-e^f(f_{yy}+f_y^2)dy\wedge dv,\\&\Omega^3_4=-e^{2f}(f_x^2+f_y^2)du\wedge dv.\end{aligned}</script><p>By the definition of sectional curvature, for any plane spanned by $e_i,e_j$,</p><script type="math/tex; mode=display">K_{ij}=\langle R(e_i,e_j)e_j,e_i\rangle=\Omega_j^i(e_i,e_j).</script><p>Hence, we can compute all the sectional curvature using curvature forms,</p><script type="math/tex; mode=display">\begin{aligned}&K_{12}=0,\\&K_{13}=K_{14}=-(f_{xx}+f_x^2),\\&K_{23}=K_{24}=-(f_{yy}+f_y^2),\\&K_{34}=-(f_x^2+f_y^2).\end{aligned}</script><p>Ricci curvature is the sum of sectional curvatures, thus,</p><script type="math/tex; mode=display">\begin{aligned}&Ric_{11}=K_{12}+K_{13}+K_{14}=-2(f_{xx}+f_x^2),\\&Ric_{22}=K_{12}+K_{23}+K_{24}=-2(f_{yy}+f_y^2),\\&Ric_{33}=K_{13}+K_{23}+K_{34}=-(f_{xx}+f_{yy}+2f_x^2+2f_y^2),\\&Ric_{44}=K_{14}+K_{24}+K_{34}=-(f_{xx}+f_{yy}+2f_x^2+2f_y^2),\\\end{aligned}</script><p>Scalar curvature is the sum of Ricci curvature, thus,</p><script type="math/tex; mode=display">S=Ric_{11}+Ric_{22}+Ric_{33}+Ric_{44}.</script><h2 id="Curvature-of-4-Dimensional-Warped-Product-Spaces"><a href="#Curvature-of-4-Dimensional-Warped-Product-Spaces" class="headerlink" title="Curvature of 4 Dimensional Warped Product Spaces"></a>Curvature of 4 Dimensional Warped Product Spaces</h2><p>More generally, let $M$ and $N$ be two Riemannian manifolds with metric $g_M$ and $g_N$, respectively. Consider the product space $M\times N$ with the following metric</p><script type="math/tex; mode=display">g=g_M+e^{2f}g_N,</script><p>where $f$ is a smooth function on $M$. This is called the <strong>warped products</strong> of $M$ and $N$, and often denoted by $M\times_{e^f}N$ (see <a href="https://max.book118.com/html/2019/0228/6024223150002012.shtm" target="_blank" rel="noopener">John Lee, Example 2.24</a>).  Let $\theta^1,\theta^2$ be the orthonormal coframe on $M$ and $\omega^1_2$ be the corresponding connection form. Similarly, let $\theta^3,\theta^4,\omega^3_4$ be the orthonormal coframe and connection form on $N$. For the product space $M\times N$, we have the orthonormal coframe</p><script type="math/tex; mode=display">\bar{\theta^1}=\theta^1,\bar{\theta^2}=\theta^2,\bar{\theta^3}=e^f\theta^3,\bar{\theta^4}=e^f\theta^4.</script><p>Assume that the connection 1-form for $M\times N$ is</p><script type="math/tex; mode=display">\bar{\omega^i_j}=a^i_j\bar{\theta_1}+b_j^i\bar{\theta_2}+c^i_j\bar{\theta_3}+d^i_j\bar{\theta_4}.</script><p>Substituting the connection 1-forms into the first structure equation, we have</p><script type="math/tex; mode=display">\begin{aligned}&\bar{\omega^1_2}=\omega^1_2,\\&\bar{\omega^1_3}=-e^{-f}\frac{de^f}{\theta^1}\bar{\theta^3},\\&\bar{\omega^1_4}=-e^{-f}\frac{de^f}{\theta^1}\bar{\theta^4},\\&\bar{\omega^2_3}=-e^{-f}\frac{de^f}{\theta^2}\bar{\theta^3},\\&\bar{\omega^2_4}=-e^{-f}\frac{de^f}{\theta^2}\bar{\theta^4},\\&\bar{\omega^3_4}=\omega^3_4.\end{aligned}</script><p>Since $de^f$ is a 1-form on $M$, it can be written as a linear combination of $\theta^1$ and $\theta^2$. Let $de^f/\theta^1$ and $de^f/\theta^2$ be the coefficients. Similarly, let $\omega^1_2=a\theta^1+b\theta^2$, by structure equation, $d\theta^1=-\omega^1_2\wedge\theta^2=-a\theta^1\wedge\theta^2$, $d\theta^2=-\omega^2_1\wedge\theta^1=-b\theta^1\wedge\theta^2$. We can write </p><script type="math/tex; mode=display">\begin{aligned}&\omega^1_2=-\frac{d\theta^1}{\theta^1\wedge\theta^2}\theta^1-\frac{d\theta^2}{\theta^1\wedge\theta^2}\theta^2,\\&\omega^3_4=-\frac{d\theta^3}{\theta^3\wedge\theta^4}\theta^3-\frac{d\theta^4}{\theta^3\wedge\theta^4}\theta^4.\end{aligned}</script><p>With these notations, we can list all the curvature 2-forms </p><script type="math/tex; mode=display">\begin{aligned}&\bar{\Omega^1_2}=d\omega^1_2=\Omega^1_2,\\&\bar{\Omega^1_3}=(\frac{de^f}{\theta^2}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^1)^2})\theta^1\wedge\theta^3+(\frac{de^f}{\theta^2}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{\theta^1\theta^2})\theta^2\wedge\theta^3,\\&\bar{\Omega^1_4}=(\frac{de^f}{\theta^2}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^1)^2})\theta^1\wedge\theta^4+(\frac{de^f}{\theta^2}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{\theta^1\theta^2})\theta^2\wedge\theta^4,\\&\bar{\Omega^2_3}=(-\frac{de^f}{\theta^1}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{\theta^2\theta^1})\theta^1\wedge\theta^3+(-\frac{de^f}{\theta^1}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^2)^2})\theta^2\wedge\theta^3,\\&\bar{\Omega^2_4}=(-\frac{de^f}{\theta^1}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{\theta^2\theta^1})\theta^1\wedge\theta^4+(-\frac{de^f}{\theta^1}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^2)^2})\theta^2\wedge\theta^4,\\&\bar{\Omega^3_4}=\Omega^3_4-((\frac{de^f}{\theta^1})^2+(\frac{de^f}{\theta^2})^2)\theta^3\wedge\theta^4.\end{aligned}</script><p>Hence, the sectional curvatures are </p><script type="math/tex; mode=display">\begin{aligned}&\bar{K}_{12}=K_{12},\\&\bar{K}_{13}=\bar{K}_{14}=e^{-f}(\frac{de^f}{\theta^2}\frac{d\theta^1}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^1)^2}),\\&\bar{K}_{23}=\bar{K}_{24}=e^{-f}(-\frac{de^f}{\theta^1}\frac{d\theta^2}{\theta^1\wedge\theta^2}-\frac{d^2e^f}{(\theta^2)^2}),\\&\bar{K}_{34}=e^{-2f}(K_{34}-(\frac{de^f}{\theta^1})^2-(\frac{de^f}{\theta^2})^2).\end{aligned}</script><h2 id="More-Examples"><a href="#More-Examples" class="headerlink" title="More Examples"></a>More Examples</h2><p>Let $\mathbb S^2$ be the unit sphere in $\mathbb R^3$. Using stereographic projection, the <em>round metric</em> in local coordinate $(x,y)$ is </p><script type="math/tex; mode=display">g_{\mathbb S^2}=\frac{4}{(1+x^2+y^2)^2}(dx^2+dy^2).</script><p>Let $\theta^1=2/(1+x^2+y^2)dx$ and $\theta^2=2/(1+x^2+y^2)dy$ be the orthonormal coframe. We compute that the connection 1-form is $\omega^1_2=-y\theta^1+x\theta^2$. Therefore, </p><script type="math/tex; mode=display">\frac{d\theta^1}{\theta^1\wedge\theta^2}=y,\frac{d\theta^2}{\theta^1\wedge\theta^2}=-x.</script><p>Denote $(1+x^2+y^2)/2$ by $J$. Then,</p><script type="math/tex; mode=display">\begin{aligned}&\frac{de^f}{\theta^1}=\frac{e^ff_xdx+e^ff_ydy}{\theta^1}=e^ff_xJ,\\&\frac{d^2e^f}{(\theta^1)^2}=\frac{d(e^ff_xJ)}{\theta^1}=e^f(f_x)^2J^2+e^ff_{xx}J^2+xe^ff_xJ,\\&\frac{d^2e^f}{\theta^1\theta^2}=\frac{d(e^ff_xJ)}{\theta^2}=e^ff_xf_yJ^2+e^ff_{xy}J^2+ye^ff_xJ,\\\end{aligned}</script><p>Similarly, we have</p><script type="math/tex; mode=display">\begin{aligned}&\frac{de^f}{\theta^2}=\frac{e^ff_xdx+e^ff_ydy}{\theta^2}=e^ff_yJ,\\&\frac{d^2e^f}{(\theta^2)^2}=\frac{d(e^ff_xJ)}{\theta^2}=e^f(f_y)^2J^2+e^ff_{yy}J^2+ye^ff_yJ,\\&\frac{d^2e^f}{\theta^2\theta^1}=\frac{d(e^ff_xJ)}{\theta^1}=e^ff_xf_yJ^2+e^ff_{xy}J^2+xe^ff_yJ,\\\end{aligned}</script><p>Note that $\frac{d^2e^f}{\theta^1\theta^2}\neq\frac{d^2e^f}{\theta^2\theta^1}$. </p><p>Let $\mathbb H^2$ be the upper plane with <em>hyperbolic metric</em>  </p><script type="math/tex; mode=display">g_{\mathbb H^2}=\frac{1}{v^2}(du^2+dv^2).</script><p>Under this metric $\mathbb H^2$ will be a Riemannian manifold with constant curvature $-1$. Consider the warped product $\mathbb S^2\times_f \mathbb H^2$. The sectional curvatures are </p><script type="math/tex; mode=display">\begin{aligned}&\bar{K}_{12}=1,\\&\bar{K}_{13}=\bar{K}_{14}=yf_yJ-(f_x)^2J^2-f_{xx}J^2-xf_xJ,\\&\bar{K}_{23}=\bar{K}_{24}=xf_xJ-(f_y)^2J^2-f_{yy}J^2-yf_yJ,\\&\bar{K}_{34}=e^{-2f}(-1-e^{2f}(f_x)^2J^2-e^{2f}(f_y)^2J^2).\end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> Geometry </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Warped Products </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Remarks on &#39;Submanifold density estimation&#39;</title>
      <link href="/2019/08/08/Remarks-on-Submanifold-density-estimation/"/>
      <url>/2019/08/08/Remarks-on-Submanifold-density-estimation/</url>
      
        <content type="html"><![CDATA[<p>There seems to be a gap in <a href="http://papers.nips.cc/paper/3826-submanifold-density-estimation" target="_blank" rel="noopener">Submanifold density estimation</a>.</p><p>In the deduction of variance, one needs to estimate </p><blockquote><p>$\frac{1}{h_m^n}\int_M f(q)\frac{1}{h_m^n}K^2(\frac{u_p(q)}{h_m})dV(q)$</p></blockquote><p>Apply theorem 3.1 to function $K^2$ one obtains</p><script type="math/tex; mode=display">\frac{1}{h^n}\int_M K^2(\frac{u_p(q)}{h})\xi(q)dV(q)\to\xi(p)\int_{\mathbb{R}^n}K^2(\|z\|)d^nz,h\to 0.</script><p>It seems $h_m^{2n}$ will cause trouble in estimation. Whatever, they claimed that the integration will converges to $f(p)\int K^2(|z|)d^nz$. However, in the expression of variance</p><blockquote><p>$Var[\frac{1}{h_m^n}K(\frac{u_p(q)}{h_m})]=E[\frac{1}{h_m^{2n}}K^2(\frac{u_p(q)}{h_m})]-(E[\frac{1}{h_m^n}K(\frac{u_p(q)}{h_m})])^2$</p></blockquote><p>the first term will converge to a constant times $f(p)$, while the latter will converge to $f(p)^2$. Thus the variance will not converge to 0.</p><p>The idea of proof is straightforward. Since $K$ will be supported in $[0,1]$, one notes that when the bandwidth $h$ is small, the integration will be zero outside the normal coordinate. Therefore, everything goes back to Euclidean space.</p><a id="more"></a> ]]></content>
      
      
      <categories>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nonparametric Estimates </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stability Theorems in Topological Data Analysis (1)</title>
      <link href="/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/"/>
      <url>/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/</url>
      
        <content type="html"><![CDATA[<hr><p><strong>Personal Comments</strong></p><p>In this series I attempt to collect results concerning stabilities in <a href="https://en.wikipedia.org/wiki/Topological_data_analysis" target="_blank" rel="noopener">topological data analysis</a>. Researches in this direction has been increasing dramatically in recent years. Without doubt there will be more in the future. However, the best will not appear until one really understands all the progresses.</p><hr><p>The first stability theorem in TDA is proved by <a href="https://link.springer.com/article/10.1007%2Fs00454-006-1276-5" target="_blank" rel="noopener">David Cohen-Steiner, Herbert Edelsbrunner, John Harer</a>. It asserts that the bottleneck distance between two persistence diagrams is bounded by the $\infty$-norm of tame functions. They soon generalized this result to $L_p$ cases for tame Lipschitz functions (see <a href="https://faculty.math.illinois.edu/~ymileyko/papers/L_p-stability.pdf" target="_blank" rel="noopener">here</a>). This theorem is fundamental in TDA. And the idea is also direct — counting points in persistence diagrams. We sketch the procedure in the following.</p><a id="more"></a><p>Recall that a <em>persistence diagram</em> is a multiset in the extended plane $\bar{\mathbb R}=\mathbb R\cup {\infty}$. A <em>tame</em> function $f:X\to\mathbb R$ defined on a topological space induces a finite filtration</p><script type="math/tex; mode=display">\emptyset=f^{-1}((-\infty,a_0])\to\cdots\to f^{-1}((-\infty,a_i])\to\cdots\to f^{-1}((-\infty,a_n])=X.</script><p>Each $a<em>i$ is called a homological critical value. Thus, at $a_i$ the homotopy type of the sublevel sets of $f$ is changed. Tameness means that it changes finite types. Let $b_i&lt;a_i&lt;b</em>{i+1}$ be interleaved sequences. The maps $H<em>k(f^{-1}(-\infty,b_i])\to H_k(f^{-1}(-\infty,b</em>{i+1}])$  are <strong>not</strong> isomorphisms. For simplicity, let us drop the subscript $k$, and denote the homology group by $H(b<em>i)$. Let $f</em>{x,y}$ denote the map $H(x)\to H(y)$. The image of $f_{x,y}$ is denoted by $H(x,y)$. Set $\beta(x,y)={\rm dim} H(x,y)$. We can define the <em>multiplicity</em> of a point $(a_i,a_j)$ in persistence diagram $D(f)$ as</p><script type="math/tex; mode=display">\mu(i,j)=\beta(b_{i-1},b_j)-\beta(b_i,b_j)+\beta(b_i,b_{j-1})-\beta(b_{i-1},b_{j-1}).</script><p>Then the <em>total multiplicity</em> (or <em>size</em>) of the persistence diagram over the diagonal is </p><script type="math/tex; mode=display">\#(D(f)-\Delta)=\sum_{i<j}\mu(i,j).</script><p>Next we <strong>count</strong> multiplicities in different areas on the diagram.</p><hr><p>Let $Q_x^y=[-\infty,x]\times[y,\infty]$ be the closed upper left quadrant. <strong>k-Triangle Lemma</strong> asserts that for $x&lt;y$ different from homological critical values, then</p><script type="math/tex; mode=display">\#(D(f)\cap Q_x^y)=\beta(x,y).</script><p>The proof is shown in the following figure. Note that the small square with plus and minus notations explains the multiplicity at a point. When summing over, plus and minus cancels. </p><p><img src="/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/ktriangle.png" alt="k-triangle lemma"> </p><hr><p>Let $f$ and $g$ be two tame functions on the same topological space. Let $\epsilon=|f-g|_{\infty}$. We want to compare the multiplicities of quadrants for $f$ and $g$. In fact, the <strong>Quadrant Lemma</strong> asserts that</p><script type="math/tex; mode=display">\#(D(f)\cap Q_{x-\epsilon}^{y+\epsilon})\le \#(D(g)\cap Q_x^y).</script><p>Symmetrically, we also have </p><script type="math/tex; mode=display">\#(D(g)\cap Q_{x-\epsilon}^{y+\epsilon})\le \#(D(f)\cap Q_x^y).</script><p>Intuitively, when we shift the quadrant upper left by $\epsilon$, the multiplicity decreases. </p><p><img src="/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/quadrant.png" alt="quadrant lemma"></p><hr><p>Let $R=[a,b]\times[c,d]$ be a closed rectangle where $a&lt;b&lt;c&lt;d$. Let $R_{\epsilon}=[a+\epsilon,c-\epsilon]\times[c+\epsilon,d-\epsilon]$ be a smaller rectangle. The <strong>Box Lemma</strong> asserts that the points in the smaller box have less multiplicities. </p><script type="math/tex; mode=display">\#(D(f)\cap R_{\epsilon})\le \#(D(g)\cap R).</script><p>Similarly, the positions of $f$ and $g$ can be exchanged in the inequality. </p><p><img src="/2019/08/01/Stability-Theorems-in-Persistent-Homology-1/box.png" alt="box lemma"></p><hr><p>Let $A$ and $B$ be multisets in the extended plane. The Hausdorff distance between $A$ and $B$ is defined as </p><script type="math/tex; mode=display">d_{\mathcal H}(A,B)=\max\{\sup_x\inf_y\|x-y\|_{\infty},\sup_y\inf_x\|y-x\|_{\infty}\},</script><p>where $x\in A$ and $y\in B$. Using box lemma, we can easily deduce that</p><script type="math/tex; mode=display">d_{\mathcal H}(D(f),D(g))\le \|f-g\|_{\infty},</script><p>since for each point in $D(f)$, there is at least one point in $D(g)$. </p><hr><p>The final step is to generalize the inequality to bottleneck distance. By definition, the bottleneck distance between $A$ and $B$ is </p><script type="math/tex; mode=display">d_{\mathcal B}(A,B)=\inf_{\gamma}\sup_x\|x-\gamma(x)\|_{\infty},</script><p>where $\gamma:A\to B$ is <strong>bijection</strong>. Note that $D(f)$ contains the diagonal with infinite points. Therefore, the bottleneck distance is well defined. Since there are finitely many points off the diagonal in a persistence diagram, we can assume that $f$ and $g$ are so close that in a square of length $|f-g|<em>{\infty}$ there is only one point in $D(f)$. On the other hand, by box lemma we also have a point in $D(g)$. This enables us to construct a bijection but the distance is $|f-g|</em>{\infty}$. For general $f$ and $g$, we interpolate piecewise linear functions and use triangle inequality to pass the conclusion to $\hat{f}$ and $\hat{g}$. In a word, we obtain the main theorem </p><blockquote><p>Let $X$ be a triangulable space with continuous tame functions $f,h:X\to \mathbb R$. Then the persistence diagrams satisfy $d<em>{\mathcal B}(D(f),D(g))\le|f-g|</em>{\infty}$.</p></blockquote><hr><p>In proving the quadrant lemma and box lemma, one notice that there is an interleaving phenomenon of the level sets of $f$ and $g$. This interleaving is generalized to modules, which is purely algebraic. See <a href="https://geometry.stanford.edu/papers/ccggo-ppmd-09/ccggo-ppmd-09.pdf" target="_blank" rel="noopener">F. Chazal etc.</a> for references.  </p>]]></content>
      
      
      <categories>
          
          <category> TDA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDA Stability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MSE of Locally Linear Regression</title>
      <link href="/2019/07/30/MSE-of-Locally-Linear-Regression/"/>
      <url>/2019/07/30/MSE-of-Locally-Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h2 id="Backgrounds"><a href="#Backgrounds" class="headerlink" title="Backgrounds"></a>Backgrounds</h2><p>Locally linear regression (also called locally weighted least squares) is an important method in nonparametric regression. Its understanding is intuitive. Suppose one has a general model</p><script type="math/tex; mode=display">y=g(x)+\epsilon(x),</script><p>where $g$ is a any smooth function, and $\epsilon(x)$ is a variable with zero mean and finite variance. Let $X_1,X_2,\cdots,X_n$ be i.i.d. samples with bounded density $f$, and $Y_1,Y_2,\cdots, Y_n$ be the corresponding responses (assume $X_i$’s are scalars ‘). By Taylor expansion, we have</p><script type="math/tex; mode=display">Y_i-g(x)\approx \beta(X_i-x)+\epsilon(x).</script><a id="more"></a><p>To emphasize the local information around $x$, we introduce the kernel function $K_h(u)=K(u/h)/h$, where $\int K(u){\rm d}u=1$ and $h&gt;0$ is called the bandwidth.  And consider the following optimization</p><script type="math/tex; mode=display">\arg\min_{g,\beta}\sum_{i=1}^n(Y_i-g-\beta(X_i-x))^2K_h(X_i-x).</script><p>We introduce the following notations</p><script type="math/tex; mode=display">\begin{gather}{\bf Y}=[Y_1,Y_2,\cdots,Y_n]^T,\\{\bf X}=\left[\begin{array}{cc}1&X_1-x\\1&X_2-x\\ \cdots&\cdots\\ 1&X_n-x\end{array}\right],\\{\bf W}={\rm diag}\{K_h(X_1-x),\cdots,K_h(X_n-x)\},\\e_1=[1,0]^T,\\e_2=[0,1]^T.\end{gather}</script><p>Then the optimization has a closed solution</p><script type="math/tex; mode=display">\begin{gather}\hat{g}(x)=e_1^T({\bf X^TWX})^{-1}{\bf X^TWY},\\\hat{\beta}=e_2^T({\bf X^TWX})^{-1}{\bf X^TWY}.\end{gather}</script><p>For the present we analyze the conditional bias and variance of $\hat{g}(x)$. Note that by assumptions we have $\mathbb E[Y|X]=g(X)$,$\mathbb E[(Y-g(X))^2|X]=\sigma^2(X)$. Let $g({\bf X})=[g(X_1),g(X_2),\cdots,g(X_n)]^T$. We have the following decomposition</p><script type="math/tex; mode=display">\begin{aligned}\hat{g}(x)-g(x)=&e_1^T({\bf X^TWX})^{-1}{\bf X^TW}({\bf Y}-g({\bf X}))\\&+e_1^T({\bf X^TWX})^{-1}{\bf X^TW}g({\bf X})-g(x).\end{aligned}</script><p>Taking expectations on both sizes, we have</p><script type="math/tex; mode=display">\begin{aligned}\mathbb E[(\hat{g}(x)-g(x))^2|{\bf X}]=&\mathbb E[(e_1^T({\bf X^TWX})^{-1}{\bf X^TW}({\bf Y}-g({\bf X})))^2|{\bf X}]\\&+(e_1^T({\bf X^TWX})^{-1}{\bf X^TW}g({\bf X})-g(x))^2.\end{aligned}</script><p>The first term is variance and the second term is square of bias.</p><h2 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h2><p>Let ${\bf \Sigma}={\rm diag}{\sigma^2(X_1),\cdots,\sigma^2(X_n)}$. Variance can be simplified as</p><script type="math/tex; mode=display">e_1^T({\bf X^TWX})^{-1}{\bf X^TW\Sigma WX}({\bf X^TWX})^{-1}e_1.</script><p>Denote $\sum_{i=1}^n K_h(X_i-x)(X_i-x)^j$ by $S_j$ for $j=0,1,2$. Then we have </p><script type="math/tex; mode=display">{\bf X^TWX}=\left[\begin{array}{cc}S_0&S_1\\S_1&S_2\end{array}\right].</script><p>Note that</p><script type="math/tex; mode=display">\mathbb E[S_j/nh^j|{\bf X}]=\int K(u)u^jf(x+hu){\rm d}u=\mu_jf(x)+o(1),</script><p>where $\mu_j=\int K(u)u^j{\rm d}u&lt;\infty$. Furthermore,</p><script type="math/tex; mode=display">\begin{aligned}{\rm Var}[S_j/nh^j|{\bf X}]&\le\frac{1}{n}\mathbb E[K_h(X_i-x)^2((X_i-x)/h)^{2j}|{\bf X}]\\&\le \frac{1}{nh}\int K(u)^2u^{2j}f(x+hu){\rm d}u.\end{aligned}</script><p>Therefore, $\rm Var\to 0$ as $nh\to \infty$. By Chebyshev’s inequality, we can write </p><script type="math/tex; mode=display">S_j/nh^j=\mu_jf(x)+o_{\mathbb P}(1),</script><p>where $o_{\mathbb P}(1)$ denotes an infinitesimal in probability. Let ${\bf H}={\rm diag}{1,h}$. Then </p><script type="math/tex; mode=display">\frac{1}{n}{\bf H^{-1}X^TWXH^{-1}}=f(x)\left[\begin{array}{cc}1&0\\0&\mu_2\end{array}\right]+o_{\mathbb P}(1),</script><p>where we assume that $\mu_j=0$ for odd $j$. </p><p>Let $\nu_j=\int K(u)^2u^j{\rm d}u$. By a similar argument we have </p><script type="math/tex; mode=display">\frac{1}{n}{\bf H^{-1}X^TW\Sigma WXH^{-1}}=f(x)\sigma^2(x)\left[\begin{array}{cc}\nu_0&0\\0&\nu_2\end{array}\right]+o_{\mathbb P}(1).</script><p>Now we can obtain that</p><script type="math/tex; mode=display">\begin{aligned}&e_1^T({\bf X^TWX})^{-1}{\bf X^TW\Sigma WX}({\bf X^TWX})^{-1}e_1\\=&\frac{1}{nh}e_1^T{\bf H^{-1}}({\bf H^{-1}X^TWXH^{-1}}/n)^{-1}(\frac{h}{n}{\bf H^{-1}X^TW\Sigma WXH^{-1}})\\&({\bf H^{-1}X^TWXH^{-1}}/n)^{-1}{\bf H^{-1}}e_1\\=&\frac{1}{nh}\left(\left(e_1^T\left[\begin{array}{cc}1&0\\0&\mu_2\end{array}\right]^{-1}\left[\begin{array}{cc}\nu_1&0\\0&\nu_2\end{array}\right]\left[\begin{array}{cc}1&0\\0&\mu_2\end{array}\right]^{-1}e_1\right)\frac{\sigma^2(x)}{f(x)}+o_{\mathbb P}(1)\right)\\=&\frac{1}{nh}(\frac{\nu_0\sigma^2(x)}{f(x)}+o_{\mathbb P}(1)).\end{aligned}</script><h2 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h2><p>Consider the Taylor expansion for $g$,</p><script type="math/tex; mode=display">g(X_i)=g(x)+g'(x)(X_i-x)+\frac{1}{2}g''(x)(X_i-x)^2+\frac{1}{6}g'''(\bar{X}_i)(X_i-x)^3.</script><p>Write $r(X_i)=g(X_i)-g(x)-g’(x)(X_i-x)$. Then we have</p><script type="math/tex; mode=display">g({\bf X})=g(x){\bf X}e_1+g'(x){\bf X}e_2+r({\bf X}).</script><p>Therefore, the bias term is </p><script type="math/tex; mode=display">\begin{aligned}&e_1^T({\bf X^TWX})^{-1}{\bf X^TW}g({\bf X})-g(x)\\=&g(x)e_1^T({\bf X^TWX})^{-1}{\bf X^TWX}e_1+g'(x)e_1^T({\bf X^TWX})^{-1}{\bf X^TWX}e_2\\&+e_1^T({\bf X^TWX})^{-1}{\bf X^TW}r({\bf X})-g(x)\\=&e_1^T({\bf X^TWX})^{-1}{\bf X^TW}r({\bf X}).\end{aligned}</script><p>For the second order term we have the following estimation</p><script type="math/tex; mode=display">\frac{g''(x)}{2nh^2}{\bf H^{-1}X^TW}[(X_1-x)^2,\cdots,(X_n-x)^2]^T=\left(\begin{array}{c}\frac{1}{2}\mu_2f(x)g''(x)\\0\end{array}\right)+o_{\mathbb P}(1).</script><p>For the third order term, we assume that $g’’’(\bar{X}_i)$ are bounded. The matrix norm is bounded by</p><script type="math/tex; mode=display">\begin{aligned}&\|\frac{1}{nh^2}{\bf H^{-1}X^TW}[(X_1-x)^3g(\bar{X}_1),\cdots,(X_n-x)^3g(\bar{X}_n)]^T\|\\\le & C\max\{\frac{1}{nh^2}\sum_{i=1}^n K_h(X_i-x)|X_i-x|^3,\frac{1}{nh^2}S_4\}.\end{aligned}</script><p>By a similar argument as above, the bound is $o_{\mathbb P}(1)$ when $n\to\infty$. Therefore, the bias is</p><script type="math/tex; mode=display">\begin{aligned}&e_1^T({\bf X^TWX})^{-1}{\bf X^TW}r({\bf X})\\=&h^2e_1^T{\bf H^{-1}}(\frac{1}{n}{\bf H^{-1}X^TWXH^{-1}})^{-1}\frac{1}{nh^2}{\bf H^{-1}X^TW}r({\bf X})\\=&\frac{h^2}{2}\left(g''(x)e_1^T\left[\begin{array}{cc}1&0\\0&\mu_2\end{array}\right]^{-1}\left[\begin{array}{cc}\mu_2\\\mu_3\end{array}\right]+o_{\mathbb P}(1)\right)\\=&\frac{h^2}{2}(g''(x)\mu_2+o_{\mathbb P}(1))\end{aligned}</script><p>Note that if $g$ is a linear function then we will <strong>NOT</strong> have bias.</p><h2 id="Derivative-Estimation"><a href="#Derivative-Estimation" class="headerlink" title="Derivative Estimation"></a>Derivative Estimation</h2><p>Finally we consider the conditional bias and variance for $\hat{\beta}$. The main difference is that ${\bf H}^{-1}e_2=e_2/h$. Therefore we will have order $O(1/{nh^3})$ in variance, and $O(h)$ in bias.  </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>The whole procedure can be generalized to multivariate case, where $X$ is a vector and $K(\cdot)$ is a multivariate kernel. The details are displayed in <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1176325632" target="_blank" rel="noopener">D. Ruppert and M.P. Wand</a>‘s paper.</p><p>Also in this paper, they considered locally polynomial regression for univariate case. Conditional bias and variance are deduced for derivatives in any order. They also explained why one can not drop <em>condition</em> in asymptotic analysis for locally linear regression.</p><p>A nice introduction to LLR (and other topics in nonparametric estimates) can be found in the <a href="https://ocw.mit.edu/courses/economics/14-385-nonlinear-econometric-analysis-fall-2007/lecture-notes/" target="_blank" rel="noopener">lecture notes</a>. This article mainly follows <a href="https://ocw.mit.edu/courses/economics/14-385-nonlinear-econometric-analysis-fall-2007/lecture-notes/local_lin_reg.pdf" target="_blank" rel="noopener">local linear regression</a>.  </p>]]></content>
      
      
      <categories>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linear Regression </tag>
            
            <tag> Least Squares </tag>
            
            <tag> Nonparametric Estimates </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Concentration in Gauss Space</title>
      <link href="/2019/07/25/Concentration-in-Gauss-Space/"/>
      <url>/2019/07/25/Concentration-in-Gauss-Space/</url>
      
        <content type="html"><![CDATA[<p>This article aims to solve an exercise in <a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html#" target="_blank" rel="noopener">High-Dimensional Probability</a> (Page 112, Exercise 5.2.3).</p><h2 id="Gaussian-Isoperimetric-Inequality"><a href="#Gaussian-Isoperimetric-Inequality" class="headerlink" title="Gaussian Isoperimetric Inequality"></a>Gaussian Isoperimetric Inequality</h2><p>Let $\mathbb R^n$ equip with the <a href="https://en.wikipedia.org/wiki/Gaussian_measure" target="_blank" rel="noopener">Gauss measure</a> $\gamma_n$:</p><script type="math/tex; mode=display">\gamma_n(E)=\frac{1}{(2\pi)^{n/2}}\int_E\exp\{-\frac{\|x\|^2}{2}\}{\rm d}x</script><p>where $E\subseteq \mathbb R^n$ is a Borel set. Then $(\mathbb R^n,\gamma_n)$ is called Gauss space. Gaussian isoperimetric inequality states that, among all the measurable sets with given volume, half spaces have the <strong>smallest perimeter</strong>. Moreover, define the Minkowski sum </p><script type="math/tex; mode=display">A+B=\{a+b\in\mathbb R^n,a\in A,b\in B\}.</script><p>We have the following statement.</p><a id="more"></a><p><strong>Theorem.</strong> Let $\epsilon&gt;0$. Among all sets $A\subseteq \mathbb R^n$ with fixed Gauss measure $\gamma_n(A)$, the half spaces minimizes the Gauss measure $\gamma_n(A+B^n(\epsilon))$, where $B^n(\epsilon)$ is a solid ball centered at origin with radius $\epsilon$.</p><p>See <a href="https://link.springer.com/content/pdf/10.1007/BF01425510.pdf" target="_blank" rel="noopener">here</a> for a general definition of Gauss space and proof of the above theorem. See <a href="https://www.jstor.org/stable/29782707?seq=1#page_scan_tab_contents" target="_blank" rel="noopener">this paper</a> for a friendly introduction of Gaussian isoperimetric inequality.</p><h2 id="Concentration"><a href="#Concentration" class="headerlink" title="Concentration"></a>Concentration</h2><p>We want to prove the following concentration inequality in Gauss spaces.</p><p><strong>Theorem.</strong> Let $X\sim\mathcal N(0,I_n)$ be a Gauss variable. $f:\mathbb R^n\to \mathbb R$ is a Lipschitz function with respect to the standard Euclidean metric. Then $f(X)-\mathbb Ef(X)$ is a sub-gaussian variable. That is,</p><script type="math/tex; mode=display">\mathbb P(|f(X)-\mathbb Ef(X)|>t)\le \exp(-ct^2),\forall t>0,</script><p>where $c&gt;0$ is a constant.</p><p>First we prove a blow-up lemma using isoperimetric inequality.</p><p><strong>Lemma.</strong> Let $A$ be a subset in Gauss space. If $\gamma_n(A)&gt;1/2$, then for any $t&gt;0$, $\gamma_n(A+B^n(t))\ge1-\frac{1}{2}\exp{-t^2/2}$.</p><p><strong>Proof of Lemma.</strong> Let $H={x\in\mathbb R^n:x_1\le 0}$ be the half space. By assumption, $\gamma_n(A)&gt;\gamma_n(H)$. The Gaussian isoperimetric inequality implies </p><script type="math/tex; mode=display">\gamma_n(A+B^n(t))>\gamma_n(H+B^n(t)).</script><p>Note that $H+B^n(t)={x\in\mathbb R_n:x_1\le t}$ is also a half space. Direct computation shows that </p><script type="math/tex; mode=display">\begin{aligned}\gamma_n(H+B^n(t))&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^t\exp\{-\frac{x_1^2}{2}\}{\rm d}x_1\\&=1-\frac{1}{\sqrt{2\pi}}\int_t^{\infty}\exp\{-\frac{x_1^2}{2}\}{\rm d}x_1\\&=1-\frac{1}{\sqrt{2\pi}}\int_0^{\infty}\exp\{-\frac{(t+y)^2}{2}\}{\rm d}y\\&\ge 1-\frac{1}{\sqrt{2\pi}}\int_0^{\infty}\exp\{-\frac{t^2}{2}\}\exp\{-\frac{y^2}{2}\}{\rm d}y\\&=1-\frac{1}{2}\exp\{-\frac{t^2}{2}\}.\end{aligned}</script><p>Therefore, we have $\gamma_n(A+B^n(t))\ge1-\frac{1}{2}\exp{-t^2/2}$. $\blacksquare$ </p><p>This blow-up lemma enables us to prove the main theorem.</p><p><strong>Proof of Main Theorem.</strong> Without loss of generality, we assume $|f(x)-f(y)|\le |x-y|$. Let $m$ denote the median of $f(X)$. That is,</p><script type="math/tex; mode=display">\mathbb P(f(X)\le m)\ge 1/2,\mathbb P(f(X)\ge m)\ge 1/2.</script><p>Consider the level set $A=f^{-1}((-\infty,m])$. Note that $\mathbb P(f(X)\le m)=\gamma_n(A)\ge 1/2$. By blow-up lemma, $\gamma_n(A+B^n(t))\ge  1-1/2\exp{-t^2/2}$. On the other hand, if $x\in A+B^n(t) $, there exists $y\in A$ such that $|x-y|\le t$. Therefore, </p><script type="math/tex; mode=display">f(x)\le \|x-y\|+f(y)=f(y)+t.</script><p>This implies $\mathbb P(f(X)\le m+t)\ge \gamma_n(A+B^n(t))$, i.e. $\mathbb P(f(X)-m&gt;t)\le 1/2\exp{-t^2/2}$. </p><p>Replace $f$ with $-f$. We can obtain a similar inequality as $\mathbb P(f(X)-m&lt;-t)\le 1/2\exp{-t^2/2}$. Hence, </p><script type="math/tex; mode=display">\mathbb P(|f(X)-m|>t)\le \exp\{-t^2/2\},</script><p>which implies $f(X)-m$ is a sub-gaussian variable. Note that $\mathbb E(f(X)-m)=\mathbb E(f(X))-m$. We know that $f(X)-\mathbb Ef(X)$ is a sub-gaussian variable. $\blacksquare$</p><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><p>Let us take $f:\mathbb R^n\to\mathbb R$ to be the norm $|\cdot|$. Let $X\sim\mathcal N(0,I_n)$ be a Gauss variable. Direct computation shows that</p><script type="math/tex; mode=display">\begin{aligned}\mathbb Ef(X)&=\mathbb E\|X\|=\frac{1}{(2\pi)^{n/2}}\int \|x\|\exp\{-\frac{\|x\|^2}{2}\}{\rm d}x\\&=\frac{1}{(2\pi)^{n/2}}\int_0^{\infty}\rho\exp\{-\rho^2/2\}{\rm d}\rho\int_{S^{n-1}(\rho)}{\rm d}\sigma\\&=\frac{1}{(2\pi)^{n/2}}\times\frac{n\pi^{n/2}}{\Gamma(\frac{n}{2}+1)}\times 2^{(n-1)/2}\Gamma(\frac{n+1}{2})\\&=\frac{n}{\sqrt{2}}\times\frac{\Gamma(\frac{n+1}{2})}{\Gamma(\frac{n+2}{2})}.\end{aligned}</script><p>By <a href="[https://en.wikipedia.org/wiki/Stirling%27s_approximation](https://en.wikipedia.org/wiki/Stirling&#39;s_approximation">Stirling’s formula</a>), the expectation is about $\sqrt{n}$ when $n$ is large. This shows that in high dimensions, the points concentrate in a sphere of radius $\sqrt{n}$. </p>]]></content>
      
      
      <categories>
          
          <category> Probability </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Concentration Inequality </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bayesian Linear Regression</title>
      <link href="/2019/07/21/Bayesian-Linear-Regression/"/>
      <url>/2019/07/21/Bayesian-Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h2 id="The-Linear-Regression-Model"><a href="#The-Linear-Regression-Model" class="headerlink" title="The Linear Regression Model"></a>The Linear Regression Model</h2><p>A <em>normal linear regression model</em> assumes an output variable $Y$, a vector of covariates ${\bf x}=(x_1,\cdots ,x_p)$, and $Y$ is linear deterministic function $\bf \beta^Tx$ with additive Gaussian noise. <a id="more"></a>That is </p><script type="math/tex; mode=display">Y={\bf \beta^Tx}+\epsilon, \epsilon\sim \mathcal N(0,\sigma^2).</script><p>Given $n$ i.i.d. samples $({\bf x_1},y_1),\cdots, ({\bf x_n},y_n)$, the joint probability density of observed data $y_1,\cdots, y_n$ conditional upon $\bf x_1,\cdots,x_n$ and the value of $\bf \beta$ and $\sigma^2$ (or, the <strong>likelihood function</strong>) is</p><script type="math/tex; mode=display">\begin{aligned}&p(y_1,\cdots,y_n|{\bf x_1,\cdots,x_n},\beta,\sigma^2)=\prod_{i=1}^np(y_i|{\bf x_i},\beta,\sigma^2)\\&=(2\pi\sigma^2)^{-n/2}\exp\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\beta^T{\bf x_i})^2\}.\end{aligned}</script><p>Maximizing the likelihood function is equivalent to minimizing the sum of squared residuals $SSR(\beta)=\sum_{i=1}^n(y_i-\beta^T{\bf x_i})^2$. Let $\bf X=[\bf x_1,\cdots,x_n]^T$ and ${\bf y}=(y_1,\cdots, y_n)^T$. We have</p><script type="math/tex; mode=display">SSR(\beta)=(\bf y-X\beta)^T(y-X\beta)=\|y-X\beta\|_2^2.</script><p>The minimizer, also called the ordinary least square (OLS) estimate, is given in the closed form (suppose $\bf X^TX$ is invertible)</p><script type="math/tex; mode=display">\hat{\beta}_{ols}=\bf (X^TX)^{-1}X^Ty.</script><p>The closed form can be obtained using <a href="https://vdisk.weibo.com/s/uGmkTKh4CQ2uJ" target="_blank" rel="noopener">linear algebra</a>, which is often explained as the geometric meaning of least square.</p><h2 id="Bayesian-Linear-Regression"><a href="#Bayesian-Linear-Regression" class="headerlink" title="Bayesian Linear Regression"></a>Bayesian Linear Regression</h2><h3 id="A-Semiconjugate-Prior-for-beta"><a href="#A-Semiconjugate-Prior-for-beta" class="headerlink" title="A Semiconjugate Prior for $\beta$"></a>A Semiconjugate Prior for $\beta$</h3><p>Suppose $\beta\sim\mathcal N(\beta_0,\Sigma_0)$. The posterior distribution for $\beta$ is </p><script type="math/tex; mode=display">\begin{aligned}&p(\beta|{\bf y,X,\sigma^2})\propto p({\bf y}|{\bf X,\beta,\sigma^2})p(\beta)\\\propto& \exp\{\bf -\frac{1}{2}(-2\beta^TX^Ty/\sigma^2+\beta^TX^TX\beta/\sigma^2-2\beta^T\Sigma_0^{-1}\beta_0+\beta^T\Sigma_0^{-1}\beta)\}\\=&\exp\{\bf \beta^T(\Sigma_0^{-1}\beta_0+X^Ty/\sigma^2)-\frac{1}{2}\beta^T(\Sigma_0^{-1}+X^TX/\sigma^2)\beta\}.\end{aligned}</script><p>Therefore, the posterior distribution for $\beta$ is Gaussian with </p><script type="math/tex; mode=display">\begin{aligned}&\mathbb E[{\bf \beta|y,X,\sigma^2}]=(\Sigma_0^{-1}+{\bf X^TX/\sigma^2})^{-1}(\Sigma_0^{-1}\beta_0+{\bf X^Ty/\sigma^2})\\&\textrm{Var}[{\bf \beta|y,X,\sigma^2}]=(\Sigma_0^{-1}+{\bf X^TX/\sigma^2})^{-1}.\end{aligned}</script><p>Suppose $\beta_0=0$ and $\Sigma_0=\alpha^{-1}{\bf I}$. The posterior density is simplified as</p><script type="math/tex; mode=display">p(\beta|{\bf y,X,\sigma^2})\propto \exp\{-\frac{1}{2}(\beta^T({\bf \alpha I+X^TX/\sigma^2})\beta-2\beta^T{\bf X^Ty}/\sigma^2)\}.</script><p>Maximizing the posterior probability is equivalent to minimizing the error</p><script type="math/tex; mode=display">\frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\beta^T{\bf x_i})^2+\alpha\beta^T\beta,</script><p>which leads to the regularized least square problem. </p><h3 id="A-Semiconjugate-Prior-for-sigma-2"><a href="#A-Semiconjugate-Prior-for-sigma-2" class="headerlink" title="A Semiconjugate Prior for $\sigma^2$"></a>A Semiconjugate Prior for $\sigma^2$</h3><p>Let $\gamma=\frac{1}{\sigma^2}$ be the precision. Suppose $\gamma\sim Gamma(\nu_0/2,\nu_o\sigma^2_0/2)$. The posterior probability for $\gamma$ is</p><script type="math/tex; mode=display">\begin{aligned}&p(\gamma|{\bf y,X,\beta})\propto p(\gamma)p({\bf y}|{\bf X,\beta,\gamma})\\\propto&(\gamma^{\nu_0/2-1}\exp\{-\gamma\nu_0\sigma^0/2\})\times(\gamma^{n/2}\exp\{-\gamma SSR(\beta)/2\})\\=&\gamma^{(\nu_0+n)/2-1}\exp\{-\gamma(\nu_0\sigma^2_0+SSR(\beta))/2\}.\end{aligned}</script><p>Hence, the posterior distribution for $\gamma$ is $Gamma((\nu_0+n)/2,(\nu_0\sigma_0^2+SSR(\beta))/2)$. </p><h3 id="Gibbs-Sampler"><a href="#Gibbs-Sampler" class="headerlink" title="Gibbs Sampler"></a>Gibbs Sampler</h3><p>Given current values ${\beta^{(s)},(\sigma^2)^{(s)}}$. We can sample a new value by</p><ol><li>Updating $\beta$: <ol><li>Compute $\mathbb E[\beta|{\bf y,X,(\sigma^2)^{(s)}}]$ and $\textrm{Var}[\beta|{\bf y,X,(\sigma^2)^{(s)}}]$;</li><li>Sample $\beta^{(s+1)}$ according to $\mathcal N(\mathbb E,\textrm{Var})$.</li></ol></li><li>Updating $\sigma^2$:<ol><li>Compute $SSR(\beta^{(s+1)})$;</li><li>Sample $(\sigma^2)^{(s+1)}$ according to inverse-gamma.</li></ol></li></ol><h3 id="Zellner’s-g-Prior"><a href="#Zellner’s-g-Prior" class="headerlink" title="Zellner’s g-Prior"></a>Zellner’s g-Prior</h3><p>In the setting of prior distribution of $\beta$, let $\beta_0=0$ and $\Sigma_0=k({\bf X^TX})^{-1}$, where $k=g\sigma^2$. We obtain what is called <a href="https://en.wikipedia.org/wiki/G-prior" target="_blank" rel="noopener">g-prior</a>. The mean and variance are simplified as </p><script type="math/tex; mode=display">\begin{aligned}&\mathbb E[\beta|{\bf y,X,\sigma^2}]=\frac{g}{g+1}\sigma^2{\bf \Phi}\\&\textrm{Var}[\beta|{\bf y,X,\sigma^2}]=\frac{g}{g+1}{\bf \Phi X^Ty}.\end{aligned}</script><p>where $\bf\Phi=(X^TX)^{-1}$. Suppose the prior distribution for $\sigma^2$ is inverse-gamma. We will show that $p(\sigma^2|{\bf y,X})$ is inverse-gamma. Compared with the above discussion, this simplifies our sampling procedure since the posterior does <strong>NOT</strong> depend on $\beta$. </p><p>Note that </p><script type="math/tex; mode=display">p({\bf y}|{\bf X,\sigma^2})=\int p({\bf y}|{\bf X,\sigma^2,\beta})p(\beta|{\bf X,\sigma^2}){\rm d}\beta.</script><p>Substituting the densities, we obtain</p><script type="math/tex; mode=display">\begin{aligned}p({\bf y}|{\bf X,\sigma^2,\beta})p(\beta|{\bf X,\sigma^2})&=C \exp\{-\frac{1}{2\sigma^2}({\bf y-X\beta})^T({\bf y-X\beta})-\frac{1}{2g\sigma^2}\beta^T{\bf \Phi^{-1}}\beta\}.\end{aligned}</script><p>where</p><script type="math/tex; mode=display">C=(2\pi\sigma^2)^{-n/2}\times|2\pi g\sigma^2{\Phi}|^{-1/2}.</script><p>The terms in the exponent can be rewritten as</p><script type="math/tex; mode=display">-\frac{1}{2\sigma^2}{\bf y^Ty}-\frac{1}{2}(\beta-m)^TV^{-1}(\beta-m)+\frac{1}{2}m^TV^{-1}m.</script><p>where</p><script type="math/tex; mode=display">\begin{aligned}&V=\frac{g}{g+1}\sigma^2{\bf \Phi}\\&m=\frac{g}{g+1}{\bf \Phi X^Ty}.\end{aligned}</script><p>The integration is</p><script type="math/tex; mode=display">p({\bf y}|{\bf X,\sigma^2})=(2\pi)^{-n/2}(1+g)^{-p/2}(\sigma^2)^{-n/2}\exp\{-\frac{1}{2\sigma^2}SSR_g\},</script><p>where</p><script type="math/tex; mode=display">SSR_g=\bf y^T(I-{\it \frac{g}{g+1}}X^T\Phi X)y.</script><p>Let $\gamma=1/\sigma^2\sim Gamma(\nu_0/2,\nu_0\sigma^2_0/2)$. Then we have</p><script type="math/tex; mode=display">\begin{aligned}p(\gamma|{\bf X,y})&\propto p(\gamma)p({\bf y}|{\bf X,\gamma})\\&\propto \gamma^{(\nu_0+n)/2-1}\exp\{-\gamma(\nu_0\sigma^2_0+SSR_g)/2\}.\end{aligned}</script><p>Therefore, the posterior distribution for $\sigma^2$ is inverse-gamma with parameter $((\nu_0+n)/2,(\nu_0\sigma^2_0+SSR_g)/2)$. </p><h3 id="Monte-Carlo-Sampler"><a href="#Monte-Carlo-Sampler" class="headerlink" title="Monte Carlo Sampler"></a>Monte Carlo Sampler</h3><p>Using g-prior, we see that the posterior distribution of $\sigma^2$ does not contain $\beta$. Thus we can use a simple sampling procedure</p><ol><li>Sample $\sigma^2$ according to inverse-gamma;</li><li>Sample $\beta\sim\mathcal N(\frac{g}{g+1}\hat{\beta}_{ols},\frac{g}{g+1}\sigma^2{\bf \Phi})$.</li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Hoff, Peter D. <em>A first course in Bayesian statistical methods</em>. Vol. 580. New York: Springer, 2009.</li><li>Bishop, Christopher M. <em>Pattern recognition and machine learning</em>. springer, 2006.</li></ol>]]></content>
      
      
      <categories>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linear Regression </tag>
            
            <tag> Least Squares </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes on KNN Density Estimates (1)</title>
      <link href="/2019/07/18/Notes-on-KNN-Density-Estimates(1)/"/>
      <url>/2019/07/18/Notes-on-KNN-Density-Estimates(1)/</url>
      
        <content type="html"><![CDATA[<p>This series is a note of <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=530638" target="_blank" rel="noopener">Y. P. Mack and M. Rosenblatt</a>‘s work in 1979. </p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Suppose we have a <strong>bounded, twice differentiable density</strong> function $f$ on $\mathbb R^p$. Let $X_1,X_2,\cdots,X_n$ be $n$ i.i.d. samples of $f$. Let $x\in\mathbb R^p$ be a fixed point so that $f(x)&gt;0$. We want to estimate $f(x)$ based on $n$ samples. The KNN estimate is given by </p><script type="math/tex; mode=display">f_n(x)=\frac{1}{n(Z_n)^p}\sum_{j=1}^n\omega(\frac{x-X_j}{Z_n}),</script><p>where $Z_n$ is the distance between $x$ and its $k$th nearest neighbor, and $\omega$ is a <strong>bounded integrable</strong> weight function with $\int\omega(u){\rm d}u=1$. </p><a id="more"></a><p>Let $\omega(u)=\frac{1}{|B<em>0(1)|}\chi</em>{B_0(1)}(u)$, where $B_0(1)={x\in\mathbb R^p:|x|\le 1}$ is the solid ball centered at origin with radius 1, and $|B_0(1)|=\frac{\pi^{p/2}}{\Gamma(\frac{p+2}{2})}$ is the volume of unit ball. Under these settings we see that the KNN estimate is </p><script type="math/tex; mode=display">f_n(x)=\frac{1}{n(Z_n)^p}\cdot\frac{k}{|B_0(1)|}=\frac{k}{n}\cdot\frac{1}{|B_0(Z_n)|}.</script><p>If we see $\frac{k}{n}$ as the probability that points lie in a ball around $x$, then it is a naive analogy of $\text{probability}=\int\text{density}\approx\text{density}\times \text{area}$. In the following we always assume that $k=k(n)$ is a function of sample size $n$ and $k(n)\to \infty,k(n)/n\to0$ as $n\to\infty$.</p><p>We first investigate several properties of $Z_n$. For simplicity, we call $Z_n$ the KNN distance variable.</p><h2 id="KNN-distance-variable-Z-n"><a href="#KNN-distance-variable-Z-n" class="headerlink" title="KNN distance variable $Z_n$"></a>KNN distance variable $Z_n$</h2><h3 id="CDF-and-PDF-of-Z-n"><a href="#CDF-and-PDF-of-Z-n" class="headerlink" title="CDF and PDF of $Z_n$"></a>CDF and PDF of $Z_n$</h3><p>For fixed $x\in\mathbb R^p$, note that $Z_n$ is a function of $n$ i.i.d. samples. Let $H(r)$ be the cumulative distribution function (<strong>CDF</strong>) of $Z_n$. Let </p><script type="math/tex; mode=display">G(r)=\mathbb P(B_x(r))=\int_{B_x(r)}f(u){\rm d}u.</script><p>For any $\epsilon&gt;0$, consider the probability $H(r+\epsilon)-H(r)=\mathbb P(r\le Z_n\le r+\epsilon)$. Suppose $\epsilon$ is small enough so that there is <strong>exactly one</strong> point lying in the shell of thickness $\epsilon$ (It happens when multiple points lie in the shell, but they will not contribute to our deduction since higher order of $\epsilon$ vanishes when taking limit). There are $n$ choices of $k$th nearest point, $\binom{n-1}{k-1}$ choices of $k-1$ points lying within the $r$ ball and others are outside the $r+\epsilon$ ball. Therefore, we have</p><script type="math/tex; mode=display">H(r+\epsilon)-H(r)=n(G(r+\epsilon)-G(r))\binom{n-1}{k-1}G(r)^{k-1}(1-G(r+\epsilon))^{n-k}.</script><p>Let $h(r)$ be the probability density function (<strong>PDF</strong>) of $Z_n$. By definition, we have</p><script type="math/tex; mode=display">h(r)=\lim\limits_{\epsilon\to0}\frac{H(r+\epsilon)-H(r)}{\epsilon}=n\binom{n-1}{k-1}G(r)^{k-1}(1-G(r))^{n-k}G'(r).</script><p>Furthermore, $G’(r)$ can be expressed as an integration of $f$. Note that</p><script type="math/tex; mode=display">\begin{aligned}G'(r)&=\lim\limits_{\delta\to 0}\frac{1}{\delta}[\int_{B_x(r+\delta)}f(u){\rm d}u-\int_{B_x(r)}f(u){\rm d}u]\\&=\lim_{\delta\to0}\frac{1}{\delta}\int_r^{r+\delta}\rho^{p-1}{\rm d}\rho\int_{S_x(1)}f(\rho,t){\rm d}\sigma(t)\\&=\int_{S_x(r)}f(t){\rm d}\sigma(t),\end{aligned}</script><p>where $S_x(r)={y\in\mathbb R^p:|y-x|=r}$ is the sphere centered at $x$ with radius $r$, and ${\rm d}\sigma$ is the volume element on the sphere. </p><p>On the other hand, for each $X_i$, $|X_i-x|$ is a random variable with CDF $G(r)$ and   PDF $G’(r)$. Note that $Z_n$ is exactly the <strong>$k$th order statistic</strong> from i.i.d. samples $|X_i-x|$. The $k$th order statistic gives the same $h(r)$ as we deduced above.</p><h3 id="Moments-of-Z-n"><a href="#Moments-of-Z-n" class="headerlink" title="Moments of $Z_n$"></a>Moments of $Z_n$</h3><p>In general, for a measurable function $\phi$, we want to compute the expectation $\mathbb{E}\phi(Z_n)$. By definition,</p><script type="math/tex; mode=display">\begin{aligned}\mathbb E\phi(Z_n)&=\int_0^{\infty}\phi(r)h(r){\rm d}r\\&=n\binom{n-1}{k-1}\int_0^{\infty}\phi(r)G(r)^{k-1}(1-G(r))^{n-k}{\rm d}G(r).\end{aligned}</script><p>In our situation, we consider a specific function of the type</p><script type="math/tex; mode=display">\phi(r)=\frac{r^{\lambda}}{G(r)^\gamma(1-G(r))^\beta},</script><p>where $\lambda,\gamma,\beta$ are nonnegative integers. Note that $G(r)\in[0,1]$. To ensure that the integration exists, we assume that $1-G(r)=O(r^{-\xi})$, where $\xi&gt;0$, as $r\to \infty$.  </p><p>Since $G(r)$ is a monotonically increasing function, we can solve $r=G^{-1}(t)$ where $t\in[0,1]$. On the one hand,</p><script type="math/tex; mode=display">\begin{aligned}G(r)&=\int_{B_x(r)}f(u){\rm d}u\\&=f(x)|B_x(r)|+\int_{B_x(r)}f(u)-f(x){\rm d}u,\\\end{aligned}</script><p>where</p><script type="math/tex; mode=display">\int_{B_x(r)}|f(u)-f(x|{\rm d}u\le K(r)\int_{B_x(r)}\|u-x\|{\rm d  }u=\frac{K(r)}{n+1}r^{p+1}|S_x(1)|.</script><p>Therefore, $t=G(r)=cf(x)r^p+o(r^p)$ as $r\to 0$, where $c=|B_x(1)|$. From the assumption $1-G(r)=O(r^{-\xi})$, we see that $t=1-O(r^{-\xi})$ as $r\to\infty$. We can write $r$ as</p><script type="math/tex; mode=display">r=G^{-1}(t)=(\frac{t}{cf(x)})^{1/p}+\eta(t),</script><p>where $\eta(t)=o(t^{1/p})$ as $t\to 0$ and $\eta(t)=O(\frac{1}{(1-t)^\xi})$ as $t\to 1$.</p><p>Using the change of variable $t=G(r)$, we see that</p><script type="math/tex; mode=display">\begin{aligned}\mathbb E\phi(Z_n)&=n\binom{n-1}{k-1}\int_0^1(\frac{t}{cf(x)}+\eta(t))^\lambda t^{k-1-\gamma}(1-t)^{n-k-\beta}{\rm d}t\\\end{aligned}</script><hr><p><strong>Personal Comments</strong></p><p>I was intended to study <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=530638" target="_blank" rel="noopener">Y. P. Mack and M. Rosenblatt</a>‘s paper thoroughly to understand the asymptotic behaviors of kNN estimator. Both results and techniques in proofs are important for me. However, I was very upset to find many subtle details I could not get through. The biggest problem appears in equation (12)</p><blockquote><script type="math/tex; mode=display">\mathbb E \phi(R_n)=n\binom{n-1}{k-1}\int_0^1((\frac{t}{cf(x)})^\lambda+o(t^\lambda))t^{k-1-\gamma}(1-t)^{n-k-\beta}{\rm d}t</script></blockquote><p>The notation $o(t)$ is misleading. It is correct when $t$ is small. However, when $t$ is close to 1, this term tends to infinity. Therefore, the estimation for this expectation is not easy. One cannot simply drop the $o(t)$ integration. But according to the paper (though they did not display the computation), $o(t)$ was simply dropped. </p><p>It was also disappointing that I found there were few papers concerning the asymptotic behaviors about kNN. One could find that the asymptotic moments for kNN distance variable are well-known. But the references were not mentioned. </p><p>There is another paper discussing the asymptotic moments of kNN distance. But it restricts to compact convex area. See <a href="https://royalsocietypublishing.org/doi/abs/10.1098/rspa.2002.1011" target="_blank" rel="noopener">Asymptotic moments of near–neighbour distance distributions</a>.</p><p>I will keep on tracking about this topic.</p>]]></content>
      
      
      <categories>
          
          <category> Statistical-Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Asymptotic Analysis </tag>
            
            <tag> Nonparametric Estimates </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Proof of &#39;Everything is connected&#39;</title>
      <link href="/2019/07/14/a%20proof%20of%20everything%20is%20connected/"/>
      <url>/2019/07/14/a%20proof%20of%20everything%20is%20connected/</url>
      
        <content type="html"><![CDATA[<h2 id="Stories"><a href="#Stories" class="headerlink" title="Stories"></a>Stories</h2><p>The idea about building my own blog was originated from a long time ago when I first read <a href="https://terrytao.wordpress.com/" target="_blank" rel="noopener">Terry Tao’s homepage</a>. In 2019, when I became a graduate student, I realized it is necessary to write down the mathematics to keep track of my research. Honestly speaking, repetitions and failures really make one suffer. It is those tiny progresses that encourage me to hold on.</p><a id="more"></a><p>The title of this blog comes from an <a href="https://mp.weixin.qq.com/s?__biz=MjM5NDEzNjI0Mg==&amp;mid=2698897605&amp;idx=1&amp;sn=a19cb1dd989bbb6740345ceb476b01a7&amp;chksm=83a52118b4d2a80e6d4254aa2dac5442302180583a257c251e051a33f18b3a4a7a993d68cd36&amp;mpshare=1&amp;scene=1&amp;srcid=0716ytVjaAJNShmEwGSlmWpF&amp;key=dd5051400a9fb58f808d2da53b802b8acf4d01cc85436ece84b72433ba3fd12b83709bd533967b30d98fcbb2cafbb52010be54affd44f1114534efd269163755ef08943a54bd6644af14d7e46d5cd77b&amp;ascene=1&amp;uin=MTU3ODQ1MjIwNg%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=hrTohvqyTgd41p4viiU31IowGnkdWinCUMu%2FwVIpE0dlQBF2lbrj1dBRey65qCVn" target="_blank" rel="noopener">essay</a> which ingeniously describes a philosophy, that every thing, every event, every people are connected with each other. Like <a href="https://en.wikipedia.org/wiki/Leonardo_da_Vinci" target="_blank" rel="noopener">da Vinci</a> said, <em>Learn how to see. Realize that everything connects to everything else</em>. When I read these, a great idea hit me that I could formulate a mathematical proposition and prove it. Though the proof seems naive, I was happy to find such an explanation. </p><h2 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h2><p>Let us recall some basics from <a href="https://en.wikipedia.org/wiki/Graphical_model" target="_blank" rel="noopener">probabilistic graphical model</a>. We use random variables to represent true events in real life. The relationship between events is modeled by <strong>conditional independence</strong>. Suppose $X,Y$ and $Z$ are three random variables. If the joint probability density function of $X,Y$ and $Z$ satisfy the following equation</p><script type="math/tex; mode=display">f(x,y|z)=f(x|z)f(y|z),</script><p>$X$ and $Y$ are said to be conditional independent given $Z$. It generalizes the concept of <strong>independence</strong> where we known nothing prior ($Z=\emptyset$). In our case, it is reasonable that two events have no relationship if they are independent when we know all the other things.</p><p>Now we can construct a graph based on the above argument. The vertices are random variables. Two distinct vertices are connected by an edge if they are not conditional independent. One event is ‘connected’ to another if there is a path joining them. Therefore, when we say ‘Everything is connected’, we actually mean that the graph is connected.</p><p>However, we should be cautious about ‘everything’ since we just considered finite number of events. Given arbitrary $n$ events, we can say ‘These $n$ things are connected’ if the corresponding graph is connected. But it is too stupid to say such things, and we are not going to infer whether a graph is connected, either. Instead, let us look at the probability that a graph with $n$ nodes is connected.</p><h2 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h2><p>Let $\mathbb{G}_n$ be the space of undirected simple graphs with $n$ nodes. $\mathbb{G}_n$ is a finite set with $2^\frac{n(n-1)}{2}$ elements. Let $\mathbb{G}_n^c$ be the subset of all connected graphs, $\mathbb G_n^d$ be the subset of all disconnected graphs. Define the connectivity ratio </p><script type="math/tex; mode=display">R_n=\frac{ \#(\mathbb{G}_n^d)}{ \#(\mathbb{G}_n)}:=\frac{K_n^d}{K_n^c+K_n^d}.</script><p>where we have $K_n^c+K_n^d=2^\frac{n(n-1)}{2}$.</p><p>Let $v<em>0,v_1,\cdots,v_n$ be vertices of a graph $G\in\mathbb{G}</em>{n+1}^d$. Suppose $C_0$ is the component containing $v_0$ with $i$ points. Let $C_1$ be the complement of $C_0$ with $n+1-i$ points. The we have the following recursion </p><script type="math/tex; mode=display">K_{n+1}^d=\sum_{i=1}^n\binom{n}{i-1}K_i^c2^\frac{(n+1-i)(n-i)}{2}.</script><p>Dividing both sides by $2^\frac{(n+1)n}{2}$, we have</p><script type="math/tex; mode=display">R_{n+1}=\sum_{i=1}^n\binom{n}{i-1}(1-R_i)2^\frac{-i(n+1-i)}{2}.</script><p>Note that $1-R_i<1$ when $i>1$. Multiply both sides by $2^n$,</1$></p><script type="math/tex; mode=display">2^nR_{n+1}<1+n+\sum_{i=2}^{n-1}\binom{n}{i-1}2^{-(n-i)(i-1)}.</script><p>When $1&lt;i&lt;n$, $2^{-(n-i)(i-1)}&lt;2^{-(n-2)}$. We obtain that $2^nR_{n+1}&lt;n+5$. That is, the connectivity ratio $R_n&lt;\frac{n+4}{2^{n-1}}$. When $n$ tends to infinity, $R_n$ tends to $0$.</p><p>Given arbitrarily $n$ events, we are right to guess that they are connected with probability greater than $1-\frac{n+4}{2^{n-1}}$. The probability grows with respect to the number of events. When $n$ goes to infinity, we are 100% certain that ‘Everything is connected’!</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      
      
      <categories>
          
          <category> About-My-Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> About My Blog </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
